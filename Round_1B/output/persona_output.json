{
  "metadata": {
    "documents": [
      "DBMS_Full_Notes.pdf",
      "DBMS_gfg.pdf",
      "OS_Full_Notes.pdf"
    ],
    "persona": "Investment Analyst",
    "job_to_be_done": "Analyze revenue trends, R&D investments, and market positioning strategies",
    "timestamp": "2025-07-28T16:00:14.575123"
  },
  "extracted_sections": [
    {
      "document": "DBMS_Full_Notes.pdf",
      "importance_rank": 1,
      "refined_text": "LEC-1: Introduction to DBMS \n1.\nWhat is Data?\na.\nData is a collection of raw, unorganized facts and details like text, observations, figures, symbols,\nand descriptions of things etc.\nIn other words, data does not carry any specific purpose and has no significance by itself.\nMoreover, data is measured in terms of bits and bytes \u2013 which are basic units of information in the\ncontext of computer storage and processing.\nb.\nData can be recorded and doesn\u2019t have any meaning unless processed.\n2.\nTypes of Data\na.\nQuantitative\ni.\nNumerical form\nii.\nWeight, volume, cost of an item.\nb.\nQualitative\ni.\nDescriptive, but not numerical.\nii.\nName, gender, hair color of a person.\n3.\nWhat is Information?\na.\nInfo. Is processed, organized, and structured data.\nb.\nIt provides context of the data and enables decision making.\nc.\nProcessed data that make sense to us.\nd.\nInformation is extracted from the data, by analyzing and interpreting pieces of data.\ne.\nE.g., you have data of all the people living in your locality, its Data, when you analyze and interpret\nthe data and come to some conclusion that:\ni.\nThere are 100 senior citizens.\nii.\nThe sex ratio is 1.1.\niii.\nNewborn babies are 100.\nThese are information.\n4.\nData vs Information\na.\nData is a collection of facts, while information puts those facts into context.\nb.\nWhile data is raw and unorganized, information is organized.\nc.\nData points are individual and sometimes unrelated. Information maps out that data to provide a\nbig-picture view of how it all fits together.\nd.\nData, on its own, is meaningless. When it\u2019s analyzed and interpreted, it becomes meaningful\ninformation.\ne.\nData does not depend on information; however, information depends on data.\nf.\nData typically comes in the form of graphs, numbers, figures, or statistics. Information is typically\npresented through words, language, thoughts, and ideas.\ng.\nData isn\u2019t sufficient for decision-making, but you can make decisions based on information.\n5.\nWhat is Database?\na.\nDatabase is an electronic place/system where data is stored in a way that it can be easily accessed,\nmanaged, and updated.\nb.\nTo make real use Data, we need Database management systems. (DBMS)\n6.\nWhat is DBMS?\na.\nA database-management system (DBMS) is a collection of interrelated data and a set of\nprograms to access those data. The collection of data, usually referred to as the database,\ncontains information relevant to an enterprise. The primary goal of a DBMS is to provide a way to\nstore and retrieve database information that is both convenient and efficient.\nb.\nA DBMS is the database itself, along with all the software and functionality. It is used to perform\ndifferent operations, like addition, access, updating, and deletion of the data.\nCodeHelp\n7. \n8.\nDBMS vs File Systems\na.\nFile-processing systems has major disadvantages.\ni.\nData Redundancy and inconsistency\nii.\nDifficulty in accessing data\niii.\nData isolation\niv.\nIntegrity problems\nv.\nAtomicity problems\nvi.\nConcurrent-access anomalies\nvii.\nSecurity problems\nb.\nAbove 7 are also the Advantages of DBMS (answer to \u201cWhy to use DBMS?\u201d)\nCodeHelp\nLEC-2: DBMS Architecture \n1.\nView of Data (Three Schema Architecture)\na.\nThe major purpose of DBMS is to provide users with an abstract view of the data. That is, the\nsystem hides certain details of how the data is stored and maintained.\nb.\nTo simplify user interaction with the system, abstraction is applied through several levels of\nabstraction.\nc.\nThe main objective of three level architecture is to enable multiple users to access the same data\nwith a personalized view while storing the underlying data only once\nd.\nPhysical level / Internal level\ni.\nThe lowest level of abstraction describes how the data are stored.\nii.\nLow-level data structures used.\niii.\nIt has Physical schema which describes physical storage structure of DB.\niv.\nTalks about: Storage allocation (N-ary tree etc), Data compression & encryption etc.\nv.\nGoal: We must define algorithms that allow efficient access to data.\ne.\nLogical level / Conceptual level:\ni.\nThe conceptual schema describes the design of a database at the conceptual level,\ndescribes what data are stored in DB, and what relationships exist among those data.\nii.\nUser at logical level does not need to be aware about physical-level structures.\niii.\nDBA, who must decide what information to keep in the DB use the logical level of\nabstraction.\niv.\nGoal: ease to use.\nf.\nView level / External level:\ni.\nHighest level of abstraction aims to simplify users\u2019 interaction with the system by\nproviding different view to different end-user.\nii.\nEach view schema describes the database part that a particular user group is interested\nand hides the remaining database from that user group.\niii.\nAt the external level, a database contains several schemas that sometimes called as\nsubschema. The subschema is used to describe the different view of the database.\niv.\nAt views also provide a security mechanism to prevent users from accessing certain parts\nof DB.\ng. \n2.\nInstances and Schemas\na.\nThe collection of information stored in the DB at a particular moment is called an instance of DB.\nCodeHelp\nb.\nThe overall design of the DB is called the DB schema.\nc.\nSchema is structural description of data. Schema doesn\u2019t change frequently. Data may change\nfrequently.\nd.\nDB schema corresponds to the variable declarations (along with type) in a program.\ne.\nWe have 3 types of Schemas: Physical, Logical, several view schemas called subschemas.\nf.\nLogical schema is most important in terms of its effect on application programs, as programmers\nconstruct apps by using logical schema.\ng.\nPhysical data independence, physical schema change should not affect logical\nschema/application programs.\n3.\nData Models:\na.\nProvides a way to describe the design of a DB at logical level.\nb.\nUnderlying the structure of the DB is the Data Model; a collection of conceptual tools for describing\ndata, data relationships, data semantics & consistency constraints.\nc.\nE.g., ER model, Relational Model, object-oriented model, object-relational data model etc.\n4.\nDatabase Languages:\na.\nData definition language (DDL) to specify the database schema.\nb.\nData manipulation language (DML) to express database queries and updates.\nc.\nPractically, both language features are present in a single DB language, e.g., SQL language.\nd.\nDDL\ni.\nWe specify consistency constraints, which must be checked, every time DB is updated.\ne.\nDML\ni.\nData manipulation involves\n1.\nRetrieval of information stored in DB.\n2.\nInsertion of new information into DB.\n3.\nDeletion of information from the DB.\n4.\nUpdating existing information stored in DB.\nii.\nQuery language, a part of DML to specify statement requesting the retrieval of\ninformation.\n5.\nHow is Database accessed from Application programs?\na.\nApps (written in host languages, C/C++, Java) interacts with DB.\nb.\nE.g., Banking system\u2019s module generating payrolls access DB by executing DML statements from\nthe host language.\nc.\nAPI is provided to send DML/DDL statements to DB and retrieve the results.\ni.\nOpen Database Connectivity (ODBC), Microsoft \u201cC\u201d.\nii.\nJava Database Connectivity (JDBC), Java.\n6.\nDatabase Administrator (DBA)\na.\nA person who has central control of both the data and the programs that access those data.\nb.\nFunctions of DBA\ni.\nSchema Definition\nii.\nStorage structure and access methods.\niii.\nSchema and physical organization modifications.\niv.\nAuthorization control.\nv.\nRoutine maintenance\n1.\nPeriodic backups.\n2.\nSecurity patches.\n3.\nAny upgrades.\n7.\nDBMS Application Architectures: Client machines, on which remote DB users work, and server machines\non which DB system runs.\na.\nT1 Architecture\ni.\nThe client, server & DB all present on the same machine.\nCodeHelp\nb.\nT2 Architecture\ni.\nApp is partitioned into 2-components.\nii.\nClient machine, which invokes DB system functionality at server end through query\nlanguage statements.\niii.\nAPI standards like ODBC & JDBC are used to interact between client and server.\nc.\nT3 Architecture\ni.\nApp is partitioned into 3 logical components.\nii.\nClient machine is just a frontend and doesn\u2019t contain any direct DB calls.\niii.\nClient machine communicates with App server, and App server communicated with DB\nsystem to access data.\niv.\nBusiness logic, what action to take at that condition is in App server itself.\nv.\nT3 architecture are best for WWW Applications.\nvi.\nAdvantages:\n1.\nScalability due to distributed application servers.\n2.\nData integrity, App server acts as a middle layer between client and DB, which\nminimize the chances of data corruption.\n3.\nSecurity, client can\u2019t directly access DB, hence it is more secure.\nCodeHelp\nLEC-3: Entity-Relationship Model\n1.\nData Model: Collection of conceptual tools for describing data, data relationships, data semantics, and consistency\nconstraints.\n2.\nER Model\n1.\nIt is a high level data model based on a perception of a real world that consists of a collection of basic objects, called\nentities and of relationships among these objects.\n2.\nGraphical representation of ER Model is ER diagram, which acts as a blueprint of DB.\n3.\nEntity: An Entity is a \u201cthing\u201d or \u201cobject\u201d in the real world that is distinguishable from all other objects.\n1.\nIt has physical existence.\n2.\nEach student in a college is an entity.\n3.\nEntity can be uniquely identified. (By a primary attribute, aka Primary Key)\n4.\nStrong Entity: Can be uniquely identified.\n5.\nWeak Entity: Can\u2019t be uniquely identified., depends on some other strong entity.\n1.\nIt doesn\u2019t have sufficient attributes, to select a uniquely identifiable attribute.\n2.\nLoan -> Strong Entity, Payment -> Weak, as instalments are sequential number counter can be generated\nseparate for each loan.\n3.\nWeak entity depends on strong entity for existence.\n4.\nEntity set\n1.\nIt is a set of entities of the same type that share the same properties, or attributes.\n2.\nE.g., Student is an entity set.\n3.\nE.g., Customer of a bank\n5.\nAttributes\n1.\nAn entity is represented by a set of attributes.\n2.\nEach entity has a value for each of its attributes.\n3.\nFor each attribute, there is a set of permitted values, called the domain, or value set, of that attribute.\n4.\nE.g., Student Entity has following attributes\nA.\nStudent_ID\nB.\nName\nC.\nStandard\nD.\nCourse\nE.\nBatch\nF.\nContact number\nG.\nAddress\n5.\nTypes of Attributes\n1.\nSimple\n1.\nAttributes which can\u2019t be divided further.\n2.\nE.g., Customer\u2019s account number in a bank, Student\u2019s Roll number etc.\n2.\nComposite\n1.\nCan be divided into subparts (that is, other attributes).\n2.\nE.g., Name of a person, can be divided into first-name, middle-name, last-name.\n3.\nIf user wants to refer to an entire attribute or to only a component of the attribute.\n4.\nAddress can also be divided, street, city, state, PIN code.\n3.\nSingle-valued\n1.\nOnly one value attribute.\n2.\ne.g., Student ID, loan-number for a loan.\n4.\nMulti-valued\n1.\nAttribute having more than one value.\n2.\ne.g., phone-number, nominee-name on some insurance, dependent-name etc.\n3.\nLimit constraint may be applied, upper or lower limits.\n5.\nDerived\n1.\nValue of this type of attribute can be derived from the value of other related attributes.\nCodeHelp\n2.\ne.g., Age, loan-age, membership-period etc.\n6.\nNULL Value\n1.\nAn attribute takes a null value when an entity does not have a value for it.\n2.\nIt may indicate \u201cnot applicable\u201d, value doesn\u2019t exist. e.g., person having no middle-name\n3.\nIt may indicate \u201cunknown\u201d.\n1.\nUnknown can indicate missing entry, e.g., name value of a customer is NULL, means it is missing as name\nmust have some value.\n2.\nNot known, salary attribute value of an employee is null, means it is not known yet.\n6.\nRelationships\n1.\nAssociation among two or more entities.\n2.\ne.g., Person has vehicle, Parent has Child, Customer borrow loan etc.\n3.\nStrong Relationship, between two independent entities.\n4.\nWeak Relationship, between weak entity and its owner/strong entity.\n1.\ne.g., Loan <instalment-payments> Payment.\n5.\nDegree of Relationship\n1.\nNumber of entities participating in a relationship.\n2.\nUnary, Only one entity participates. e.g., Employee manages employee.\n3.\nBinary, two entities participates. e.g., Student takes Course.\n4.\nTernary relationship, three entities participates. E.g, Employee works-on branch, employee works-on job.\n5.\nBinary are common.\n7.\nRelationships Constraints\n1.\nMapping Cardinality / Cardinality Ratio\n1.\nNumber of entities to which another entity can be associated via a relationship.\n2.\nOne to one, Entity in A associates with at most one entity in B, where A & B are entity sets. And an entity\nof B is associated with at most one entity of A.\n1.\nE.g., Citizen has Aadhar Card.\n3.\nOne to many, Entity in A associated with N entity in B. While entity in B is associated with at most one\nentity in A.\n1.\ne.g., Citizen has Vehicle.\n4.\nMany to one, Entity in A associated with at most one entity in B. While entity in B can be associated with\nN entity in A.\n1.\ne.g., Course taken by Professor.\n5.\nMany to many, Entity in A associated with N entity in B. While entity in B also associated with N entity in\nA.\n1.\nCustomer buys product.\n2.\nStudent attend course."
    },
    {
      "document": "DBMS_Full_Notes.pdf",
      "importance_rank": 2,
      "refined_text": "2.\nParticipation Constraints\n1.\nAka, Minimum cardinality constraint.\n2.\nTypes, Partial & Total Participation.\n3.\nPartial Participation, not all entities are involved in the relationship instance.\n4.\nTotal Participation, each entity must be involved in at least one relationship instance.\n5.\ne.g., Customer borrow loan, loan has total participation as it can\u2019t exist without customer entity. And \ncustomer has partial participation.\n6.\nWeak entity has total participation constraint, but strong may not have total.\n8.\nER Notations\nCodeHelp\nCodeHelp\nLEC-4: Extended ER Features\n1.\nBasic ER Features studied in the LEC-3, can be used to model most DB features but when complexity increases, it is\nbetter to use some Extended ER features to model the DB Schema.\n2.\nSpecialisation\n1.\nIn ER model, we may require to subgroup an entity set into other entity sets that are distinct in some way with other\nentity sets.\n2.\nSpecialisation is splitting up the entity set into further sub entity sets on the basis of their functionalities,\nspecialities and features.\n3.\nIt is a Top-Down approach.\n4.\ne.g., Person entity set can be divided into customer, student, employee. Person is superclass and other specialised\nentity sets are subclasses.\n1.\nWe have \u201cis-a\u201d relationship between superclass and subclass.\n2.\nDepicted by triangle component.\n5.\nWhy Specialisation?\n1.\nCertain attributes may only be applicable to a few entities of\u2028\nthe parent entity set.\n2.\nDB designer can show the distinctive features of the sub entities.\n3.\nTo group such entities we apply Specialisation, to overall refine the DB blueprint.\n3.\nGeneralisation\n1.\nIt is just a reverse of Specialisation.\n2.\nDB Designer, may encounter certain properties of two entities are overlapping. Designer may consider to make a\nnew generalised entity set. That generalised entity set will be a super class.\n3.\n\u201cis-a\u201d relationship is present between subclass and super class.\n4.\ne.g., Car, Jeep and Bus all have some common attributes, to avoid data repetition for the common attributes. DB\ndesigner may consider to Generalise to a new entity set \u201cVehicle\u201d.\n5.\nIt is a Bottom-up approach.\n6.\nWhy Generalisation?\n1.\nMakes DB more refined and simpler.\n2.\nCommon attributes are not repeated.\n4.\nAttribute Inheritance\n1.\nBoth Specialisation and Generalisation, has attribute inheritance.\n2.\nThe attributes of higher level entity sets are inherited by lower level entity sets.\n3.\nE.g., Customer & Employee inherit the attributes of Person.\n5.\nParticipation Inheritance\n1.\nIf a parent entity set participates in a relationship then its child entity sets will also participate in that relationship.\n6.\nAggregation\n1.\nHow to show relationships among relationships? - Aggregation is the technique.\n2.\nAbstraction is applied to treat relationships as higher-level entities. We can call it Abstract entity.\n3.\nAvoid redundancy by aggregating relationship as an entity set itself.\nCodeHelp\nLEC-7: Relational Model\n1.\nRelational Model (RM) organises the data in the form of relations (tables).\n2.\nA relational DB consists of collection of tables, each of which is assigned a unique name.\n3.\nA row in a table represents a relationship among a set of values, and table is collection of such relationships.\n4.\nTuple: A single row of the table representing a single data point / a unique record.\n5.\nColumns: represents the attributes of the relation. Each attribute, there is a permitted value, called domain of the\nattribute.\n6.\nRelation Schema: defines the design and structure of the relation, contains the name of the relation and all the\ncolumns/attributes.\n7.\nCommon RM based DBMS systems, aka RDBMS: Oracle, IBM, MySQL, MS Access.\n8.\nDegree of table: number of attributes/columns in a given table/relation.\n9.\nCardinality: Total no. of tuples in a given relation.\n10. Relational Key: Set of attributes which can uniquely identify an each tuple.\n11.\nImportant properties of a Table in Relational Model\n1.\nThe name of relation is distinct among all other relation.\n2.\nThe values have to be atomic. Can\u2019t be broken down further.\n3.\nThe name of each attribute/column must be unique.\n4.\nEach tuple must be unique in a table.\n5.\nThe sequence of row and column has no significance.\n6.\nTables must follow integrity constraints - it helps to maintain data consistency across the tables.\n12. Relational Model Keys\n1.\nSuper Key (SK): Any P&C of attributes present in a table which can uniquely identify each tuple.\n2.\nCandidate Key (CK): minimum subset of super keys, which can uniquely identify each tuple. It contains no\nredundant attribute.\n1.\nCK value shouldn\u2019t be NULL.\n3.\nPrimary Key (PK):\n1.\nSelected out of CK set, has the least no. of attributes.\n4.\nAlternate Key (AK)\n1.\nAll CK except PK.\n5.\nForeign Key (FK)\n1.\nIt creates relation between two tables.\n2.\nA relation, say r1, may include among its attributes the PK of an other relation, say r2. This attribute is called FK\nfrom r1 referencing r2.\n3.\nThe relation r1 is aka Referencing (Child) relation of the FK dependency, and r2 is called Referenced (Parent)\nrelation of the FK.\n4.\nFK helps to cross reference between two different relations.\n6.\nComposite Key: PK formed using at least 2 attributes.\n7.\nCompound Key: PK which is formed using 2 FK.\n8.\nSurrogate Key:\n1.\nSynthetic PK.\n2.\nGenerated automatically by DB, usually an integer value.\n3.\nMay be used as PK.\n13. Integrity Constraints\n1.\nCRUD Operations must be done with some integrity policy so that DB is always consistent.\n2.\nIntroduced so that we do not accidentally corrupt the DB.\n3.\nDomain Constraints\n1.\nRestricts the value in the attribute of relation, specifies the Domain.\n2.\nRestrict the Data types of every attribute.\n3.\nE.g., We want to specify that the enrolment should happen for candidate birth year < 2002.\n4.\nEntity Constraints\n1.\nEvery relation should have PK. PK != NULL.\nCodeHelp\n5.\nReferential Constraints\n1.\nSpecified between two relations & helps maintain consistency among tuples of two relations.\n2.\nIt requires that the value appearing in specified attributes of any tuple in referencing relation also appear in the \nspecified attributes of at least one tuple in the referenced relation.  \n3.\nIf FK in referencing table refers to PK of referenced table then every value of the FK in referencing table must be \nNULL or available in referenced table.\n4.\nFK must have the matching PK for its each value in the parent table or it must be NULL.\n6.\nKey Constraints: The six types of key constraints present in the Database management system are:-\n1.\nNOT NULL:  This constraint will restrict the user from not having a NULL value. It ensures that every element in \nthe database has a value.\n2.\nUNIQUE: It helps us to ensure that all the values consisting in a column are different from each other.\n3.\nDEFAULT: it is used to set the default value to the column. The default value is added to the columns if no value \nis specified for them. \n4.\nCHECK: It is one of the integrity constraints in DBMS. It keeps the check that integrity of data is maintained \nbefore and after the completion of the CRUD.\n5.\nPRIMARY KEY: This is an attribute or set of attributes that can uniquely identify each entity in the entity set. The \nprimary key must contain unique as well as not null values.\n6.\nFOREIGN KEY: Whenever there is some relationship between two entities, there must be some common \nattribute between them. This common attribute must be the primary key of an entity set and will become the \nforeign key of another entity set. This key will prevent every action which can result in loss of connection \nbetween tables.\nCodeHelp\nLEC-8: Transform - ER Model to Relational Model\n1.\nBoth ER-Model and Relational Model are abstract logical representation of real world enterprises. Because the two\nmodels implies the similar design principles, we can convert ER design into Relational design.\n2.\nConverting a DB representation from an ER diagram to a table format is the way we arrive at Relational DB-design from\nan ER diagram.\n3.\nER diagram notations to relations:\n1.\nStrong Entity\n1.\nBecomes an individual table with entity name, attributes becomes columns of the relation.\n2.\nEntity\u2019s Primary Key (PK) is used as Relation\u2019s PK.\n3.\nFK are added to establish relationships with other relations.\n2.\nWeak Entity\n1.\nA table is formed with all the attributes of the entity.\n2.\nPK of its corresponding Strong Entity will be added as FK.\n3.\nPK of the relation will be a composite PK, {FK + Partial discriminator Key}.\n3.\nSingle Values Attributes\n1.\nRepresented as columns directly in the tables/relations.\n4.\nComposite Attributes\n1.\nHandled by creating a separate attribute itself in the original relation for each composite attribute.\n2.\ne.g., Address: {street-name, house-no}, is a composite attribute in customer relation, we add address-street-\nname & address-house-name as new columns in the attribute and ignore \u201caddress\u201d as an attribute.\n5.\nMultivalued Attributes\n1.\nNew tables (named as original attribute name) are created for each multivalued attribute.\n2.\nPK of the entity is used as column FK in the new table.\n3.\nMultivalued attribute\u2019s similar name is added as a column to define multiple values.\n4.\nPK of the new table would be {FK + multivalued name}.\n5.\ne.g., For Strong entity Employee, dependent-name is a multivalued attribute.\n1.\nNew table named dependent-name will be formed with columns emp-id, and dname.\n2.\nPK: {emp-id, name}\n3.\nFK: {emp-id}\n6.\nDerived Attributes: Not considered in the tables.\n7.\nGeneralisation\n1.\nMethod-1: Create a table for the higher level entity set. For each lower-level entity set, create a table that\nincludes a column for each of the attributes of that entity set plus a column for each attribute of the primary key\nof the higher-level entity set.\u2028\nFor e.g., Banking System generalisation of Account - saving & current.\n1.\nTable 1: account (account-number, balance)\n2.\nTable 2: savings-account (account-number, interest-rate, daily-withdrawal-limit)\n3.\nTable 3: current-account (account-number, overdraft-amount, per-transaction-charges)\n2.\nMethod-2: An alternative representation is possible, if the generalisation is disjoint and complete\u2014that is, if no\nentity is a member of two lower-level entity sets directly below a higher-level entity set, and if every entity in\nthe higher level entity set is also a member of one of the lower-level entity sets. Here, do not create a table for\nthe higher-level entity set. Instead, for each lower-level entity set, create a table that includes a column for each\nof the attributes of that entity set plus a column for each attribute of the higher-level entity sets.\u2028\nTables would be:\n1.\nTable 1: savings-account (account-number, balance, interest-rate, daily-withdrawal-limit)\n2.\nTable 2: current-account (account-number, balance, overdraft-amount, per-transaction-charges)\n3.\nDrawbacks of Method-2: If the second method were used for an overlapping generalisation, some values such\nas balance would be stored twice unnecessarily. Similarly, if the generalisation were not complete\u2014that is, if\nsome accounts were neither savings nor current accounts\u2014then such accounts could not be represented with\nthe second method.\n8.\nAggregation\nCodeHelp\n1.\nTable of the relationship set is made.\n2.\nAttributes includes primary keys of entity set and aggregation set\u2019s entities.\n3.\nAlso, add descriptive attribute if any on the relationship.\nCodeHelp\nLEC-9: SQL in 1-Video\n1.\nSQL: Structured Query Language, used to access and manipulate data.\n2.\nSQL used CRUD operations to communicate with DB.\n1.\nCREATE - execute INSERT statements to insert new tuple into the relation.\n2.\nREAD - Read data already in the relations.\n3.\nUPDATE - Modify already inserted data in the relation.\n4.\nDELETE - Delete specific data point/tuple/row or multiple rows.\n3.\nSQL is not DB, is a query language.\n4.\nWhat is RDBMS? (Relational Database Management System)\n1.\nSoftware that enable us to implement designed relational model.\n2.\ne.g., MySQL, MS SQL, Oracle, IBM etc.\n3.\nTable/Relation is the simplest form of data storage object in R-DB.\n4.\nMySQL is open-source RDBMS, and it uses SQL for all CRUD operations\n5.\nMySQL used client-server model, where client is CLI or frontend that used services provided by MySQL server.\n6.\nDifference between SQL and MySQL\n1.\nSQL is Structured Query language used to perform CRUD operations in R-DB, while MySQL is a RDBMS used to\nstore, manage and administrate DB (provided by itself) using SQL.\nSQL DATA TYPES (Ref: https://www.w3schools.com/sql/sql_datatypes.asp)\n1.\nIn SQL DB, data is stored in the form of tables.\n2.\nData can be of different types, like INT, CHAR etc.\nDATATYPE\nDescription\nCHAR\nstring(0-255), string with size = (0, 255], e.g., \nCHAR(251)\nVARCHAR\nstring(0-255)\nTINYTEXT\nString(0-255)\nTEXT\nstring(0-65535)\nBLOB\nstring(0-65535)\nMEDIUMTEXT\nstring(0-16777215)\nMEDIUMBLOB\nstring(0-16777215)\nLONGTEXT\nstring(0-4294967295)\nLONGBLOB\nstring(0-4294967295)\nTINYINT\ninteger(-128 to 127)\nSMALLINT\ninteger(-32768 to 32767)\nMEDIUMINT\ninteger(-8388608 to 8388607)\nINT\ninteger(-2147483648 to 2147483647)\nBIGINT\ninteger (-9223372036854775808 to \n9223372036854775807)\nFLOAT\nDecimal with precision to 23 digits\nDOUBLE\nDecimal with 24 to 53 digits\nDATATYPE\nCodeHelp\n3.\nSize: TINY < SMALL < MEDIUM < INT < BIGINT.\n4.\nVariable length Data types e.g., VARCHAR, are better to use as they occupy space equal to the actual data size.\n5.\nValues can also be unsigned e.g., INT UNSIGNED.\n6.\nTypes of SQL commands:\n1.\nDDL (data definition language): defining relation schema.\n1.\nCREATE: create table, DB, view.\n2.\nALTER TABLE: modification in table structure. e.g, change column datatype or add/remove columns.\n3.\nDROP: delete table, DB, view.\n4.\nTRUNCATE: remove all the tuples from the table.\n5.\nRENAME: rename DB name, table name, column name etc.\n2.\nDRL/DQL (data retrieval language / data query language): retrieve data from the tables.\n1.\nSELECT\n3.\nDML (data modification language): use to perform modifications in the DB\n1.\nINSERT: insert data into a relation\n2.\nUPDATE: update relation data.\n3.\nDELETE: delete row(s) from the relation.\n4.\nDCL (Data Control language): grant or revoke authorities from user.\n1.\nGRANT: access privileges to the DB\n2.\nREVOKE: revoke user access privileges.\n5.\nTCL (Transaction control language): to manage transactions done in the DB\n1.\nSTART TRANSACTION: begin a transaction\n2.\nCOMMIT: apply all the changes and end transaction\n3.\nROLLBACK: discard changes and end transaction\n4.\nSAVEPOINT: checkout within the group of transactions in which to rollback.\nMANAGING DB (DDL)\n1.\nCreation of DB\n1.\nCREATE DATABASE IF NOT EXISTS db-name;\n2.\nUSE db-name; //need to execute to choose on which DB CREATE TABLE etc commands will be executed.\u2028\n//make switching between DBs possible.\n3.\nDROP DATABASE IF EXISTS db-name; //dropping database.\n4.\nSHOW DATABASES; //list all the DBs in the server.\n5.\nSHOW TABLES; //list tables in the selected DB.\nDECIMAL\nDouble stored as string\nDATE\nYYYY-MM-DD\nDATETIME\nYYYY-MM-DD HH:MM:SS\nTIMESTAMP\nYYYYMMDDHHMMSS\nTIME\nHH:MM:SS\nENUM\nOne of the preset values\nSET\nOne or many of the preset values\nBOOLEAN\n0/1\nBIT\ne.g., BIT(n), n upto 64, store values in bits.\nDescription\nDATATYPE\nCodeHelp\nDATA RETRIEVAL LANGUAGE (DRL)\n1.\nSyntax: SELECT <set of column names> FROM <table_name>;\n2.\nOrder of execution from RIGHT to LEFT.\n3.\nQ. Can we use SELECT keyword without using FROM clause?\n1.\nYes, using DUAL Tables.\n2.\nDual tables are dummy tables created by MySQL, help users to do certain obvious actions without referring to user \ndefined tables.\n3.\ne.g., SELECT 55 + 11;\u2028\nSELECT now();\u2028\nSELECT ucase(); etc.\n4.\nWHERE\n1.\nReduce rows based on given conditions.\n2.\nE.g., SELECT * FROM customer WHERE age > 18;\n5.\nBETWEEN\n1.\nSELECT * FROM customer WHERE age between 0 AND 100;\n2.\nIn the above e.g., 0 and 100 are inclusive.\n6.\nIN\n1.\nReduces OR conditions;\n2.\ne.g., SELECT *  FROM officers WHERE officer_name IN ('Lakshay', \u2018Maharana Pratap', \u2018Deepika\u2019);\n7.\nAND/OR/NOT\n1.\nAND: WHERE cond1 AND cond2\n2.\nOR: WHERE cond1 OR cond2\n3.\nNOT: WHERE col_name NOT IN (1,2,3,4);\n8.\nIS NULL\n1.\ne.g., SELECT * FROM customer WHERE prime_status is NULL;\n9.\nPattern Searching / Wildcard (\u2018%\u2019, \u2018_\u2019)\n1.\n\u2018%\u2019, any number of character from 0 to n. Similar to \u2018*\u2019 asterisk in regex.\n2.\n\u2018_\u2019, only one character.\n3.\nSELECT * FROM customer WHERE name LIKE \u2018%p_\u2019;\n10. ORDER BY\n1.\nSorting the data retrieved using WHERE clause.\n2.\nORDER BY <column-name> DESC;\n3.\nDESC = Descending and ASC = Ascending\n4.\ne.g., SELECT * FROM customer ORDER BY name DESC;\n11.\nGROUP BY\n1.\nGROUP BY Clause is used to collect data from multiple records and group the result by one or more column. It is \ngenerally used in a SELECT statement.\n2.\nGroups into category based on column given.\n3.\nSELECT c1, c2, c3 FROM sample_table WHERE cond GROUP BY c1, c2, c3.\n4.\nAll the column names mentioned after SELECT statement shall be repeated in GROUP BY, in order to successfully \nexecute the query.\n5.\nUsed with aggregation functions to perform various actions.\n1.\nCOUNT()\n2.\nSUM()\n3.\nAVG()\n4.\nMIN()\n5.\nMAX()\n12. DISTINCT\n1.\nFind distinct values in the table.\n2.\nSELECT DISTINCT(col_name) FROM table_name;\n3.\nGROUP BY can also be used for the same\n1.\n\u201cSelect col_name from table GROUP BY col_name;\u201d same output as above DISTINCT query.\nCodeHelp\n2.\nSQL is smart enough to realise that if you are using GROUP BY and not using any aggregation function, then\nyou mean \u201cDISTINCT\u201d.\n13. GROUP BY HAVING\n1.\nOut of the categories made by GROUP BY, we would like to know only particular thing (cond).\n2.\nSimilar to WHERE.\n3.\nSelect COUNT(cust_id), country from customer GROUP BY country HAVING COUNT(cust_id) > 50;\n4.\nWHERE vs HAVING\n1.\nBoth have same function of filtering the row base on certain conditions.\n2.\nWHERE clause is used to filter the rows from the table based on specified condition\n3.\nHAVING clause is used to filter the rows from the groups based on the specified condition.\n4.\nHAVING is used after GROUP BY while WHERE is used before GROUP BY clause.\n5.\nIf you are using HAVING, GROUP BY is necessary.\n6.\nWHERE can be used with SELECT, UPDATE & DELETE keywords while GROUP BY used with SELECT.\nCONSTRAINTS (DDL)\n1.\nPrimary Key\n1.\nPK is not null, unique and only one per table."
    },
    {
      "document": "DBMS_Full_Notes.pdf",
      "importance_rank": 3,
      "refined_text": "SUB QUERIES\n1.\nOuter query depends on inner query.\n2.\nAlternative to joins.\n3.\nNested queries.\n4.\nSELECT column_list (s) FROM  table_name  WHERE  column_name OPERATOR \n(SELECT column_list (s)  FROM table_name [WHERE]);\n5.\ne.g., SELECT * FROM table1 WHERE col1 IN (SELECT col1 FROM table1);\n6.\nSub queries exist mainly in 3 clauses\n1.\nInside a WHERE clause.\nJOIN\nSET Operations\nCombines multiple tables based on matching \ncondition.\nCombination is resulting set from two or more \nSELECT statements.\nColumn wise combination.\nRow wise combination.\nData types of two tables can be different.\nDatatypes of corresponding columns from each \ntable should be the same.\nCan generate both distinct or duplicate rows.\nGenerate distinct rows.\nThe number of column(s) selected may or may not \nbe the same from each table.\nThe number of column(s) selected must be the \nsame from each table.\nCombines results horizontally.\nCombines results vertically.\nCodeHelp\n2.\nInside a FROM clause.\n3.\nInside a SELECT clause.\n7.\nSubquery using FROM clause\n1.\nSELECT MAX(rating) FROM (SELECT * FROM movie WHERE country = \u2018India\u2019) as temp;\n8.\nSubquery using SELECT\n1.\nSELECT (SELECT column_list(s) FROM T_name WHERE condition), columnList(s) FROM T2_name WHERE \ncondition;\n9.\nDerived Subquery\n1.\nSELECT columnLists(s) FROM (SELECT columnLists(s) FROM table_name WHERE [condition]) as new_table_name;\n10. Co-related sub-queries\n1.\nWith a normal nested subquery, the inner SELECT query \nruns first and executes once, returning values to be used by \nthe main query. A correlated subquery, however, executes \nonce for each candidate row considered by the outer query. \nIn other words, the inner query is driven by the outer query.\nJOIN VS SUB-QUERIES\nMySQL VIEWS\n1.\nA view is a database object that has no values. Its contents are based on the base table. It contains rows and columns \nsimilar to the real table.\n2.\nIn MySQL, the View is a virtual table created by a query by joining one or more tables. It is operated similarly to the base \ntable but does not contain any data of its own.\n3.\nThe View and table have one main difference that the views are definitions built on top of other tables (or views). If any \nchanges occur in the underlying table, the same changes reflected in the View also.\n4.\nCREATE VIEW view_name AS SELECT columns FROM tables [WHERE conditions];\n5.\nALTER VIEW view_name AS SELECT columns FROM table WHERE conditions;\n6.\nDROP VIEW IF EXISTS view_name;\n7.\nCREATE VIEW Trainer AS SELECT c.course_name, c.trainer, t.email FROM courses c, contact t WHERE c.id = t.id; (View \nusing Join clause).\nNOTE: We can also import/export table schema from files (.csv or json).\nJOINS\nSUBQUERIES\nFaster\nSlower\nJoins maximise calculation burden on DBMS\nKeeps responsibility of calculation on user.\nComplex, difficult to understand and implement\nComparatively easy to understand and implement.\nChoosing optimal join for optimal use case is \ndifficult\nEasy.\nCodeHelp\nLEC-11: Normalisation\n1.\nNormalisation is a step towards DB optimisation.\n2.\nFunctional Dependency (FD)\n1.\nIt's a relationship between the primary key attribute (usually) of the relation to that of the other attribute of the\nrelation.\n2.\nX -> Y, the left side of FD is known as a Determinant, the right side of the production is known as a Dependent.\n3.\nTypes of FD\n1.\nTrivial FD\n1.\nA \u2192 B has trivial functional dependency if B is a subset of A. A->A, B->B are also Trivial FD.\n2.\nNon-trivial FD\n1.\nA \u2192 B has a non-trivial functional dependency if B is not a subset of A. [A intersection B is NULL].\n4.\nRules of FD (Armstrong\u2019s axioms)\n1.\nReflexive\n1.\nIf \u2018A\u2019 is a set of attributes and \u2018B\u2019 is a subset of \u2018A\u2019. Then, A\u2192 B holds.\n2.\nIf A \u2287 B then A \u2192 B.\n2.\nAugmentation\n1.\nIf B can be determined from A, then adding an attribute to this functional dependency won\u2019t change\nanything.\n2.\nIf A\u2192 B holds, then AX\u2192 BX holds too. \u2018X\u2019 being a set of attributes.\n3.\nTransitivity\n1.\nIf A determines B and B determines C, we can say that A determines C.\n2.\nif A\u2192 B and B\u2192 C then A\u2192 C.\n3.\nWhy Normalisation?\n1.\nTo avoid redundancy in the DB, not to store redundant data.\n4.\nWhat happen if we have redundant data?\n1.\nInsertion, deletion and updation anomalies arises.\n5.\nAnomalies\n1.\nAnomalies means abnormalities, there are three types of anomalies introduced by data redundancy.\n2.\nInsertion anomaly\n1.\nWhen certain data (attribute) can not be inserted into the DB without the presence of other data.\n3.\nDeletion anomaly\n1.\nThe delete anomaly refers to the situation where the deletion of data results in the unintended loss of some\nother important data.\n4.\nUpdation anomaly (or modification anomaly)\n1.\nThe update anomaly is when an update of a single data value requires multiple rows of data to be updated.\n2.\nDue to updation to many places, may be Data inconsistency arises, if one forgets to update the data at all the\nintended places.\n5.\nDue to these anomalies, DB size increases and DB performance become very slow.\n6.\nTo rectify these anomalies and the effect of these of DB, we use Database optimisation technique called\nNORMALISATION.\n6.\nWhat is Normalisation?\n1.\nNormalisation is used to minimise the redundancy from a relations. It is also used to eliminate undesirable\ncharacteristics like Insertion, Update, and Deletion Anomalies.\n2.\nNormalisation divides the composite attributes into individual attributes OR larger table into smaller and links them\nusing relationships.\n3.\nThe normal form is used to reduce redundancy from the database table.\n7.\nTypes of Normal forms\n1.\n1NF\n1.\nEvery relation cell must have atomic value.\n2.\nRelation must not have multi-valued attributes.\nCodeHelp\n2.\n2NF\n1.\nRelation must be in 1NF.\n2.\nThere should not be any partial dependency.\n1.\nAll non-prime attributes must be fully dependent on PK.\n2.\nNon prime attribute can not depend on the part of the PK.\n3.\n3NF\n1.\nRelation must be in 2NF.\n2.\nNo transitivity dependency exists.\n1.\nNon-prime attribute should not find a non-prime attribute.\n4.\nBCNF (Boyce-Codd normal form)\n1.\nRelation must be in 3NF.\n2.\nFD: A -> B, A must be a super key.\n1.\nWe must not derive prime attribute from any prime or non-prime attribute.\n8.\nAdvantages of Normalisation\n1.\nNormalisation helps to minimise data redundancy.\n2.\nGreater overall database organisation.\n3.\nData consistency is maintained in DB.\nCodeHelp\nLEC-12: Transaction\n1.\nTransaction\n1.\nA unit of work done against the DB in a logical sequence.\n2.\nSequence is very important in transaction.\n3.\nIt is a logical unit of work that contains one or more SQL statements. The result of all these statements in a\ntransaction either gets completed successfully (all the changes made to the database are permanent) or if at any\npoint any failure happens it gets rollbacked (all the changes being done are undone.)\n2.\nACID Properties\n1.\nTo ensure integrity of the data, we require that the DB system maintain the following properties of the transaction.\n2.\nAtomicity\n1.\nEither all operations of transaction are reflected properly in the DB, or none are.\n3.\nConsistency\n1.\nIntegrity constraints must be maintained before and after transaction.\n2.\nDB must be consistent after transaction happens.\n4.\nIsolation\n1.\nEven though multiple transactions may execute concurrently, the system guarantees that, for every pair of\ntransactions Ti and Tj, it appears to Ti that either Tj finished execution before Ti started, or Tj started execution\nafter Ti finished. Thus, each transaction is unaware of other transactions executing concurrently in the system.\n2.\nMultiple transactions can happen in the system in isolation, without interfering each other.\n5.\nDurability\n1.\nAfter transaction completes successfully, the changes it has made to the database persist, even if there are\nsystem failures.\n3.\nTransaction states\n1.\nActive state\n1.\nThe very first state of the life cycle of the transaction, all the read and write operations are being\nperformed. If they execute without any error the T comes to Partially committed state. Although if any\nerror occurs then it leads to a Failed state.\n2.\nPartially committed state\n1.\nAfter transaction is executed the changes are saved in the buffer in the main memory. If the changes made\nare permanent on the DB then the state will transfer to the committed state and if there is any failure, the T\nwill go to Failed state.\n3.\nCommitted state\nCodeHelp\n1.\nWhen updates are made permanent on the DB. Then the T is said to be in the committed state. Rollback \ncan\u2019t be done from the committed states. New consistent state is achieved at this stage.\n4.\nFailed state\n1.\nWhen T is being executed and some failure occurs. Due to this it is impossible to continue the execution of \nthe T.\n5.\nAborted state\n1.\nWhen T reaches the failed state, all the changes made in the buffer are reversed. After that the T rollback \ncompletely. T reaches abort state after rollback. DB\u2019s state prior to the T is achieved.\n6.\nTerminated state\n1.\nA transaction is said to have terminated if has either committed or aborted.\nCodeHelp\nLEC-13: How to implement Atomicity and Durability in Transactions\n1.\nRecovery Mechanism Component of DBMS supports atomicity and durability.\n2.\nShadow-copy scheme\n1.\nBased on making copies of DB (aka, shadow copies).\n2.\nAssumption only one Transaction (T) is active at a time.\n3.\nA pointer called db-pointer is maintained on the disk; which at any instant points to current copy of DB.\n4.\nT, that wants to update DB first creates a complete copy of DB.\n5.\nAll further updates are done on new DB copy leaving the original copy (shadow copy) untouched.\n6.\nIf at any point the T has to be aborted the system deletes the new copy. And the old copy is not affected.\n7.\nIf T success, it is committed as,\n1.\nOS makes sure all the pages of the new copy of DB written on the disk.\n2.\nDB system updates the db-pointer to point to the new copy of DB.\n3.\nNew copy is now the current copy of DB.\n4.\nThe old copy is deleted.\n5.\nThe T is said to have been COMMITTED at the point where the updated db-pointer is written to disk.\n8.\nAtomicity\n1.\nIf T fails at any time before db-pointer is updated, the old content of DB are not affected.\n2.\nT abort can be done by just deleting the new copy of DB.\n3.\nHence, either all updates are reflected or none.\n9.\nDurability\n1.\nSuppose, system fails are any time before the updated db-pointer is written to disk.\n2.\nWhen the system restarts, it will read db-pointer & will thus, see the original content of DB and none of the effects of T will \nbe visible.\n3.\nT is assumed to be successful only when db-pointer is updated.\n4.\nIf system fails after db-pointer has been updated. Before that all the pages of the new copy were written to disk. Hence, \nwhen system restarts, it will read new DB copy.\n10. The implementation is dependent on write to the db-pointer being atomic. Luckily, disk system provide atomic updates to entire \nblock or at least a disk sector. So, we make sure db-pointer lies entirely in a single sector. By storing db-pointer at the beginning \nof a block.\n11.\nInefficient, as entire DB is copied for every Transaction.\n3.\nLog-based recovery methods\n1.\nThe log is a sequence of records. Log of each transaction is maintained in some stable storage so that if any failure occurs, then \nit can be recovered from there.\n2.\nIf any operation is performed on the database, then it will be recorded in the log.\n3.\nBut the process of storing the logs should be done before the actual transaction is applied in the database.\n4.\nStable storage is a classification of computer data storage technology that guarantees atomicity for any given write operation \nand allows software to be written that is robust against some hardware and power failures.\n5.\nDeferred DB Modifications\n1.\nEnsuring atomicity by recording all the DB modifications in the log but deferring the execution of all the write operations \nuntil the final action of the T has been executed.\n2.\nLog information is used to execute deferred writes when T is completed.\n3.\nIf system crashed before the T completes, or if T is aborted, the information in the logs are ignored.\n4.\nIf T completes, the records associated to it in the log file are used in executing the deferred writes.\n5.\nIf failure occur while this updating is taking place, we preform redo.\n6.\nImmediate DB Modifications\n1.\nDB modifications to be output to the DB while the T is still in active state.\n2.\nDB modifications written by active T are called uncommitted modifications.\n3.\nIn the event of crash or T failure, system uses old value field of the log records to restore modified values.\n4.\nUpdate takes place only after log records in a stable storage.\n5.\nFailure handling\n1.\nSystem failure before T completes, or if T aborted, then old value field is used to undo the T.\n2.\nIf T completes and system crashes, then new value field is used to redo T having commit logs in the logs.\nCodeHelp\nLEC-14: Indexing in DBMS\n1.\nIndexing is used to optimise the performance of a database by minimising the number of disk accesses required when a query is \nprocessed.\n2.\nThe index is a type of data structure. It is used to locate and access the data in a database table quickly.\n3.\nSpeeds up operation with read operations like SELECT queries, WHERE clause etc.\n4.\nSearch Key: Contains copy of primary key or candidate key \nof the table or something else.\n5.\nData Reference: Pointer holding the address of disk block \nwhere the value of the corresponding key is stored.\n6.\nIndexing is optional, but increases access speed. It is not the \nprimary mean to access the tuple, it is the secondary mean.\n7.\nIndex file is always sorted.\n8.\nIndexing Methods\n1.\nPrimary Index (Clustering Index)\n1.\nA file may have several indices, on different search keys. If the data file containing the records is sequentially ordered, a \nPrimary index is an index whose search key also defines the sequential order of the file.\n2.\nNOTE: The term primary index is sometimes used to mean an index on a primary key. However, such usage is \nnonstandard and should be avoided.\n3.\nAll files are ordered sequentially on some search key. It could be Primary Key or non-primary key.\n4.\nDense And Sparse Indices\n1.\nDense Index\n1.\nThe dense index contains an index record for every search key value in the data file.\n2.\nThe index record contains the search-key value and a pointer to the first data record with that search-key value. \nThe rest of the records with the same search-key value would be stored sequentially after the first record.\n3.\nIt needs more space to store index record itself. The index records have the search key and a pointer to the actual \nrecord on the disk.\n2.\nSparse Index\n1.\nAn index record appears for only some of the search-key values.\n2.\nSparse Index helps you to resolve the issues of dense Indexing in DBMS. In this method of indexing technique, a \nrange of index columns stores the same data block address, and when data needs to be retrieved, the block \naddress will be fetched.\n5.\nPrimary Indexing can be based on Data file is sorted w.r.t Primary Key attribute or non-key attributes.\n6.\nBased on Key attribute\n1.\nData file is sorted w.r.t primary key attribute.\n2.\nPK will be used as search-key in Index.\n3.\nSparse Index will be formed i.e., no. of entries in the index file = no. of blocks in datafile.\n7.\nBased on Non-Key attribute\n1.\nData file is sorted w.r.t non-key attribute.\n2.\nNo. Of entries in the index  = unique non-key attribute value in the data file.\n3.\nThis is dense index as, all the unique values have an entry in the \nindex file.\n4.\nE.g., Let\u2019s assume that a company recruited many employees in \nvarious departments. In this case, clustering indexing in DBMS \nshould be created for all employees who belong to the same \ndept.\n8.\nMulti-level Index\n1.\nIndex with two or more levels.\n2.\nIf the single level index become enough large that the binary \nsearch it self would take much time, we can break down \nindexing into multiple levels.\n2.\nSecondary Index (Non-Clustering Index)\n1.\nDatafile is unsorted. Hence, Primary Indexing is not possible.\n2.\nCan be done on key or non-key attribute.\n3.\nCalled secondary indexing because normally one indexing is already \napplied.\n4.\nNo. Of entries in the index file = no. of records in the data file.\n5.\nIt's an example of Dense index.\nCodeHelp\n9.\nAdvantages of Indexing\n1.\nFaster access and retrieval of data.\n2.\nIO is less.\n10. Limitations of Indexing\n1.\nAdditional space to store index table\n2.\nIndexing Decrease performance in INSERT, DELETE, and UPDATE query.\n\u2028\nCodeHelp\nLEC-15: NoSQL\n1.\nNoSQL databases (aka \"not only SQL\") are non-tabular databases and store data differently than relational tables. NoSQL databases \ncome in a variety of types based on their data model. The main types are document, key-value, wide-column, and graph. They \nprovide flexible schemas and scale easily with large amounts of data and high user loads.\n1.\nThey are schema free.\n2.\nData structures used are not tabular, they are more flexible, has the ability to adjust dynamically.\n3.\nCan handle huge amount of data (big data).\n4.\nMost of the NoSQL are open sources and has the capability of horizontal scaling.\n5.\nIt just stores data in some format other than relational.\n2.\nHistory behind NoSQL\n1.\nNoSQL databases emerged in the late 2000s as the cost of storage dramatically decreased. Gone were the days of needing to \ncreate a complex, difficult-to-manage data model in order to avoid data duplication. Developers (rather than storage) were \nbecoming the primary cost of software development, so NoSQL databases optimised for developer productivity.\n2.\nData becoming unstructured more, hence structuring (defining schema in advance) them had becoming costly.\n3.\nNoSQL databases allow developers to store huge amounts of unstructured data, giving them a lot of flexibility.\n4.\nRecognising the need to rapidly adapt to changing requirements in a software system. Developers needed the ability to iterate \nquickly and make changes throughout their software stack \u2014 all the way down to the database. NoSQL databases gave them \nthis flexibility.\n5.\nCloud computing also rose in popularity, and developers began using public clouds to host their applications and data. They \nwanted the ability to distribute data across multiple servers and regions to make their applications resilient, to scale out instead \nof scale up, and to intelligently geo-place their data. Some NoSQL databases like MongoDB provide these capabilities.\n3.\nNoSQL Databases Advantages\nA.\nFlexible Schema\n1.\nRDBMS has pre-defined schema, which become an issue when we do not have all the data with us or we need to change \nthe schema. It's a huge task to change schema on the go.\nB.\nHorizontal Scaling\n1.\nHorizontal scaling, also known as scale-out, refers to bringing on additional nodes to share the load. This is difficult with \nrelational databases due to the difficulty in spreading out related data across nodes. With non-relational databases, this is \nmade simpler since collections are self-contained and not coupled relationally. This allows them to be distributed across \nnodes more simply, as queries do not have to \u201cjoin\u201d them together across nodes.\n2.\nScaling horizontally is achieved through Sharding OR Replica-sets.\nC.\nHigh Availability\n1.\nNoSQL databases are highly available due to its auto replication feature i.e. whenever any kind of failure happens data \nreplicates itself to the preceding consistent state.\n2.\nIf a server fails, we can access that data from another server as well, as in NoSQL database data is stored at multiple \nservers.\nD.\nEasy insert and read operations.\n1.\nQueries in NoSQL databases can be faster than SQL databases. Why? Data in SQL databases is typically normalised, so \nqueries for a single object or entity require you to join data from multiple tables. As your tables grow in size, the joins can \nbecome expensive. However, data in NoSQL databases is typically stored in a way that is optimised for queries. The rule of \nthumb when you use MongoDB is data that is accessed together should be stored together. Queries typically do not require \njoins, so the queries are very fast.\n2.\nBut difficult delete or update operations.\nE.\nCaching mechanism.\nF.\nNoSQL use case is more for Cloud applications.\n4.\nWhen to use NoSQL?\n1.\nFast-paced Agile development\n2.\nStorage of structured and semi-structured data\n3.\nHuge volumes of data\n4.\nRequirements for scale-out architecture\n5.\nModern application paradigms like micro-services and real-time streaming.\n5.\nNoSQL DB Misconceptions\n1.\nRelationship data is best suited for relational databases.\n1.\nA common misconception is that NoSQL databases or non-relational databases don\u2019t store relationship data well. NoSQL \ndatabases can store relationship data \u2014 they just store it differently than relational databases do. In fact, when compared \nwith relational databases, many find modelling relationship data in NoSQL databases to be easier than in relational \ndatabases, because related data doesn\u2019t have to be split between tables. NoSQL data models allow related data to be \nnested within a single data structure.\n2.\nNoSQL databases don't support ACID transactions.\nCodeHelp\n1.\nAnother common misconception is that NoSQL databases don't support ACID transactions. Some NoSQL databases like \nMongoDB do, in fact, support ACID transactions.\n6.\nTypes of NoSQL Data Models\n1.\nKey-Value Stores\n1.\nThe simplest type of NoSQL database is a key-value store. Every data element in the database is stored as a key value pair \nconsisting of an attribute name (or \"key\") and a value. In a sense, a key-value store is like a relational database with only \ntwo columns: the key or attribute name (such as \"state\") and the value (such as \"Alaska\").\n2.\nUse cases include shopping carts, user preferences, and user profiles.\n3.\ne.g., Oracle NoSQL, Amazon DynamoDB, MongoDB also supports Key-Value store, Redis.\n4.\nA key-value database associates a value (which can be anything from a number or simple string to a complex object) with \na key, which is used to keep track of the object. In its simplest form, a key-value store is like a dictionary/array/map object \nas it exists in most programming paradigms, but which is stored in a persistent way and managed by a Database \nManagement System (DBMS).\n5.\nKey-value databases use compact, efficient index structures to be able to quickly and reliably locate a value by its key, \nmaking them ideal for systems that need to be able to find and retrieve data in constant time.\n6.\nThere are several use-cases where choosing a key value store approach is an optimal solution:\na)\nReal time random data access, e.g., user session attributes in an online application such as gaming or finance.\nb)\nCaching mechanism for frequently accessed data or configuration based on keys.\nc)\nApplication is designed on simple key-based queries.\n2.\nColumn-Oriented / Columnar / C-Store / Wide-Column\n1.\nThe data is stored such that each row of a column will be next to other rows from that same column.\n2.\nWhile a relational database stores data in rows and reads data row by row, a column store is organised as a set of columns. \nThis means that when you want to run analytics on a small number of columns, you can read those columns directly \nwithout consuming memory with the unwanted data. Columns are often of the same type and benefit from more efficient \ncompression, making reads even faster. Columnar databases can quickly aggregate the value of a given column (adding up \nthe total sales for the year, for example). Use cases include analytics.\n3.\ne.g., Cassandra, RedShift, Snowflake.\n3.\nDocument Based Stores\n1.\nThis DB store data in documents similar to JSON (JavaScript Object Notation) objects. Each document contains pairs of \nfields and values. The values can typically be a variety of types including things like strings, numbers, booleans, arrays, or \nobjects.\n2.\nUse cases include e-commerce platforms, trading platforms, and mobile app development across industries.\n3.\nSupports ACID properties hence, suitable for Transactions.\n4.\ne.g., MongoDB, CouchDB.\n4.\nGraph Based Stores\n1.\nA graph database focuses on the relationship between data elements. Each element is stored as a node (such as a person \nin a social media graph). The connections between elements are called links or relationships. In a graph database, \nconnections are first-class elements of the database, stored directly. In relational databases, links are implied, using data to \nexpress the relationships.\n2.\nA graph database is optimised to capture and search the connections between data elements, overcoming the overhead \nassociated with JOINing multiple tables in SQL.\n3.\nVery few real-world business systems can survive solely on graph queries. As a result graph databases are usually run \nalongside other more traditional databases.\n4.\nUse cases include fraud detection, social networks, and knowledge graphs.\n7.\nNoSQL Databases Dis-advantages\n1.\nData Redundancy\n1.\nSince data models in NoSQL databases are typically optimised for queries and not for reducing data duplication, NoSQL \ndatabases can be larger than SQL databases. Storage is currently so cheap that most consider this a minor drawback, and \nsome NoSQL databases also support compression to reduce the storage footprint.\n2.\nUpdate & Delete operations are costly.\n3.\nAll type of NoSQL Data model doesn\u2019t fulfil all of your application needs\n1.\nDepending on the NoSQL database type you select, you may not be able to achieve all of your use cases in a single \ndatabase. For example, graph databases are excellent for analysing relationships in your data but may not provide what \nyou need for everyday retrieval of the data such as range queries. When selecting a NoSQL database, consider what your \nuse cases will be and if a general purpose database like MongoDB would be a better option.\n4.\nDoesn\u2019t support ACID properties in general.\n5.\nDoesn\u2019t support data entry with consistency constraints.\nCodeHelp\n8.\nSQL vs NoSQL\n\u2028\nSQL Databases\nNoSQL Databases\nData Storage Model\nTables with fixed rows and \ncolumns\nDocument: JSON documents, \nKey-value: key-value pairs, Wide-\ncolumn: tables with rows and \ndynamic columns, Graph: nodes \nand edges\nDevelopment History\nDeveloped in the 1970s with a \nfocus on reducing data \nduplication\nDeveloped in the late 2000s with \na focus on scaling and allowing \nfor rapid application change \ndriven by agile and DevOps \npractices.\nExamples\nOracle, MySQL, Microsoft SQL \nServer, and PostgreSQL\nDocument: MongoDB and \nCouchDB, Key-value: Redis and \nDynamoDB, Wide-column: \nCassandra and HBase, Graph: \nNeo4j and Amazon Neptune\nPrimary Purpose\nGeneral Purpose\nDocument: general purpose, Key-\nvalue: large amounts of data with \nsimple lookup queries, Wide-\ncolumn: large amounts of data \nwith predictable query patterns, \nGraph: analyzing and traversing \nrelationships between connected \ndata\nSchemas\nFixed\nFlexible\nScaling\nVertical (Scale-up)\nHorizontal (scale-out across \ncommodity servers)\nACID Properties\nSupported\nNot Supported, except in DB like \nMongoDB etc.\nJOINS\nTypically Required\nTypically not required\nData to object mapping\nRequired object-relational \nmapping\nMany do not require ORMs. \nMongoDB documents map \ndirectly to data structures in most \npopular programming languages.\nCodeHelp\nLEC-16: Types of Databases\n1.\nRelational Databases\n1.\nBased on Relational Model.\n2.\nRelational databases are quite popular, even though it was a system designed in the 1970s. Also known as relational database \nmanagement systems (RDBMS), relational databases commonly use Structured Query Language (SQL) for operations such as \ncreating, reading, updating, and deleting data. Relational databases store information in discrete tables, which can be JOINed \ntogether by fields known as foreign keys. For example, you might have a User table which contains information about all your \nusers, and join it to a Purchases table, which contains information about all the purchases they\u2019ve made. MySQL, Microsoft SQL \nServer, and Oracle are types of relational databases.\n3.\nthey are ubiquitous, having acquired a steady user base since the 1970s\n4.\nthey are highly optimised for working with structured data.\n5.\nthey provide a stronger guarantee of data normalisation\n6.\nthey use a well-known querying language through SQL\n7.\nScalability issues (Horizontal Scaling).\n8.\nData become huge, system become more complex.\n2.\nObject Oriented Databases\n1.\nThe object-oriented data model, is based on the object-oriented-programming paradigm, which is now in wide use. \nInheritance, object-identity, and encapsulation (information hiding), with methods to provide an interface to objects, are \namong the key concepts of object-oriented programming that have found applications in data modelling. The object-oriented \ndata model also supports a rich type system, including structured and collection types. While inheritance and, to some extent, \ncomplex types are also present in the E-R model, encapsulation and object-identity distinguish the object-oriented data model \nfrom the E-R model.\n2.\nSometimes the database can be very complex, having multiple relations. So, maintaining a relationship between them can be \ntedious at times.\n1.\nIn Object-oriented databases data is treated as an object.\n2.\nAll bits of information come in one instantly available object package instead of multiple tables.\n3.\nAdvantages\n1.\nData storage and retrieval is easy and quick.\n2.\nCan handle complex data relations and more variety of data types that standard relational databases.\n3.\nRelatively friendly to model the advance real world problems\n4.\nWorks with functionality of OOPs and Object Oriented languages.\n4.\nDisadvantages\n1.\nHigh complexity causes performance issues like read, write, update and delete operations are slowed down.\n2.\nNot much of a community support as isn\u2019t widely adopted as relational databases.\n3.\nDoes not support views like relational databases.\n5.\ne.g., ObjectDB, GemStone etc.\n3.\nNoSQL Databases\n1.\nNoSQL databases (aka \"not only SQL\") are non-tabular databases and store data differently than relational tables. NoSQL \ndatabases come in a variety of types based on their data model. The main types are document, key-value, wide-column, and \ngraph. They provide flexible schemas and scale easily with large amounts of data and high user loads.\n2.\nThey are schema free.\n3.\nData structures used are not tabular, they are more flexible, has the ability to adjust dynamically.\n4.\nCan handle huge amount of data (big data).\n5.\nMost of the NoSQL are open sources and has the capability of horizontal scaling.\n6.\nIt just stores data in some format other than relational.\n7.\nRefer LEC-15 notes\u2026\n4.\nHierarchical Databases\n1.\nAs the name suggests, the hierarchical database model is most appropriate for use cases in which the main focus of information \ngathering is based on a concrete hierarchy, such as several individual employees reporting to a single department at a \ncompany.\n2.\nThe schema for hierarchical databases is defined by its tree-like organisation, in which there is typically a root \u201cparent\u201d \ndirectory of data stored as records that links to various other subdirectory branches, and each subdirectory branch, or child \nrecord, may link to various other subdirectory branches.\n3.\nThe hierarchical database structure dictates that, while a parent record can have several child records, each child record can only \nhave one parent record. Data within records is stored in the form of fields, and each field can only contain one value. Retrieving \nhierarchical data from a hierarchical database architecture requires traversing the entire tree, starting at the root node.\n4.\nSince the disk storage system is also inherently a hierarchical structure, these models can also be used as physical models.\n5.\nThe key advantage of a hierarchical database is its ease of use. The one-to-many organisation of data makes traversing the \ndatabase simple and fast, which is ideal for use cases such as website drop-down menus or computer folders in systems like \nCodeHelp\nMicrosoft Windows OS. Due to the separation of the tables from physical storage structures, information can easily be added or \ndeleted without affecting the entirety of the database. And most major programming languages offer functionality for reading \ntree structure databases.\n6.\nThe major disadvantage of hierarchical databases is their inflexible nature. The one-to-many structure is not ideal for complex \nstructures as it cannot describe relationships in which each child node has multiple parents nodes. Also the tree-like \norganisation of data requires top-to-bottom sequential searching, which is time consuming, and requires repetitive storage of \ndata in multiple different entities, which can be redundant.\n7.\ne.g., IBM IMS.\n5.\nNetwork Databases\n1.\nExtension of Hierarchical databases\n2.\nThe child records are given the freedom to associate with multiple parent records.\n3.\nOrganised in a Graph structure.\n4.\nCan handle complex relations.\n5.\nMaintenance is tedious.\n6.\nM:N links may cause slow retrieval.\n7.\nNot much web community support.\n8.\ne.g., Integrated Data Store (IDS), IDMS (Integrated Database Management System), \nRaima Database Manager, TurboIMAGE etc.\nCodeHelp\nLEC-17: Clustering in DBMS\n1.\nDatabase Clustering (making Replica-sets) is the process of combining more than one servers or instances connecting a single database. \nSometimes one server may not be adequate to manage the amount of data or the number of requests, that is when a Data Cluster is needed. \nDatabase clustering, SQL server clustering, and SQL clustering are closely associated with SQL is the language used to manage the database \ninformation.\n2.\nReplicate the same dataset on different servers.\n3.\nAdvantages\n1.\nData Redundancy: Clustering of databases helps with data redundancy, as we store the same data at multiple servers. Don\u2019t confuse this \ndata redundancy as repetition of the same data that might lead to some anomalies. The redundancy that clustering offers is required and is \nquite certain due to the synchronisation. In case any of the servers had to face a failure due to any possible reason, the data is available at other \nservers to access.\n2.\nLoad balancing: or scalability doesn\u2019t come by default with the database. It has to be brought by clustering regularly. It also depends on the \nsetup. Basically, what load balancing does is allocating the workload among the different servers that are part of the cluster. This indicates that \nmore users can be supported and if for some reasons if a huge spike in the traffic appears, there is a higher assurance that it will be able to \nsupport the new traffic. One machine is not going to get all of the hits. This can provide scaling seamlessly as required. This links directly to \nhigh availability. Without load balancing, a particular machine could get overworked and traffic would slow down, leading to decrement of \nthe traffic to zero.\n3.\nHigh availability: When you can access a database, it implies that it is available. High availability refers the amount of time a database is \nconsidered available. The amount of availability you need greatly depends on the number of transactions you are running on your database \nand how often you are running any kind of analytics on your data. With database clustering, we can reach extremely high levels of availability \ndue to load balancing and have extra machines. In case a server got shut down the database will, however, be available.\n4.\nHow does Clustering Work?\n1.\nIn cluster architecture, all requests are split with many computers so that an individual user request is executed and produced by a number of \ncomputer systems. The clustering is serviceable definitely by the ability of load balancing and high-availability. If one node collapses, the \nrequest is handled by another node. Consequently, there are few or no possibilities of absolute system failures.\nCodeHelp\nLEC-18: Partitioning & Sharding in DBMS (DB Optimisation) \n1.\nA big problem can be solved easily when it is chopped into several smaller sub-problems. That is what the partitioning technique does. It divides a \nbig database containing data metrics and indexes into smaller and handy slices of data called partitions. The partitioned tables are directly used by \nSQL queries without any alteration. Once the database is partitioned, the data definition language can easily work on the smaller partitioned slices, \ninstead of handling the giant database altogether. This is how partitioning cuts down the problems in managing large database tables.\n2.\nPartitioning is the technique used to divide stored database objects into separate servers. Due to this, there is an increase in performance, \ncontrollability of the data. We can manage huge chunks of data optimally. When we horizontally scale our machines/servers, we know that it gives us \na challenging time dealing with relational databases as it\u2019s quite tough to maintain the relations. But if we apply partitioning to the database that is \nalready scaled out i.e. equipped with multiple servers, we can partition our database among those servers and handle the big data easily.\n3.\nVertical Partitioning\n1.\nSlicing relation vertically / column-wise.\n2.\nNeed to access different servers to get complete tuples.\n4.\nHorizontal Partitioning\n1.\nSlicing relation horizontally / row-wise.\n2.\nIndependent chunks of data tuples are stored in different servers.\n5.\nWhen Partitioning is Applied?\n1.\nDataset become much huge that managing and dealing with it become a tedious task.\n2.\nThe number of requests are enough larger that the single DB server access is taking huge time and hence the system\u2019s response time become \nhigh.\n6.\nAdvantages of Partitioning\n1.\nParallelism\n2.\nAvailability\n3.\nPerformance\n4.\nManageability\n5.\nReduce Cost, as scaling-up or vertical scaling might be costly.\n7.\nDistributed Database\n1.\nA single logical database that is, spread across multiple locations (servers) and logically interconnected by network.\n2.\nThis is the product of applying DB optimisation techniques like Clustering, Partitioning and Sharding.\n3.\nWhy this is needed? READ Point 5.\n8.\nSharding\n1.\nTechnique to implement Horizontal Partitioning.\n2.\nThe fundamental idea of Sharding is the idea that instead of having all the data sit on one DB instance, we split it up and introduce a \nRouting layer so that we can forward the request to the right instances that actually contain the data.\n3.\nPros\n1.\nScalability\n2.\nAvailability\n4.\nCons\n1.\n Complexity, making partition mapping, Routing layer to be implemented in the system, Non-uniformity that creates the necessity of Re-\nSharding\n2.\nNot well suited for Analytical type of queries, as the data is spread across different DB instances. (Scatter-Gather problem)\nCodeHelp\n- Lakshay\nDatabase Scaling Patterns\nStep by Step Scaling\nCodeHelp\nWhat will you learn?\n\u2022 Step by Step manner, when to choose which Scaling option. \n\u2022 Which Scaling option is feasible practically at the moment.\nCodeHelp\nA Case Study\nCab Booking APP\n\u2022 Tiny startup. \n\u2022 ~10 customers onboard. \n\u2022 A single small machine DB stores all customers, trips, locations, booking \ndata, and customer trip history. \n\u2022 ~1 trip booking in 5 mins.\nCodeHelp\nYour App becoming famous, but\u2026\nThe PROBLEM begins\n\u2022 Requests scales upto 30 bookings per minute. \n\u2022 Your tiny DB system has started performing poorly. \n\u2022 API latency has increased a lot. \n\u2022 Transactions facing Deadlock, Starvation, and frequent failure. \n\u2022 Sluggish App experience. \n\u2022 Customer dis-satisfaction.\nCodeHelp\nIs there any solution?\n\u2022 We have to apply some kind of performance optimisation measures. \n\u2022 We might have to scale our system going forward.\nCodeHelp\nPattern 1\nQuery Optimisation & Connection Pool Implementation\n\u2022 Cache frequently used non-dynamic data like, booking history, payment \nhistory, user profiles etc. \n\u2022 Introduce Database Redundancy. (Or may be use NoSQL) \n\u2022 Use connection pool libraries to Cache DB connections. \n\u2022 Multiple application threads can use same DB connection. \n\u2022 Good optimisations as of now. \n\u2022 Scaled the business to one more city, and now getting ~100 booking per \nminute.\nCodeHelp\nPattern 2\nVertical Scaling or Scale-up\n\u2022 Upgrading our initial tiny machine. \n\u2022 RAM by 2x and SSD by 3x etc. \n\u2022 Scale up is pocket friendly till a point only. \n\u2022 More you scale up, cost increases exponentially. \n\u2022 Good Optimisation as of now. \n\u2022 Business is growing, you decided to scale it to 3 more cities and now getting \n300 booking per minute.\nCodeHelp\nPattern 3\nCommand Query Responsibility Segregation (CQRS)\n\u2022 The scaled up big machine is not able to handle all read/write requests. \n\u2022 Separate read/write operations physical machine wise. \n\u2022 2 more machines as replica to the primary machine. \n\u2022 All read queries to replicas. \n\u2022 All write queries to primary. \n\u2022 Business is growing, you decided to scale it to 2 more cities. \n\u2022 Primary is not able to handle all write requests. \n\u2022 Lag between primary and replica is impacting user experience.\nCodeHelp\nPattern 4\nMulti Primary Replication\n\u2022 Why not distribute write request to replica also? \n\u2022 All machines can work as primary & replica. \n\u2022 Multi primary configuration is a logical circular ring. \n\u2022 Write data to any node. \n\u2022 Read data from any node that replies to the broadcast first. \n\u2022 You scale to 5 more cities & your system is in pain again. (~50 req/s)\nCodeHelp\nPattern 5\nPartitioning of Data by Functionality\n\u2022 What about separating the location tables in separate DB schema? \n\u2022 What about putting that DB in separate machines with primary-replica or \nmulti-primary configuration? \n\u2022 Different DB can host data categorised by different functionality. \n\u2022 Backend or application layer has to take responsibility to join the results. \n\u2022 Planning to expand your business to other country.\nCodeHelp\nPattern 6\nHorizontal Scaling or Scale-out\n\u2022 Sharding - multiple shards. \n\u2022 Allocate 50 machines - all having same DB schema - each machine just hold \na part of data. \n\u2022 Locality of data should be there. \n\u2022 Each machine can have their own replicas, may be used in failure recovery. \n\u2022 Sharding is generally hard to apply. But \u201cNo Pain, No Gain\u201d \n\u2022 Scaling the business across continents.\nCodeHelp\nPattern 7\nData Centre Wise Partition\n\u2022 Requests travelling across continents are having high latency. \n\u2022 What about distributing traffic across data centres? \n\u2022 Data centres across continents. \n\u2022 Enable cross data centre replication which helps disaster recovery. \n\u2022 This always maintain Availability of your system. \n\u2022 Now, Plan for an IPO :p\nCodeHelp\nLEC-20: CAP Theorem\n1.\nBasic and one of the most important concept in Distributed Databases.\n2.\nUseful to know this to design efficient distributed system for your given business logic. \n3.\nLet\u2019s first breakdown CAP\n1.\nConsistency: In a consistent system, all nodes see the same data simultaneously. If we perform a read operation on a consistent system, it \nshould return the value of the most recent write operation. The read should cause all nodes to return the same data. All users see the same data \nat the same time, regardless of the node they connect to. When data is written to a single node, it is then replicated across the other nodes in \nthe system.\n2.\nAvailability: When availability is present in a distributed system, it means that the system remains operational all of the time. Every request \nwill get a response regardless of the individual state of the nodes. This means that the system will operate even if there are multiple nodes \ndown. Unlike a consistent system, there\u2019s no guarantee that the response will be the most recent write operation.\n3.\nPartition Tolerance: When a distributed system encounters a partition, it means that there\u2019s a break in communication between nodes. If a \nsystem is partition-tolerant, the system does not fail, regardless of whether messages are dropped or delayed between nodes within the \nsystem. To have partition tolerance, the system must replicate records across combinations of nodes and networks. \n4.\nWhat does the CAP Theorem says,\n1.\nThe CAP theorem states that a distributed system can only provide two of three properties simultaneously: consistency, availability, and \npartition tolerance. The theorem formalises the tradeoff between consistency and availability when there\u2019s a partition.\n5.\nCAP Theorem NoSQL Databases: NoSQL databases are great for distributed networks. They allow for horizontal scaling, and they can quickly scale \nacross multiple nodes. When deciding which NoSQL database to use, it\u2019s important to keep the CAP theorem in mind.\n1.\nCA Databases: CA databases enable consistency and availability across all nodes. Unfortunately, CA databases can\u2019t deliver fault tolerance. In \nany distributed system, partitions are bound to happen, which means this type of database isn\u2019t a very practical choice. That being said, you still \ncan find a CA database if you need one. Some relational databases, such as MySQL or PostgreSQL, allow for consistency and availability. You can \ndeploy them to nodes using replication.\n2.\nCP Databases: CP databases enable consistency and partition tolerance, but not availability. When a partition occurs, the system has to turn \noff inconsistent nodes until the partition can be fixed. MongoDB is an example of a CP database. It\u2019s a NoSQL database management system \n(DBMS) that uses documents for data storage. It\u2019s considered schema-less, which means that it doesn\u2019t require a defined database schema. It\u2019s \ncommonly used in big data and applications running in different locations. The CP system is structured so that there\u2019s only one primary \nnode that receives all of the write requests in a given replica set. Secondary nodes replicate the data in the primary nodes, so if the \nprimary node fails, a secondary node can stand-in. In banking system Availability is not as important as consistency, so we can opt it \n(MongoDB).\n3.\nAP Databases: AP databases enable availability and partition tolerance, but not consistency. In the event of a partition, all nodes are available, \nbut they\u2019re not all updated. For example, if a user tries to access data from a bad node, they won\u2019t receive the most up-to-date version of the \ndata. When the partition is eventually resolved, most AP databases will sync the nodes to ensure consistency across them. Apache Cassandra is \nan example of an AP database. It\u2019s a NoSQL database with no primary node, meaning that all of the nodes remain available. Cassandra allows \nfor eventual consistency because users can re-sync their data right after a partition is resolved. For apps like Facebook, we value availability \nmore than consistency, we\u2019d opt for AP Databases like Cassandra or Amazon DynamoDB.\nCodeHelp\nLEC-21: The Master-Slave Database Concept\n1.\nMaster-Slave is a general way to optimise IO in a system where number of requests goes way high that a single DB server is not able to handle it \nefficiently."
    },
    {
      "document": "DBMS_gfg.pdf",
      "importance_rank": 4,
      "refined_text": "Database Management System (DBMS) \n \nIntroduction to database Management System \n \n \n1. All about DBMS \nA Database Management System (DBMS) is a software system that is designed \nto manage and organize data in a structured manner. It allows users to create, \nmodify, and query a database, as well as manage the security and access controls \nfor that database. \nDBMS provides an environment to store and retrieve the data in coinvent and \nefficient manner. \n \nKey Features of DBMS \n\uf0b7 Data modeling: A DBMS provides tools for creating and modifying \ndata models, which define the structure and relationships of the data \nin a database. \n\uf0b7 Data storage and retrieval: A DBMS is responsible for storing and \nretrieving data from the database and can provide various methods for \nsearching and querying the data. \n\uf0b7 Concurrency control: A DBMS provides mechanisms for controlling \nconcurrent access to the database, to ensure that multiple users can \naccess the data without conflicting with each other. \n\uf0b7 Data integrity and security: A DBMS provides tools for enforcing \ndata integrity and security constraints, such as constraints on the \nvalues of data and access controls that restrict who can access the data. \n\uf0b7 Backup and recovery: A DBMS provides mechanisms for backing \nup and recovering the data in the event of a system failure. \n\uf0b7 DBMS can be classified into two types: Relational Database \nManagement System (RDBMS) and Non-Relational Database \nManagement System (NoSQL or Non-SQL) \n\uf0b7 RDBMS: Data is organized in the form of tables and each table has a \nset of rows and columns. The data are related to each other through \nprimary and foreign keys. \n\uf0b7 NoSQL: Data is organized in the form of key-value pairs, documents, \ngraphs, or column-based. These are designed to handle large-scale, \nhigh-performance scenarios. \nA database is a collection of interrelated data which helps in the efficient \nretrieval, insertion, and deletion of data from the database and organizes the data \nin the form of tables, views, schemas, reports, etc. For Example, a university \ndatabase organizes the data about students, faculty, admin staff, etc. which helps \nin the efficient retrieval, insertion, and deletion of data from it. \n \n \nDatabase Languages \n1) Data Definition Language \n2) Data Manipulation Language \n3) Data Control Language \n4) Transactional Control Language \n \nData Definition Language \nDDL is the short name for Data Definition Language, which deals with database \nschemas and descriptions, of how the data should reside in the database. \n0 seconds of 21 seconds Volume 0% \n\uf0b7 CREATE: to create a database and its objects like (table, index, \nviews, store procedure, function, and triggers) \n\uf0b7 ALTER: alters the structure of the existing database \n\uf0b7 DROP: delete objects from the database \n\uf0b7 TRUNCATE: remove all records from a table, including all spaces \nallocated for the records are removed \n\uf0b7 COMMENT: add comments to the data dictionary \n\uf0b7 RENAME: rename an object \n \nData Manipulation Language \nDML is the short name for Data Manipulation Language which deals with data \nmanipulation and includes most common SQL statements such SELECT, \nINSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve, \ndelete, and update data in a database. \n\uf0b7 SELECT: retrieve data from a database \n\uf0b7 INSERT: insert data into a table \n\uf0b7 UPDATE: updates existing data within a table \n\uf0b7 DELETE: Delete all records from a database table \n\uf0b7 MERGE: UPSERT operation (insert or update) \n\uf0b7 CALL: call a PL/SQL or Java subprogram \n\uf0b7 EXPLAIN PLAN: interpretation of the data access path \n\uf0b7 LOCK TABLE: concurrency Control \n \nData Control Language \nDCL is short for Data Control Language which acts as an access specifier to the \ndatabase. (Basically, to grant and revoke permissions to users in the database \n\uf0b7 GRANT: grant permissions to the user for running DML (SELECT, \nINSERT, DELETE\u2026) commands on the table \n\uf0b7 REVOKE: revoke permissions to the user for running DML \n(SELECT, INSERT, DELETE\u2026) command on the specified table \n \n \n \n \n \nTransactional Control Language \nTCL is short for Transactional Control Language which acts as a manager for \nall types of transactional data and all transactions. Some of the commands of \nTCL are. \n\uf0b7 Roll Back: Used to cancel or Undo changes made in the database.  \n\uf0b7 Commit: It is used to apply or save changes in the database \n\uf0b7 Save Point: It is used to save the data on the temporary basis in the \ndatabase. \n \nData retrieval language: \nDRL is short for Data Retrieval Language which is used for retrieval of data. It \ncan also be said as DML. \n\uf0b7 SELECT: Used for extracting the required data. \n \nApplications of DBMS: \n\uf0b7 Enterprise Information: Sales, accounting, human resources, \nManufacturing, online retailers. \n\uf0b7 Banking and Finance Sector: Banks maintaining the customer \ndetails, accounts, loans, banking transactions, credit card transactions. \nFinance: Storing the information about sales and holdings, purchasing \nof financial stocks and bonds. \n\uf0b7 University: Maintaining the information about student course \nenrolled information, student grades, staff roles. \n\uf0b7 Airlines: Reservations and schedules. \n\uf0b7 Telecommunications: Prepaid, postpaid bills maintenance. \n \nParadigm Shift from File System to DBMS \n File System manages data using files on a hard disk. Users are allowed to create, \ndelete, and update the files according to their requirements. Let us consider the \nexample of file-based University Management System. Data of students is \navailable to their respective Departments, Academics Section, Result Section, \nAccounts Section, Hostel Office, etc. Some of the data is common for all \nsections like Roll No, Name, Father Name, Address, and Phone number of \nstudents but some data is available to a particular section only like Hostel \nallotment number which is a part of the hostel office. Let us discuss the issues \nwith this system: \n\uf0b7 Redundancy of data: Data is said to be redundant if the same data is \ncopied at many places. If a student wants to change their Phone \nnumber, he or she has to get it updated in various sections. Similarly, \nold records must be deleted from all sections representing that student. \n\uf0b7 Inconsistency of Data: Data is said to be inconsistent if multiple \ncopies of the same data do not match each other. If the Phone number \nis different in Accounts Section and Academics Section, it will be \n \n \ninconsistent. Inconsistency may be because of typing errors or not \nupdating all copies of the same data. \n\uf0b7 Difficult Data Access: A user should know the exact location of the \nfile to access data, so the process is very cumbersome and tedious. If \nthe user wants to search the student hostel allotment number of a \nstudent from 10000 unsorted students\u2019 records, how difficult it can be. \n\uf0b7 Unauthorized Access: File Systems may lead to unauthorized access \nto data. If a student gets access to a file having his marks, he can \nchange it in an unauthorized way. \n\uf0b7 No Concurrent Access: The access of the same data by multiple users \nat the same time is known as concurrency. The file system does not \nallow concurrency as data can be accessed by only one user at a time. \n\uf0b7 No Backup and Recovery: The file system does not incorporate any \nbackup and recovery of data if a file is lost or corrupted. \n \nAdvantages of DBMS \n\uf0b7 Data organization: A DBMS allows for the organization and storage \nof data in a structured manner, making it easy to retrieve and query the \ndata as needed. \n\uf0b7 Data integrity: A DBMS provides mechanisms for enforcing data \nintegrity constraints, such as constraints on the values of data and \naccess controls that restrict who can access the data. \n\uf0b7 Concurrent access: A DBMS provides mechanisms for controlling \nconcurrent access to the database, to ensure that multiple users can \naccess the data without conflicting with each other. \n\uf0b7 Data security: A DBMS provides tools for managing the security of \nthe data, such as controlling access to the data and encrypting sensitive \ndata. \n\uf0b7 Backup and recovery: A DBMS provides mechanisms for backing \nup and recovering the data in the event of a system failure. \n\uf0b7 Data sharing: A DBMS allows multiple users to access and share the \nsame data, which can be useful in a collaborative work environment. \n \nDisadvantages of DBMS \n\uf0b7 Complexity: DBMS can be complex to set up and maintain, requiring \nspecialized knowledge and skills. \n\uf0b7 Performance overhead: The use of a DBMS can add overhead to the \nperformance of an application, especially in cases where high levels \nof concurrency are required. \n\uf0b7 Scalability: The use of a DBMS can limit the scalability of an \napplication since it requires the use of locking and other \nsynchronization mechanisms to ensure data consistency. \n \n \n\uf0b7 Cost: The cost of purchasing, maintaining, and upgrading a DBMS \ncan be high, especially for large or complex systems. \n\uf0b7 Limited Use Cases: Not all use cases are suitable for a DBMS, some \nsolutions do not need high reliability, consistency or security and may \nbe better served by other types of data storage. \nThese are the main reasons which made a shift from file system to DBMS.  \nAlso, see. \nA Database Management System (DBMS) is a software system that allows users \nto create, maintain, and manage databases. It is a collection of programs that \nenables users to access and manipulate data in a database. A DBMS is used to \nstore, retrieve, and manipulate data in a way that provides security, privacy, and \nreliability. \n \nSeveral Types of DBMS \n\uf0b7 Relational DBMS (RDBMS): An RDBMS stores data in tables with \nrows and columns and uses SQL (Structured Query Language) to \nmanipulate the data. \n\uf0b7 Object-Oriented DBMS (OODBMS): An OODBMS stores data as \nobjects, which can be manipulated using object-oriented programming \nlanguages. \n\uf0b7 NoSQL DBMS: A NoSQL DBMS stores data in non-relational data \nstructures, such as key-value pairs, document-based models, or graph \nmodels. \n \n2. Database Architecture \nA Database stores a lot of critical information to access data quickly and \nsecurely. Hence it is important to select the correct architecture for efficient data \nmanagement. DBMS Architecture helps users to get their requests done while \nconnecting to the database. We choose database architecture depending on \nseveral factors like the size of the database, number of users, and relationships \nbetween the users. There are two types of database models that we generally \nuse, logical model and physical model. Several types of architecture are there in \nthe database which we will deal with in the next section. \nTypes of DBMS Architecture \nThere are several types of DBMS Architecture that we use according to the \nusage requirements. Types of DBMS Architecture are discussed here. \n\uf0b7 1-Tier Architecture \n\uf0b7 2-Tier Architecture \n\uf0b7 3-Tier Architecture \n \n\uf0d8 1-Tier Architecture \n \n \nIn 1-Tier Architecture the database is directly available to the user, the user can \ndirectly sit on the DBMS and use it that is, the client, server, and Database are \nall present on the same machine. For Example: to learn SQL we set up an SQL \nserver and the database on the local system. This enables us to directly interact \nwith the relational database and execute operations. The industry won\u2019t use this \narchitecture they logically go for 2-tier and 3-tier Architecture. \n \nAdvantages of 1-Tier Architecture \nBelow mentioned are the advantages of 1-Tier Architecture. \n\uf0b7 Simple Architecture: 1-Tier Architecture is the simplest architecture \nto set up, as only a single machine is required to maintain it. \n\uf0b7 Cost-Effective: No additional hardware is required for implementing \n1-Tier Architecture, which makes it cost-effective. \n\uf0b7 Easy to Implement: 1-Tier Architecture can be easily deployed, and \nhence it is mostly used in small projects.  \n \n\uf0d8 2-Tier Architecture \nThe 2-tier architecture is like a basic client-server model. The application at \nthe client end directly communicates with the database on the server side. \nAPIs like ODBC and JDBC are used for this interaction. The server side is \nresponsible for providing query processing and transaction management \nfunctionalities. On the client side, the user interfaces and application programs \nare run. The application on the client side establishes a connection with the \nserver side to communicate with the DBMS.  \nAn advantage of this type is that maintenance and understanding are easier, \nand compatible with existing systems. However, this model gives poor \nperformance when there are many users. \n                                Figure 1: 2-Tire Architecture \n \nAdvantages of 2-Tier Architecture \n\uf0b7 Easy to Access: 2-Tier Architecture makes easy access to the \ndatabase, which makes fast retrieval. \n\uf0b7 Scalable: We can scale the database easily, by adding clients or \nupgrading hardware. \n\uf0b7 Low Cost: 2-Tier Architecture is cheaper than 3-Tier Architecture \nand Multi-Tier Architecture. \n \n \n\uf0b7 Easy Deployment: 2-Tier Architecture is easier to deploy than 3-Tier \nArchitecture. \n\uf0b7 Simple: 2-Tier Architecture is easily understandable as well as simple \nbecause of only two components. \n \n3-Tier Architecture \nIn 3-Tier Architecture, there is another layer between the client and the server. \nThe client does not directly communicate with the server. Instead, it interacts \nwith an application server which further communicates with the database system \nand then the query processing and transaction management takes place. This \nintermediate layer acts as a medium for the exchange of partially processed data \nbetween the server and the client. This type of architecture is used in the case of \nlarge web applications.  \n                                        Figure 1: 3-Tire Architecture \nAdvantages of 3-Tier Architecture \n\uf0b7 Enhanced scalability: Scalability is enhanced due to the distributed \ndeployment of application servers. Now, individual connections need \nnot be made between the client and server. \n\uf0b7 Data Integrity: 3-Tier Architecture maintains Data Integrity. Since \nthere is a middle layer between the client and the server, data \ncorruption can be avoided/removed. \n\uf0b7 Security: 3-Tier Architecture Improves Security. This type of model \nprevents direct interaction of the client with the server thereby \nreducing access to unauthorized data. \n \n \n \nDisadvantages of 3-Tier Architecture \n\uf0b7 More Complex: 3-Tier Architecture is more complex in comparison \nto 2-Tier Architecture. Communication Points are also doubled in 3-\nTier Architecture. \n\uf0b7 Difficult to Interact: It becomes difficult for this sort of interaction \nto take place due to the presence of middle layers. \n \n3. Need for DBMS \nA Data Base Management System is a system software for easy, efficient and \nreliable data processing and management. It can be used for: \n\uf0b7 Creation of a database. \n\uf0b7 Retrieval of information from the database. \n\uf0b7 Updating the database. \n\uf0b7 Managing a database. \n\uf0b7 Multiple User Interface \n\uf0b7 Data scalability, expandability, and flexibility: We can \nchange schema of the database, all schema will be updated \naccording to it. \n\uf0b7 Overall, the time for developing an application is reduced. \n\uf0b7 Security: Simplifies data storage as it is possible to assign \nsecurity permissions allowing restricted access to data. \nData organization: DBMS allow users to organize large amounts of data in a \nstructured and systematic way. Data is organized into tables, fields, and records, \nmaking it easy to manage, store, and retrieve information. \nData scalability: DBMS are designed to handle large amounts of data and are \nscalable to meet the growing needs of organizations. As organizations grow, \nDBMS can scale up to handle increasing amounts of data and user traffic. \n \n\uf0b7 Data Organization and Management: \nOne of the primary needs for a DBMS is data organization and management. \nDBMSs allow data to be stored in a structured manner, which helps in easier \nretrieval and analysis. A well-designed database schema enables faster access \nto information, reducing the time required to find relevant data. A DBMS also \nprovides features like indexing and searching, which make it easier to locate \nspecific data within the database. This allows organizations to manage their data \nmore efficiently and effectively. \n \n\uf0b7 Data Security and Privacy: \nDBMSs provide a robust security framework that ensures the confidentiality, \nintegrity, and availability of data. They offer authentication and authorization \nfeatures that control access to the database. DBMSs also provide encryption \n \n \ncapabilities to protect sensitive data from unauthorized access. Moreover, \nDBMSs comply with various data privacy regulations such as the GDPR, \nHIPAA, and CCPA, ensuring that organizations can store and manage their data \nin compliance with legal requirements. \n \n\uf0b7 Data Integrity and Consistency: \nData integrity and consistency are crucial for any database. DBMSs provide \nmechanisms that ensure the accuracy and consistency of data. These \nmechanisms include constraints, triggers, and stored procedures that enforce \ndata integrity rules. DBMSs also provide features like transactions that ensure \nthat data changes are atomic, consistent, isolated, and durable (ACID). \n \n\uf0b7 Concurrent Data Access: \nA DBMS provides a concurrent access mechanism that allows multiple users to \naccess the same data simultaneously. This is especially important for \norganizations that require real-time data access. DBMSs use locking \nmechanisms to ensure that multiple users can access the same data without \ncausing conflicts or data corruption. \n \n\uf0b7 Data Analysis and Reporting: \nDBMSs provide tools that enable data analysis and reporting. These tools allow \norganizations to extract useful insights from their data, enabling better decision-\nmaking. DBMSs support various data analysis techniques such as OLAP, data \nmining, and machine learning. Moreover, DBMSs provide features like data \nvisualization and reporting, which enable organizations to present their data in \na visually appealing and understandable way. \n \n\uf0b7 Scalability and Flexibility: \nDBMSs provide scalability and flexibility, enabling organizations to handle \nincreasing amounts of data. DBMSs can be scaled horizontally by adding more \nservers or vertically by increasing the capacity of existing servers. This makes \nit easier for organizations to handle large amounts of data without compromising \nperformance. Moreover, DBMSs provide flexibility in terms of data modeling, \nenabling organizations to adapt their databases to changing business \nrequirements. \n \n\uf0b7 Cost-Effectiveness: \nDBMSs are cost-effective compared to traditional file-based systems. They \nreduce storage costs by eliminating redundancy and optimizing data storage. \nThey also reduce development costs by providing tools for database design, \nmaintenance, and administration. Moreover, DBMSs reduce operational costs \nby automating routine tasks and providing self-tuning capabilities. \n4. Challenges of Database Security in DBMS \n \n \nThe vast increase in volume and speed of threats to databases and many \ninformation assets, research efforts need to be consider to the following issues \nsuch as data quality, intellectual property rights, and database survivability.  \n \nLet us discuss them one by one.  \ni) \nData quality \u2013 \n\uf0b7 The database community basically needs techniques and some \norganizational solutions to assess and attest the quality of data. These \ntechniques may include the simple mechanism such as quality stamps \nthat are posted on different websites. We also need techniques that will \nprovide us more effective integrity semantics verification tools for \nassessment of data quality, based on many techniques such as record \nlinkage. \n\uf0b7 We also need application-level recovery techniques to automatically \nrepair the incorrect data. \n\uf0b7 The ETL that is extracted transform and load tools widely used for \nloading the data in the data warehouse are presently grappling with \nthese issues. \n \nii) \nIntellectual property rights \u2013  \nAs the use of Internet and intranet is increasing day by day, legal and \ninformational aspects of data are becoming major concerns for many \norganizations. To address this concerns watermark technique are used which \nwill help to protect content from unauthorized duplication and distribution by \ngiving the provable power to the ownership of the content. Traditionally they \nare dependent upon the availability of a large domain within which the objects \ncan be altered while retaining its essential or important properties. However, \nresearch is needed to access the robustness of many such techniques and the \nstudy and investigate many different approaches or methods that aimed to \nprevent intellectual property rights violation.  \n \niii) \nDatabase survivability \u2013  \nDatabase systems need to operate and continued their functions even with the \nreduced capabilities, despite disruptive events such as information warfare \nattacks A DBMS in addition to making every effort to prevent an attack and \ndetecting one in the event of the occurrence should be able to do the following: \n\uf0b7 Confident: We should take immediate action to eliminate the \nattacker\u2019s access to the system and to isolate or contain the problem to \nprevent further spread. \n\uf0b7 Damage assessment: Determine the extent of the problem, including \nfailed function and corrupted data. \n\uf0b7 Recover: Recover corrupted or lost data and repair or reinstall failed \nfunction to reestablish a normal level of operation. \n \n \n\uf0b7 Reconfiguration: Reconfigure to allow the operation to continue in a \ndegraded mode while recovery proceeds. \n\uf0b7 Fault treatment: To the extent possible, identify the weakness \nexploited in the attack and takes steps to prevent a recurrence. \n \nDatabase security  \nIt is an essential aspect of database management systems (DBMS) as it involves \nprotecting the confidentiality, integrity, and availability of the data stored in the \ndatabase. The challenges of database security in DBMS include: \n\uf0b7 Authentication and Authorization: One of the biggest challenges of \ndatabase security is ensuring that only authorized users can access the \ndatabase. The DBMS must authenticate users and grant them appropriate \naccess rights based on their roles and responsibilities. \n\uf0b7 Encryption: Data encryption is an effective way to protect sensitive data \nin transit and at rest. However, it can also be a challenge to implement \nand manage encryption keys and ensure that encrypted data is not \ncompromised. \n\uf0b7 Access Control: Access control involves regulating the access to data \nwithin the database. It can be challenging to implement access control \nmechanisms that allow authorized users to access the data they need while \npreventing unauthorized users from accessing it. \n\uf0b7 Auditing and Logging: DBMS must maintain an audit trail of all \nactivities in the database. This includes monitoring who accesses the \ndatabase, what data is accessed, and when it is accessed. This can be a \nchallenge to implement and manage, especially in large databases. \n\uf0b7 Database Design: The design of the database can also impact security. A \npoorly designed database can lead to security vulnerabilities, such as SQL \ninjection attacks, which can compromise the confidentiality, integrity, \nand availability of data. \n\uf0b7 Malicious attacks: Cyberattacks such as hacking, malware, and phishing \npose a significant threat to the security of databases. DBMS must have \nrobust security measures in place to prevent and detect such attacks. \n\uf0b7 Physical Security: Physical security of the database is also important, as \nunauthorized physical access to the server can lead to data breaches. \n \nFeatures that are used to enhance database security: \n\uf0b7 Backup and Recovery: DBMS systems include backup and recovery \nfeatures that ensure that data can be restored in the event of a system \nfailure or security breach. Backups can be created at regular intervals and \nstored securely to prevent unauthorized access. \n\uf0b7 Access Controls: Access controls can be used to restrict access to certain \nparts of the database based on user roles or permissions. For example, a \n \n \nDBMS can enforce rules such as not allowing a user to drop tables or \ngranting read-only access to some users. \n\uf0b7 Database Auditing and Testing Tools: Database auditing and testing \ntools allow security personnel to monitor and test the security of the \ndatabase. This helps in identifying security gaps and weaknesses in the \nsystem. \n\uf0b7 Data Masking: DBMS systems support data masking features which are \nused to protect sensitive data by obscuring it from view. This is especially \nuseful in cases where sensitive data needs to be accessed by third-party \nvendors or contractors. \n \n5. Advantage of DBMS over File system \nFile System: A File Management system is a DBMS that allows access to single \nfiles or tables at a time. In a File System, data is directly stored in a set of files. \nIt contains flat files that have no relation to other files (when only one table is \nstored in a single file, then this file is known as a flat file).  \nDBMS: A Database Management System (DBMS) is application software that \nallows users to efficiently define, create, maintain, and share databases. \nDefining a database involves specifying the data types, structures and \nconstraints of the data to be stored in the database. Creating a database involves \nstoring the data on some storage medium that is controlled by DBMS. \nMaintaining a database involves updating the database whenever required to \nevolve and reflect changes in the Mini world and also generating reports for \neach change. Sharing a database involves allowing multiple users to access the \ndatabase. DBMS also serves as an interface between the database and end users \nor application programs. It provides control access to the data and ensures that \ndata is consistent and correct by defining rules on them.  \nAn application program accesses the database by sending queries or requests for \ndata to the DBMS. A query causes some data to be retrieved from the database.  \nAdvantages of DBMS over File system are: \n\uf0b7 Data redundancy and inconsistency: Redundancy is the concept of \nrepetition of data i.e. each data may have more than a single copy. The \nfile system cannot control the redundancy of data as each user defines \nand maintains the needed files for a specific application to run. There \nmay be a possibility that two users are maintaining the data of the same \nfile for different applications. Hence changes made by one user do not \nreflect in files used by second users, which leads to inconsistency of \ndata. Whereas DBMS controls redundancy by maintaining a single \nrepository of data that is defined once and is accessed by many users. \nAs there is no or less redundancy, data remains consistent. \n \n \n\uf0b7 Data sharing: The file system does not allow sharing of data or \nsharing is too complex. Whereas in DBMS, data can be shared easily \ndue to a centralized system. \n\uf0b7 Data concurrency: Concurrent access to data means more than one \nuser is accessing the same data at the same time. Anomalies occur \nwhen changes made by one user get lost because of changes made by \nanother user. The file system does not provide any procedure to stop \nanomalies. Whereas DBMS provides a locking system to stop \nanomalies to occur. \n\uf0b7 Data searching: For every search operation performed on the file \nsystem, a different application program has to be written. While \nDBMS provides inbuilt searching operations. The user only has to \nwrite a small query to retrieve data from the database. \n\uf0b7 Data integrity: There may be cases when some constraints need to be \napplied to the data before inserting it into the database. The file system \ndoes not provide any procedure to check these constraints \nautomatically. Whereas DBMS maintains data integrity by enforcing \nuser-defined constraints on data by itself. \n\uf0b7 System crashing: In some cases, systems might have crashed due to \nvarious reasons. It is a bane in the case of file systems because once \nthe system crashes, there will be no recovery of the data that\u2019s been \nlost. A DBMS will have the recovery manager which retrieves the data \nmaking it another advantage over file systems.  \n\uf0b7 Data security: A file system provides a password mechanism to \nprotect the database but how long can the password be protected? No \none can guarantee that. This doesn\u2019t happen in the case of DBMS. \nDBMS has specialized features that help provide shielding to its data.  \n\uf0b7 Backup: It creates a backup subsystem to restore the data if required. \n\uf0b7 Interfaces: It provides different multiple user interfaces like graphical \nuser interface and application program interface. \n\uf0b7 Easy Maintenance: It is easily maintainable due to its centralized \nnature. \n \n6. Data Abstraction and Data Independence \nDatabase systems comprise complex data structures. To make the system \nefficient in terms of retrieval of data, and reduce complexity in terms of usability \nof users, developers use abstraction i.e. hide irrelevant details from the users. \nThis approach simplifies database design.  \n \nLevel of Abstraction in a DBMS \nThere are mainly 3 levels of data abstraction:  \n \n \n\uf0b7 Physical or Internal Level \n\uf0b7 Logical or Conceptual Level \n\uf0b7 View or External Level \n \nPhysical or Internal Level \nThis is the lowest level of data abstraction. It tells us how the data is stored in \nmemory. Access methods like sequential or random access and file \norganization methods like B+ trees and hashing are used for the same. \nUsability, size of memory, and the number of times the records are factors that \nwe need to know while designing the database.  \nSuppose we need to store the details of an employee. Blocks of storage and the \namount of memory used for these purposes are kept hidden from the user. \n \nLogical or Conceptual Level \nThis level comprises the information that is stored in the database in the form \nof tables. It also stores the relationship among the data entities in relatively \nsimple structures. At this level, the information available to the user at the \nview level is unknown.  \nWe can store the various attributes of an employee and relationships, e.g. with \nthe manager can also be stored.  \n \nView or External Level \nThis is the highest level of abstraction. Only a part of the actual database is \nviewed by the users. This level exists to ease the accessibility of the database by \nan individual user. Users view data in the form of rows and columns. Tables and \nrelations are used to store data. Multiple views of the same database may exist. \nUsers can just view the data and interact with the database, storage and \nimplementation details are hidden from them.  \nExample: In case of storing customer data, \n\uf0b7 Physical level \u2013 it will contain block of storages (bytes, GB, TB, etc) \n\uf0b7 Logical level \u2013 it will contain the fields and the attributes of data. \n\uf0b7 View level \u2013 it works with CLI or GUI access of database \n \n \n                                         Figure 3: Data Abstraction \nThe main purpose of data abstraction is to achieve data independence in order \nto save the time and cost required when the database is modified or altered.  \nData Independence \nIt is mainly defined as a property of DBMS that helps you to change the \ndatabase schema at one level of a system without requiring to change the \nschema at the next level. it helps to keep the data separated from all programs \nthat makes use of it. \nWe have namely two levels of data independence arising from these levels of \nabstraction:  \n\uf0b7 Physical level data independence \n\uf0b7 Logical level data independence \n \nPhysical Level Data Independence \nIt refers to the characteristic of being able to modify the physical schema without \nany alterations to the conceptual or logical schema, done for optimization \npurposes, e.g., the Conceptual structure of the database would not be affected \nby any change in storage size of the database system server. Changing from \nsequential to random access files is one such example. These alterations or \nmodifications to the physical structure may include:  \n\uf0b7 Utilizing new storage devices. \n\uf0b7 Modifying data structures used for storage. \n\uf0b7 Altering indexes or using alternative file organization techniques etc. \n \nLogical Level Data Independence \nIt refers characteristic of being able to modify the logical schema without \naffecting the external schema or application program. The user view of the data \nwould not be affected by any changes to the conceptual view of the data. These \nchanges may include insertion or deletion of attributes, altering table structures \nentities or relationships to the logical schema, etc. \n \n \n \nER-Model \n7. Introduction to ER-Model \nThe Entity Relational Model is a model for identifying entities to be represented \nin the database and representation of how those entities are related. The ER data \nmodel specifies enterprise schema that represents the overall logical structure of \na database graphically.  \nThe Entity Relationship Diagram explains the relationship among the entities \npresent in the database. ER models are used to model real-world objects like a \nperson, a car, or a company and the relation between these real-world objects. \nIn short, the ER Diagram is the structural format of the database.  \n \nSymbols Used in ER Model \nER Model is used to model the logical view of the system from a data \nperspective which consists of these symbols: \n\uf0b7 Rectangles: Rectangles represent Entities in the ER Model. \n\uf0b7 Ellipses: Ellipses represent Attributes in the ER Model. \n\uf0b7 Diamond: Diamonds represent Relationships among Entities. \n\uf0b7 Lines: Lines represent attributes to entities and entity sets with other \nrelationship types. \n\uf0b7 Double Ellipse: Double Ellipses represent Multi-Valued Attributes. \n\uf0b7 Double Rectangle: Double Rectangle represents a Weak Entity. \n \nComponents of ER Diagram \nER Model consists of Entities, Attributes, and Relationships among Entities in \na Database System. \ni) \nEntity \nAn Entity may be an object with a physical existence \u2013 a particular person, car, \nhouse, or employee \u2013 or it may be an object with a conceptual existence \u2013 a \ncompany, a job, or a university course.  \nEntity Set: An Entity is an object of Entity Type, and a set of all entities is \ncalled an entity set. For Example, E1 is an entity having Entity Type Student \nand the set of all students is called Entity Set.  \n                                   Figure 4: Entity Set \n \n \na. Strong Entity \nA Strong Entity is a type of entity that has a key Attribute. Strong Entity does \nnot depend on other Entity in the Schema. It has a primary key, that helps in \nidentifying it uniquely, and it is represented by a rectangle. These are called \nStrong Entity Types. \n \nb. Weak Entity \nAn Entity type has a key attribute that uniquely identifies each entity in the \nentity set. But some entity type exists for which key attributes cannot be defined. \nThese are called Weak Entity types.  \n \nFor Example, A company may store the information of dependents (Parents, \nChildren, Spouse) of an Employee. But the dependents do not have existed \nwithout the employee. So Dependent will be a Weak Entity Type and \nEmployee will be Identifying Entity type for Dependent, which means it \nis Strong Entity Type. \nA weak entity type is represented by a Double Rectangle. The participation of \nweak entity types is always total. The relationship between the weak entity type \nand its identifying strong entity type is called identifying relationship and it is \nrepresented by a double diamond.  \n \n1) Attributes \nAttributes are the properties that define the entity type. For example, Roll_No, \nName, DOB, Age, Address, and Mobile_No are the attributes that define entity \ntype Student. In ER diagram, the attribute is represented by an oval.  \n                                        Figure 5: Attribute \na) Key Attribute \nThe attribute which uniquely identifies each entity in the entity set is called \nthe key attribute. For example, Roll_No will be unique for each student. In ER \ndiagram, the key attribute is represented by an oval with underlying lines. \n                                       Figure 6: Key Attribute \n \n \nb) Composite Attribute \nAn attribute composed of many other attributes is called a composite \nattribute. For example, the Address attribute of the student Entity type consists \nof Street, City, State, and Country. In ER diagram, the composite attribute is \nrepresented by an oval comprising of ovals.  \n                                    Figure 7: Composite Attribute \nc) Multivalued Attribute \nAn attribute consisting of more than one value for a given entity. For example, \nPhone_No (can be more than one for a given student). In ER diagram, a \nmultivalued attribute is represented by a double oval.  \n                                 Figure 8. Multivalued Attribute \nd)  Derived Attribute \nAn attribute that can be derived from other attributes of the entity type is known \nas a derived attribute. e.g., Age (can be derived from DOB). In ER diagram, the \nderived attribute is represented by a dashed oval.  \n \n \n7.1. Recursive Relationships in ER diagram \nA relationship between two entities of a similar entity type is called \na recursive relationship. Here the same entity type participates more than once \nin a relationship type with a different role for each instance. In other words, a \nrelationship has always been between occurrences in two different entities. \nHowever, the same entity can participate in the relationship. This is termed \na recursive relationship.  \nRecursive relationships are often used to represent hierarchies or networks, \nwhere an entity can be connected to other entities of the same type.  \n \n \nFor example, in an organizational chart, an employee can have a relationship \nwith other employees who are also in a managerial position. Similarly, in a \nsocial network, a user can have a relationship with other users who are their \nfriends. \nTo represent a recursive relationship in an ER diagram, we use a self-join, which \nis a join between a table and itself. In other words, we create a relationship \nbetween the same entity type. The self-join involves creating two instances of \nthe same entity and connecting them with a relationship. One instance is \nconsidered the parent, and the other instance is considered the child. \nWe use cardinality constraints to specify the number of instances of the entity \nthat can participate in the relationship. For example, in an organizational chart, \nan employee can have many subordinates, but each subordinate can only have \none manager. This is represented as a one-to-many (1:N) relationship between \nthe employee entity and itself. \nOverall, recursive relationships are an important concept in ER modeling, and \nthey allow us to represent complex relationships between entities of the same \ntype. They are particularly useful in modeling hierarchical data structures and \nnetworks. \n                           Figure 9: Recursive Relationship \nExample: Let us suppose that we have an employee table. A manager \nsupervises a subordinate. Every employee can have a supervisor except the CEO \nand there can be at most one boss for each employee. One employee may be the \nboss of more than one employee. Let us suppose that REPORTS_TO is a \nrecursive relationship on the Employee entity type where each Employee plays \ntwo roles. \ni) Supervisor \nii) Subordinate \n \n \n                                     Figure 10: Recursive Relation  \n \nSupervisors and subordinates are called \u201cRole Names.\u201d Here the degree of the \nREPORTS_TO relationship is 1 i.e., a unary relationship.   \n\uf0b7 The minimum cardinality of the Supervisor entity is ZERO since the \nlowest level employee may not be a manager for anyone. \n\uf0b7 The maximum cardinality of the Supervisor entity is N since an \nemployee can manage many employees. \n\uf0b7 Similarly, the Subordinate entity has a minimum cardinality of ZERO \nto account for the case where CEO can never be a subordinate. \n\uf0b7 Its maximum cardinality is ONE since a subordinate employee can \nhave at most one supervisor. \nNote \u2013 Here none of the participants have total participation since both \nminimum cardinalities are Zero. Hence, the relationships are connected by a \nsingle line instead of a double line in the ER diagram.  \nExample \nCREATE TABLE employee ( \n    id INT PRIMARY KEY, \n    name VARCHAR (50), \n    manager_id INT, \n    FOREIGN KEY (manager_id) REFERENCES employee(id) \n); \nHere, the employee table has a foreign key column called manager_id that \nreferences the id column of the same employee table. This allows you to create \na recursive relationship where an employee can have a manager who is also an \nemployee. \n \n \n \n \n7.2. Minimization of ER Diagram \n \nEntity-Relationship (ER) Diagram is a diagrammatic representation of data in \ndatabases, it shows how data is related to one another. In this article, we require \nprevious knowledge of ER diagrams and how to draw ER diagrams. \nMinimization of ER Diagram simply means reducing the quantity of the tables \nin the ER Diagram. When there are so many tables present in the ER DIagram, \nit decreases the readability and understandability of the ER Diagram, and it also \nbecomes difficult for the admin also to understand these. Minimizing the ER \nDiagram helps in better understanding. We reduce tables depending on the \ncardinality. \n \nCardinality \nThe number of times an entity of an entity set participates in a relationship set \nis known as cardinality. Cardinality can be of different types: \ni) \nOne-to-One:  \nWhen each entity in each entity set can take part only once in the relationship, \nthe cardinality is one-to-one. Let, us assume that a male can marry one female \nand a female can marry one male. So, the relationship will be one-to-one.  \nthe total number of tables that can be used in this is 2. \n                      Figure 11. one to one cardinality \nUsing Sets, it can be represented as:  \n                Figure 12. Set Representation of One-to-One \n \n \nii) \nOne-to-Many:  \nIn one-to-many mapping as well where each entity can be related to more than \none relationship and the total number of tables that can be used in this is 2. Let \nus assume that one surgeon department can accommodate many doctors. So, the \nCardinality will be 1 to M. It means one department has many Doctors. \ntotal number of tables that can used is 3. \n                            Figure 13. one to many cardinality \niii) \nMany-to-One:  \nWhen entities in one entity set can take part only once in the relationship set and \nentities in other entity sets can take part more than once in the relationship set, \ncardinality is many to one. Let us assume that a student can take only one course \nbut one course can be taken by many students. So, the cardinality will be n to 1. \nIt means that for one course there can be n students but for one student, there \nwill be only one course.  \nThe total number of tables that can be used in this is 3. \n                        Figure 14. many to one cardinality \nUsing Sets, it can be represented as: \n                 Figure 15. Set Representation of Many-to-One \n \n \niv) \nMany-to-Many:  \nWhen entities in all entity sets can take part more than once in the relationship \ncardinality is many to many. Let us assume that a student can take more than \none course and one course can be taken by many students. So, the relationship \nwill be many to many.  \nthe total number of tables that can be used in this is 3. \n                          Figure 16: many to many cardinality \nUsing Sets, it can be represented as:  \n                       Figure 17: Many-to-Many Set Representation \nIn this example, student S1 is enrolled in C1 and C3 and Course C3 is enrolled \nby S1, S3, and S4. So, it is many-to-many relationships.  \n \n \n \n \n \n7.3. Enhanced ER Model \nToday the complexity of the data is increasing so it becomes more and more \ndifficult to use the traditional ER model for database modeling. To reduce this \ncomplexity of modeling we must make improvements or enhancements to the \nexisting ER model to make it able to handle the complex application in a better \nway.  \nEnhanced entity-relationship diagrams are advanced database diagrams very \nsimilar to regular ER diagrams which represent the requirements and \ncomplexities of complex databases.  \nIt is a diagrammatic technique for displaying the Sub Class and Super Class; \nSpecialization and Generalization; Union or Category; Aggregation etc.  \nGeneralization and Specialization: These are very common relationships \nfound in real entities. However, this kind of relationship was added later as an \nenhanced extension to the classical ER model. Specialized classes are often \ncalled subclass while a generalized class is called a superclass, probably \ninspired by object-oriented programming. A sub-class is best understood \nby \u201cIS-A analysis\u201d. The following statements hopefully make some sense to \nyour mind \u201cTechnician IS-A Employee\u201d, and \u201cLaptop IS-A Computer\u201d.  \nAn entity is a specialized type/class of another entity. For example, a Technician \nis a special Employee in a university system Faculty is a special class of \nEmployees. We call this phenomenon generalization/specialization. In the \nexample here Employee is a generalized entity class while the Technician and \nFaculty are specialized classes of Employee.  \nExample: \nThis example instance of \u201csub-class\u201d relationships. Here we have four sets of \nemployees: Secretary, Technician, and Engineer. The employee is a super-class \nof the rest three sets of individual sub-class is a subset of Employee set. \n \n \n                                 Figure 18: ER Model \n\uf0b7 An entity belonging to a sub-class is related to some super-class entity. \nFor instance, emp, no 1001 is a secretary, and his typing speed is 68. \nEmp no 1009 is an engineer (sub-class) and her trade is \u201cElectrical\u201d, \nso forth. \n\uf0b7 Sub-class entity \u201cinherits\u201d all attributes of super-class; for example, \nemployee 1001 will have attributes eno, name, salary, and typing \nspeed. \n \nEnhanced ER model of above example  \n                                    Figure 19: Enhanced ER Model \n \n \nConstraints \u2013 There are two types of constraints on the \u201cSub-class\u201d \nrelationship.   \ni) Total or Partial \u2013  \nA sub-classing relationship is total if every super-class entity is to be associated \nwith some sub-class entity, otherwise partial. Sub-class \u201cjob type-based \nemployee category\u201d is partial sub-classing \u2013 not necessary every employee is \none of (secretary, engineer, and technician), i.e. union of these three types is a \nproper subset of all employees. Whereas other sub-classing \u201cSalaried Employee \nAND Hourly Employee\u201d is total; the union of entities from sub-classes is equal \nto the total employee set, i.e. every employee necessarily has to be one of them. \nii) Overlapped or Disjoint \u2013  \nIf an entity from a super-set can be related (can occur) in multiple sub-class sets, \nthen it is overlapped sub-classing, otherwise disjoint. Both the examples: job-\ntype based, and salaries/hourly employee sub-classing are disjoint. \n \nNote \u2013 These constraints are independent of each other: can be \u201coverlapped \nand total or partial\u201d or \u201cdisjoint and total or partial.\u201d Also, sub-classing has \ntransitive properties.  \nMultiple Inheritance (sub-class of multiple superclasses) \u2013  \nAn entity can be a sub-class of multiple entity types; such entities are sub-class \nof multiple entities and have multiple super-classes; Teaching Assistant can \nsubclass of Employee and Student both. A faculty in a university system can \nbe a subclass of Employee and Alumnus. In multiple inheritances, attributes of \nsub-class are the union of attributes of all super-classes.  \nUnion \u2013   \n\uf0b7 Set of Library Members is UNION of Faculty, Student, and Staff. A \nunion relationship indicates either type; for example, a library member \nis either Faculty or Staff or Student. \n\uf0b7 Below are two examples that show how UNION can be depicted in \nERD \u2013 Vehicle Owner is UNION of PERSON and Company, and RTO \nRegistered Vehicle is UNION of Car and Truck. \n \nHere are some of the key features of the EER model: \n\uf0b7 Subtypes and Supertypes: The EER model allow for the creation of \nsubtypes and supertypes. A supertype is a generalization of one or \nmore subtypes, while a subtype is a specialization of a supertype. For \nexample, a vehicle could be a supertype, while car, truck, and \nmotorcycle could be subtypes. \n\uf0b7 Generalization and Specialization: Generalization is the process of \nidentifying common attributes and relationships between entities and \ncreating a supertype based on these common features. Specialization \nis the process of identifying unique attributes and relationships \nbetween entities and creating subtypes based on these unique features. \n \n \n\uf0b7 Inheritance: Inheritance is a mechanism that allows subtypes to \ninherit attributes and relationships from their supertype. This means \nthat any attribute or relationship defined for a supertype is \nautomatically inherited by all its subtypes. \n\uf0b7 Constraints: The EER model allows for the specification of \nconstraints that must be satisfied by entities and relationships. \nExamples of constraints include cardinality constraints, which specify \nthe number of relationships that can exist between entities, and \nparticipation constraints, which specify whether an entity is required \nto participate in a relationship. \n\uf0b7 Overall, the EER model provides a powerful and flexible way to model \ncomplex data relationships, making it a popular choice for database \ndesign. An Enhanced Entity-Relationship (EER) model is an extension \nof the traditional Entity-Relationship (ER) model that includes \nadditional features to represent complex relationships between entities \nmore accurately. Some of the main features of the EER model are: \n\uf0b7 Subclasses and Super classes: EER model allows for the creation of \na hierarchical structure of entities where a superclass can have one or \nmore subclasses. Each subclass inherits attributes and relationships \nfrom its superclass, and it can also have its unique attributes and \nrelationships. \n\uf0b7 Specialization and Generalization: EER model uses the concepts of \nspecialization and generalization to create a hierarchy of entities. \nSpecialization is the process of defining subclasses from a superclass, \nwhile generalization is the process of defining a superclass from two \nor more subclasses. \n\uf0b7 Attribute Inheritance: EER model allows attributes to be inherited \nfrom a superclass to its subclasses. This means that attributes defined \nin the superclass are automatically inherited by all its subclasses. \n\uf0b7 Union Types: EER model allows for the creation of a union type, \nwhich is a combination of two or more entity types. The union type \ncan have attributes and relationships that are common to all the entity \ntypes that make up the union. \n\uf0b7 Aggregation: EER model allows for the creation of an aggregate \nentity that represents a group of entities as a single entity. The \naggregate entity has its unique attributes and relationships. \n\uf0b7 Multi-valued Attributes: EER model allows an attribute to have \nmultiple values for a single entity instance. For example, an entity \nrepresenting a person may have multiple phone numbers. \n\uf0b7 Relationships with Attributes: EER model allows relationships \nbetween entities to have attributes. These attributes can describe the \nnature of the relationship or provide additional information about the \nrelationship. \n \n \n \n7.4. Mapping from ER Model to Relational Model \n \nAfter designing the ER diagram of system, we need to convert it to Relational \nmodels which can directly be implemented by any RDBMS like Oracle, MySQL \netc.  In this article we will discuss how to convert ER diagram to Relational \nModel for different scenarios.  \nCase 1:  Binary Relationship with 1:1 cardinality with total participation \nof an entity  \n                   Figure 20: Mapping ER Model to Relational Model \nA person has 0 or 1 passport number and Passport is always owned by 1 person. \nSo, it is 1:1 cardinality with full participation constraint from Passport.  \nFirst Convert each entity and relationship to tables.  Person table \ncorresponds to Person Entity with key as Per-Id. Similarly, Passport table \ncorresponds to Passport Entity with key as Pass-No. Has Table represents \nrelationship between Person and Passport (Which person has which passport). \nSo, it will take attribute Per-Id from Person and Pass-No from Passport.  \nPerson \n \nHas \n \nPassport \nPer-Id \nOther \nPerson \nAttribute  \nPer-\nId \nPass-\nNo \n \nPass-No \nOther \nPassportAttribute \nPR1 \n\u2013 \n \nPR1 \nPS1 \n \nPS1 \n\u2013 \nPR2 \n\u2013 \n \nPR2 \nPS2 \n \nPS2 \n\u2013 \nPR3 \n\u2013 \n  \n  \n  \n  \n  \n  \n                                              Table 1 \nAs we can see from Table 1, each Per-Id and Pass-No has only one entry \nin  Has Table. So we can merge all three tables into 1 with attributes shown in \nTable 2. Each Per-Id will be unique and not null. So it will be the key. Pass-No \ncan\u2019t be key because for some person, it can be NULL.  \nPer-ID \nOther Person Attribute \nPass-No \nOther Passport Attribute \n                                               Table 2 \n \n \nCase 2: Binary Relationship with 1:1 cardinality and partial participation \nof both entities  \n                               Figure 21: Binary Relationship 1:1 \nA male marries 0 or 1 female and vice versa as well. So, it is 1:1 cardinality with \npartial participation constraint from both. First Convert each entity and \nrelationship to tables.  Male table corresponds to Male Entity with key as M-Id. \nSimilarly Female table corresponds to Female Entity with key as F-Id. Marry \nTable represents relationship between Male and Female (Which Male marries \nwhich female). So, it will take attribute M-Id from Male and F-Id from Female.  \nMale \n \nMarry \nFemale \nM-Id \nOther \nMale \nAttribute  \nM-Id \nF-Id \n \nF-Id \nOther \nFemaleAttribute \nM1 \n\u2013 \n \nM1 \nF2 \n \nF1 \n\u2013 \nM2 \n\u2013 \n \nM2 \nF1 \n \nF2 \n\u2013 \nM3 \n\u2013 \n  \n  \n  \n  \nF3 \n\u2013 \n                                               Table 3 \nAs we can see from Table 3, some males and some females do not marry. If \nwe merge 3 tables into 1, for some M-Id, F-Id will be NULL. So, there is no \nattribute which is always not NULL. So, we cannot merge all three tables into \n1. We can convert into 2 tables. In table 4, M-Id who are married will have F-\nId associated. For others, it will be NULL. Table 5 will have information of all \nfemales. Primary Keys have been underlined.  \nM-Id \nOther male Attribute \nF-Id \n                                Table 4 \nF-Id \nOther Female Attribute \n                         Table 5 \n \n \nCase 3: Binary Relationship with n: 1 cardinality  \n                    Figure 22: Binary Relationship with n:1 cardinality \nIn this scenario, every student can enroll only in one elective course but for an \nelective course there can be more than one student. First Convert each entity \nand relationship to tables.  Student table corresponds to Student Entity with key \nas S-Id. Similarly Elective_Course table corresponds to Elective_Course Entity \nwith key as E-Id. Enrolls Table represents relationship between Student and \nElective_Course (Which student enrolls in which course). So it will take \nattribute S-Id from Student and E-Id from Elective_Course.  \nStudent \n  \n Enrolls \nElective_Course \nS-Id \nOther \nStudent \nAttribute  \nS-Id \nE-\nId \n \nE-Id \nOther Elective \nCourseAttribute \nS1 \n\u2013 \n \nS1 \nE1  \nE1 \n\u2013 \nS2 \n\u2013 \n \nS2 \nE2  \nE2 \n\u2013 \nS3 \n\u2013 \n  \nS3 \nE1   \nE3 \n\u2013 \nS4 \n\u2013 \n  \nS4 \nE1   \n  \n  \n                                                 Table 6 \nAs we can see from Table 6, S-Id is not repeating in Enrolls Table. So, it can \nbe considered as a key of Enrolls table. Both Student and Enrolls Table\u2019s key \nis same; we can merge it as a single table. The resultant tables are shown in \nTable 7 and Table 8. Primary Keys have been underlined.  \nS-Id \nOther Student Attribute \nE-Id \n                               Table 7  \nE-Id \nOther Elective Course Attribute \n                           Table 8 \n \n \nCase 4: Binary Relationship with m: n cardinality \n                         Figure 23: Binary Relation with m: n cardinality \nIn this scenario, every student can enroll in more than 1 compulsory course and \nfor a compulsory course there can be more than 1 student. First Convert each \nentity and relationship to tables.  Student table corresponds to Student Entity \nwith key as S-Id. Similarly Compulsory_Courses table corresponds to \nCompulsory Courses Entity with key as C-Id. Enrolls Table represents \nrelationship between Student and Compulsory_Courses (Which student enrolls \nin which course). So, it will take attribute S-Id from Person and C-Id from \nCompulsory_Courses.  \nStudent \n \nEnrolls \nCompulsory_Courses \nS-\nId \nOther \nStudent \nAttribute  \nS-Id \nC-\nId \n \nC-Id \nOther \nCompulsory \nCourseAttribute \nS1 \n\u2013 \n S1 \nC1  C1 \n\u2013 \nS2 \n\u2013 \n S1 \nC2  C2 \n\u2013 \nS3 \n\u2013 \n  S3 \nC1   C3 \n\u2013 \nS4 \n\u2013 \n  S4 \nC3   C4 \n\u2013 \n  \n  \n  S4 \nC2     \n  \n  \n  \n  S3 \nC3     \n  \n                                               Table 9 \nAs we can see from Table 9, S-Id and C-Id both are repeating in Enrolls Table. \nBut its combination is unique; so it can be considered as a key of Enrolls table. \nAll tables\u2019 keys are different, these can\u2019t be merged.  Primary Keys of all tables \nhave been underlined.  \n \n \n \nCase 5: Binary Relationship with weak entity \n                     Figure 24: Binary Relationship with weak entity \nIn this scenario, an employee can have many dependents and one dependent can \ndepend on one employee. A dependent does not have any existence without an \nemployee (e.g; you as a child can be dependent of your father in his company). \nSo, it will be a weak entity and its participation will always be total. Weak Entity \ndoes not have key of its own. So, its key will be combination of key of its \nidentifying entity (E-Id of Employee in this case) and its partial key (D-Name).  \nFirst Convert each entity and relationship to tables.  Employee table corresponds \nto Employee Entity with key as E-Id. Similarly, Dependents table corresponds \nto Dependent Entity with key as D-Name and E-Id. Has Table represents \nrelationship between Employee and Dependents (Which employee has which \ndependents). So, it will take attribute E-Id from Employee and D-Name from \nDependents.  \nEmployee \nHas \nDependents \nE-\nId \nOther \nEmployee \nAttribute \n \nE-\nId \nD-Name \n \nD-Name E-\nId \nOther \nDependentsAttribute \nE1 \u2013 \n E1 RAM \n RAM \nE1 \u2013 \nE2 \u2013 \n E1 SRINI \n SRINI \nE1 \u2013 \nE3 \u2013 \n E2 RAM \n RAM \nE2 \u2013 \n  \n  \n E3 ASHISH  ASHISH E3 \u2013 \n                                                 Table 10 \nAs we can see from Table 10, E-Id, D-Name is key for Has as well as \nDependents Table. So, we can merge these two into 1. So the resultant tables \nare shown in Tables 11 and 12. Primary Keys of all tables have been underlined.  \nE-Id \nOther Employee Attribute \n              Table 11 \nD-Name \nE-Id \nOther DependentsAttribute \n                Table 12 \n \n \nRelational Model  \n \n8. Introduction to Relational Model \nE.F. Codd proposed the relational Model to model data in the form of relations \nor tables. After designing the conceptual model of the Database using ER \ndiagram, we need to convert the conceptual model into a relational model which \ncan be implemented using any RDBMS language like Oracle SQL, MySQL, etc. \nSo, we will see what the Relational Model is. \n \nWhat is the Relational Model?  \nThe relational model represents how data is stored in Relational Databases. A \nrelational database consists of a collection of tables, each of which is assigned \na unique name. Consider a relation STUDENT with attributes ROLL_NO, \nNAME, ADDRESS, PHONE, and AGE shown in the table. \nTable Student \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n2 \nRAMESH \nGURGAON 9652431543 \n18 \n3 \nSUJIT \nROHTAK \n9156253131 \n20 \n4 \nSURESH \nDELHI \n \n18 \nImportant Terminologies \n\uf0b7 Attribute: Attributes are the properties that define an entity. \ne.g., ROLL_NO, NAME, ADDRESS \n\uf0b7 Relation Schema: A relation schema defines the structure of the \nrelation and represents the name of the relation with its attributes. e.g., \nSTUDENT (ROLL_NO, NAME, ADDRESS, PHONE, and AGE) is \nthe relation schema for STUDENT. If a schema has more than 1 \nrelation, it is called Relational Schema. \n\uf0b7 Tuple: Each row in the relation is known as a tuple. The above relation \ncontains 4 tuples, one of which is shown as:   \n1 \nRAM \nDELHI \n9772667313 18 \n \n\uf0b7 Relation Instance: The set of tuples of a relation at a particular \ninstance of time is called a relation instance. Table 1 shows the relation \ninstance of STUDENT at a particular time. It can change whenever \nthere is an insertion, deletion, or update in the database. \n\uf0b7 Degree: The number of attributes in the relation is known as the \ndegree of the relation. The STUDENT relation defined above has \ndegree 5. \n \n \n\uf0b7 Cardinality: The number of tuples in a relation is known \nas cardinality. The STUDENT relation defined above has cardinality \n4. \n\uf0b7 Column: The column represents the set of values for a particular \nattribute. The column ROLL_NO is extracted from the relation \nSTUDENT. \nROLL_NO \n1 \n2 \n3 \n4 \n \n\uf0b7 NULL Values: The value which is not known or unavailable is called \na NULL value. It is represented by blank space. e.g., PHONE of \nSTUDENT having ROLL_NO 4 is NULL.  \n\uf0b7 Relation Key: These are basically the keys that are used to identify \nthe rows uniquely or also help in identifying tables. These are of the \nfollowing types. \n\uf0b7 Primary Key \n\uf0b7 Candidate Key \n\uf0b7 Super Key \n\uf0b7 Foreign Key \n\uf0b7 Alternate Key \n\uf0b7 Composite Key \n \n8.1. Coddy Rule and it\u2019s Features \nFeatures of the relational model and Codd\u2019s Rules : \nTables/Relations: The basic building block of the relational model is the table \nor relation, which represents a collection of related data. Each table consists of \ncolumns, also known as attributes or fields, and rows, also known as tuples or \nrecords. \nPrimary Keys: In the relational model, each row in a table must have a unique \nidentifier, which is known as the primary key. This ensures that each row is \nunique, can be accessed, and manipulated easily. \nForeign Keys: Foreign keys are used to link tables together and enforce \nreferential integrity. They ensure that data in one table is consistent with data in \nanother table. \nNormalization: The process of organizing data into tables and eliminating \nredundancy is known as normalization. Normalization is important in the \nrelational model because it helps to ensure that data is consistent and easy to \nmaintain. \n \n \n \n \nCodd\u2019s Rules \u2013  \nCodd\u2019s Rules are a set of 12 rules that define the characteristics of a true \nrelational DBMS. These rules ensure that the DBMS is consistent, reliable, and \neasy to use. \nAtomicity, Consistency, Isolation, Durability (ACID): The ACID properties are \na set of properties that ensure that transactions are processed reliably in the \nrelational model. Transactions are sets of operations that are executed as a single \nunit, ensuring that data is consistent and accurate. \n \nAdvantages of Relational Algebra \nRelational Algebra is a formal language used to specify queries to retrieve data \nfrom a relational database. It has several advantages that make it a popular \nchoice for managing and manipulating data. Here are some of the advantages of  \n \nRelational Algebra: \n\uf0b7 Simplicity: Relational Algebra provides a simple and easy-to-understand \nset of operators that can be used to manipulate data. It is based on a set of \nmathematical concepts and principles, which makes it easy to learn and \nuse. \n\uf0b7 Formality: Relational Algebra is a formal language that provides a \nstandardized and rigorous way of expressing queries. This makes it easier \nto write and debug queries, and also ensures that queries are correct and \nconsistent. \n\uf0b7 Abstraction: Relational Algebra provides a high-level abstraction of the \nunderlying database structure, which makes it easier to work with large \nand complex databases. It allows users to focus on the logical structure of \nthe data, rather than the physical storage details. \n\uf0b7 Portability: Relational Algebra is independent of any specific database \nmanagement system, which means that queries can be easily ported to \nother systems. This makes it easy to switch between different databases \nor vendors without having to rewrite queries. \n\uf0b7 Efficiency: Relational Algebra is optimized for efficiency and \nperformance, which means that queries can be executed quickly and with \nminimal resources. This is particularly important for large and complex \ndatabases, where performance is critical. \n\uf0b7 Extensibility: Relational Algebra provides a flexible and extensible \nframework that can be extended with new operators and functions. This \nallows developers to customize and extend the language to meet their \nspecific needs. \n \n \n \n \n \n \nDisadvantages of Relational Algebra \nWhile Relational Algebra has many advantages, it also has some limitations and \ndisadvantages that should be considered when using it.  \nHere are some of the disadvantages of Relational Algebra: \n\uf0b7 Complexity: Although Relational Algebra is based on mathematical \nprinciples, it can be complex and difficult to understand for non-experts. \nThe syntax and semantics of the language can be challenging, and it may \nrequire significant training and experience to use it effectively. \n\uf0b7 Limited Expressiveness: Relational Algebra has a limited set of \noperators, which can make it difficult to express certain types of queries. \nIt may be necessary to use more advanced techniques, such as subqueries \nor joins, to express complex queries. \n\uf0b7 Lack of Flexibility: Relational Algebra is designed for use with \nrelational databases, which means that it may not be well-suited for other \ntypes of data storage or management systems. This can limit its flexibility \nand applicability in certain contexts. \n\uf0b7 Performance Limitations: While Relational Algebra is optimized for \nefficiency and performance, it may not be able to handle large or complex \ndatasets. Queries can become slow and resource-intensive when dealing \nwith large amounts of data or complex queries. \n\uf0b7 Limited Data Types: Relational Algebra is designed for use with simple \ndata types, such as integers, strings, and dates. It may not be well-suited \nfor more complex data types, such as multimedia files or spatial data. \n\uf0b7 Lack of Integration: Relational Algebra is often used in conjunction \nwith other programming languages and tools, which can create integration \nchallenges. It may require additional programming effort to integrate \nRelational Algebra with other systems and tools. \nRelational Algebra is a powerful and useful tool for managing and manipulating \ndata in relational databases, it has some limitations and disadvantages that \nshould be carefully considered when using it. \n \nCodd\u2019s Twelve Rules of Relational Database \nCodd rules were proposed by E.F. Codd which should be satisfied by \nthe relational model. Codd\u2019s Rules are basically used to check whether DBMS \nhas the quality to become Relational Database Management System (RDBMS). \nBut, it is rare to find that any product has fulfilled all the rules of Codd. They \ngenerally follow the 8-9 rules of Codd. E.F. Codd has proposed 13 rules which \nare popularly known as Codd\u2019s 12 rules. These rules are stated as follows: \n\uf0b7 Rule 0: Foundation Rule\u2013 For any system that is advertised as, or \nclaimed to be, a relational database management system, that system \nmust be able to manage databases entirely through its relational \ncapabilities. \n \n \n\uf0b7 Rule 1: Information Rule\u2013 Data stored in the Relational model must \nbe a value of some cell of a table. \n\uf0b7 Rule 2: Guaranteed Access Rule\u2013 Every data element must be \naccessible by the table name, its primary key, and the name of the \nattribute whose value is to be determined. \n\uf0b7 Rule 3: Systematic Treatment of NULL values\u2013 NULL value in the \ndatabase must only correspond to missing, unknown, or not applicable \nvalues. \n\uf0b7 Rule 4: Active Online Catalog\u2013 The structure of the database must \nbe stored in an online catalog that can be queried by authorized users. \n\uf0b7 Rule 5: Comprehensive Data Sub-language Rule- A database \nshould be accessible by a language supported for definition, \nmanipulation, and transaction management operation. \n\uf0b7 Rule 6: View Updating Rule- Different views created for various \npurposes should be automatically updatable by the system. \n\uf0b7 Rule 7: High-level insert, update and delete rule- Relational Model \nshould support insert, delete, update, etc. operations at each level of \nrelations. Also, set operations like Union, Intersection, and minus \nshould be supported. \n\uf0b7 Rule 8: Physical data independence- Any modification in the \nphysical location of a table should not enforce modification at the \napplication level. \n\uf0b7 Rule 9: Logical data independence- Any modification in the logical \nor conceptual schema of a table should not enforce modification at the \napplication level. For example, merging two tables into one should not \naffect the application accessing it which is difficult to achieve. \n\uf0b7 Rule 10: Integrity Independence- Integrity constraints modified at \nthe database level should not enforce modification at the application \nlevel. \n\uf0b7 Rule 11: Distribution Independence- Distribution of data over \nvarious locations should not be visible to end-users. \n\uf0b7 Rule 12: Non-Subversion Rule- Low-level access to data should not \nbe able to bypass the integrity rule to change data. \n \nQuestion. Given the basic ER and relational models, which of the following is \nINCORRECT?  [GATE CS 2012]  \n(A) An attribute of an entity can have more than one value. \n(B) An attribute of an entity can be a composite. \n(C) In a row of a relational table, an attribute can have more than one value.  \n(D) In a row of a relational table, an attribute can have exactly one value or a \nNULL value.  \nAnswer: In the relation model, an attribute can\u2019t have more than one value. So, \noption 3 is the answer. \n \n \nConstraints in Relational Model \nWhile designing the Relational Model, we define some conditions which must \nhold for data present in the database are called Constraints. These constraints \nare checked before performing any operation (insertion, deletion, and updation) \nin the database. If there is a violation of any of the constraints, the operation will \nfail. \nDomain Constraints \nThese are attribute-level constraints. An attribute can only take values that lie \ninside the domain range. e.g.; If a constraint AGE>0 is applied to STUDENT \nrelation, inserting a negative value of AGE will result in failure. \nKey Integrity \nEvery relation in the database should have at least one set of attributes that \ndefines a tuple uniquely. Those set of attributes is called keys. e.g.; ROLL_NO \nin STUDENT is key. No two students can have the same roll number. So, a key \nhas two properties:  \n\uf0b7 It should be unique for all tuples. \n\uf0b7 It cannot have NULL values. \n \nReferential Integrity \nWhen one attribute of a relation can only takes values from another attribute of \nthe same relation or any other relation, it is called referential integrity. Let us \nsuppose we have 2 relations. \nTable Student  \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE BRANCH_CODE \n1 \nRAM \nDELHI \n9455123451 18 \nCS \n2 \nRAMESH GURGAON 9652431543 18 \nCS \n3 \nSUJIT \nROHTAK \n9156253131 20 \nECE \n4 \nSURESH \nDELHI \n \n18 \nIT \n \nTable Branch  \nBRANCH_CODE \nBRANCH_NAME \nCS \nCOMPUTER SCIENCE \nIT \nINFORMATION TECHNOLOGY \nECE \nELECTRONICS AND COMMUNICATION ENGINEERING \nCV \nCIVIL ENGINEERING \nBRANCH_CODE of STUDENT can only take the values which are present in \nBRANCH_CODE of BRANCH which is called referential integrity constraint. \nThe relation which is referencing another relation is called REFERENCING \nRELATION (STUDENT in this case) and the relation to which other relations \nrefer is called REFERENCED RELATION (BRANCH in this case).  \n \n \n \nRelational Schema and Instances \nRelational Schema \nIn the context of databases, a relational schema represents the blueprint or \nstructure of a relational database. It defines the organization of data in the form \nof tables, specifying the names of the tables, the names of the attributes \n(columns), and the data types associated with each attribute. The relational \nschema essentially outlines the framework that governs how data is stored and \norganized within the database. \n \nInstances \nInstances, refer to the actual data contained within the tables of a relational \ndatabase at a specific point in time. They represent the rows or records within \nthe tables, each row corresponding to a unique set of values that align with the \ndefined attributes in the schema. These instances reflect the real-world data that \nis stored, retrieved, and manipulated within the database system. \nUnderstanding the distinction between the relational schema, which outlines the \nstructure, and instances, which represent the actual data entries, is crucial for \ncomprehending how data is organized and managed within a relational database \nsystem. \n \n8.2. Anomalies in Relational Model \nAnomalies in the relational model refer to inconsistencies or errors that can arise \nwhen working with relational databases, specifically in the context of data \ninsertion, deletion, and modification. There are different types of anomalies that \ncan occur in referencing and referenced relations which can be discussed as:   \nThese anomalies can be categorized into three types: \n\uf0b7 Insertion Anomalies \n\uf0b7 Deletion Anomalies \n\uf0b7 Update Anomalies. \nHow Are Anomalies Caused in DBMS? \nDatabase anomalies are the faults in the database caused due to poor \nmanagement of storing everything in the flat database. It can be removed with \nthe process of Normalization, which generally splits the database which results \nin reducing the anomalies in the database. \nSTUDENT Table 1 \nSTUD_\nNO \nSTUD_NA\nME \nSTUD_PH\nONE \nSTUD_ST\nATE \nSTUD-\nCOUNTRY \nSTUD_AGE \n1 \nRAM \n9716271721 Haryana \nIndia \n20 \n2 \nRAM \n9898291281 Punjab \nIndia \n19 \n3 \nSUJIT \n7898291981 Rajasthan \nIndia \n18 \n4 \nSURESH \n  \nPunjab \nIndia \n21 \n                                       Table 1 \n \n \n \nSTUDENT_COURSE \nSTUD_NO \nCOURSE_NO \nCOURSE_NAME \n1 \nC1 \nDBMS \n2 \nC2 \nComputer Networks \n1 \nC2 \nComputer Networks \n                                          Table 2 \n \nInsertion Anomaly: If a tuple is inserted in referencing relation and referencing \nattribute value is not present in referenced attribute, it will not allow insertion \nin referencing relation.  \nExample:  \nIf we try to insert a record in STUDENT_COURSE with STUD_NO =7, it will \nnot allow it.  \n \nDeletion and Updation Anomaly: If a tuple is deleted or updated from \nreferenced relation and the referenced attribute value is used by referencing \nattribute in referencing relation, it will not allow deleting the tuple from \nreferenced relation. \nExample: If we want to update a record from STUDENT_COURSE with \nSTUD_NO =1, We must update it in both rows of the table. If we try to delete \na record from STUDENT with STUD_NO =1, it will not allow it.  \nTo avoid this, the following can be used in query: \n\uf0b7 ON DELETE/UPDATE SET NULL: If a tuple is deleted or updated \nfrom referenced relation and the referenced attribute value is used by \nreferencing attribute in referencing relation, it will delete/update the \ntuple from referenced relation and set the value of referencing attribute \nto NULL. \n\uf0b7 ON DELETE/UPDATE CASCADE: If a tuple is deleted or updated \nfrom referenced relation and the referenced attribute value is used by \nreferencing attribute in referencing relation, it will delete/update the \ntuple from referenced relation and referencing relation as well. \n \nHow These Anomalies Occur? \n\uf0b7 Insertion Anomalies: These anomalies occur when it is not possible \nto insert data into a database because the required fields are missing or \nbecause the data is incomplete. For example, if a database requires that \nevery record has a primary key, but no value is provided for a \nparticular record, it cannot be inserted into the database. \n\uf0b7 Deletion anomalies: These anomalies occur when deleting a record \nfrom a database and can result in the unintentional loss of data. For \nexample, if a database contains information about customers and \norders, deleting a customer record may also delete all the orders \nassociated with that customer. \n \n \n\uf0b7 Update anomalies:  These anomalies occur when modifying data in a \ndatabase and can result in inconsistencies or errors. For example, if a \ndatabase contains information about employees and their salaries, \nupdating an employee\u2019s salary in one record but not in all related \nrecords could lead to incorrect calculations and reporting. \n \nRemoval of Anomalies \nThese anomalies can be avoided or minimized by designing databases that \nadhere to the principles of normalization. Normalization involves organizing \ndata into tables and applying rules to ensure data is stored in a consistent and \nefficient manner. By reducing data redundancy and ensuring data integrity, \nnormalization helps to eliminate anomalies and improve the overall quality of \nthe database \nAccording to E.F.Codd, who is the inventor of the Relational Database, the \ngoals of Normalization include: \n\uf0b7 It helps in vacating all the repeated data from the database. \n\uf0b7 It helps in removing undesirable deletion, insertion, and update \nanomalies. \n\uf0b7 It helps in making a proper and useful relationship between tables.   \nAdvantages Anomalies in Relational Model \n\uf0b7 Data Integrity: Relational databases enforce data integrity through \nvarious constraints such as primary keys, foreign keys, and referential \nintegrity rules, ensuring that the data is accurate and consistent. \n\uf0b7 Scalability: Relational databases are highly scalable and can handle \nlarge amounts of data without sacrificing performance. \n\uf0b7 Flexibility: The relational model allows for flexible querying of data, \nmaking it easier to retrieve specific information and generate reports. \n\uf0b7 Security: Relational databases provide robust security features to \nprotect data from unauthorized access. \nDisadvantages of Anomalies in Relational Model \n\uf0b7 Redundancy: When the same data is stored in various locations, a \nrelational architecture may cause data redundancy. This can result in \ninefficiencies and even inconsistent data. \n\uf0b7 Complexity: Establishing and keeping up a relational database calls \nfor specific knowledge and abilities and can be difficult and time-\nconsuming. \n\uf0b7 Performance: Because more tables must be joined in order to access \ninformation, performance may degrade as a database gets larger. \n\uf0b7 Incapacity to manage unstructured data: Text documents, videos, \nand other forms of semi-structured or unstructured data are not well-\nsuited for the relational paradigm. \n \n8.3.  Key and Superkey Concepts \n \n \n \nKey: \nIn the context of a database, a key is a unique attribute or a set of attributes that \ncan uniquely identify each record in a table. It helps in maintaining the integrity \nand consistency of data by ensuring that there are no duplicate records. In \nsimpler terms, a key act as a unique identifier for each row in a table, enabling \nefficient retrieval, updating, and deletion of data. It serves as a primary means \nof establishing relationships between different tables within the database. \n \nSuperkey: \nA superkey refers to a set of one or more attributes that, when taken collectively, \ncan uniquely identify each record within a table. Unlike a key, a superkey may \ncontain additional attributes that are not necessary for uniquely identifying each \nrecord. This means that a superkey can have more attributes than the minimum \nrequired for uniqueness. In essence, a superkey acts as a broader identifier \nencompassing one or more keys within it. \nUnderstanding the distinction between a key and a superkey is crucial for \ndesigning efficient and reliable databases. Keys are the minimal set of attributes \nrequired for unique identification, while superkey represent a broader set of \nattributes that include the key and possibly more. The proper identification and \nutilization of keys and superkey are essential for maintaining data integrity and \nestablishing relationships within the database. \nAny set of attributes that allows us to identify unique rows (tuples) in a given \nrelationship is known as super keys. Out of these super keys, we can always \nchoose a proper subset among these that can be used as a primary key. Such \nkeys are known as Candidate keys. If there is a combination of two or more \nattributes that are being used as the primary key then we call it a Composite key. \n \n8.4. Tuple Relational Calculus \nTuple Relational Calculus (TRC) is a non-procedural query language used in \nrelational database management systems (RDBMS) to retrieve data from tables. \nTRC is based on the concept of tuples, which are ordered sets of attribute values \nthat represent a single row or record in a database table. \nTRC is a declarative language, meaning that it specifies what data is required \nfrom the database, rather than how to retrieve it. TRC queries are expressed as \nlogical formulas that describe the desired tuples. \nSyntax: The basic syntax of TRC is as follows: \n{ t | P(t) } \nwhere it is a tuple variable and P(t) is a logical formula that describes the \nconditions that the tuples in the result must satisfy. The curly braces {} are used \nto indicate that the expression is a set of tuples. \n \n \nFor example, let\u2019s say we have a table called \u201cEmployees\u201d with the \nfollowing attributes: \nEmployee ID \nName \nSalary \nDepartment ID \n \nTo retrieve the names of all employees who earn more than $50,000 per year, \nwe can use the following TRC query: \n{t | Employees(t) \u2227 t.Salary > 50000 } \nIn this query, the \u201cEmployees(t)\u201d expression specifies that the tuple variable t \nrepresents a row in the \u201cEmployees\u201d table. The \u201c\u2227\u201d symbol is the logical AND \noperator, which is used to combine the condition \u201ct.Salary > 50000\u201d with the \ntable selection. \nThe result of this query will be a set of tuples, where each tuple contains the \nName attribute of an employee who earns more than $50,000 per year. \nTRC can also be used to perform more complex queries, such as joins and nested \nqueries, by using additional logical operators and expressions. \nWhile TRC is a powerful query language, it can be more difficult to write and \nunderstand than other SQL-based query languages, such as Structured Query \nLanguage (SQL). However, it is useful in certain applications, such as in the \nformal verification of database schemas and in academic research. \nTuple Relational Calculus is a non-procedural query language, unlike \nrelational algebra. Tuple Calculus provides only the description of the query but \nit does not provide the methods to solve it. Thus, it explains what to do but not \nhow to do it. \n \nTuple Relational Query \nIn Tuple Calculus, a query is expressed as {t| P(t)} \nwhere t = resulting tuples,  \nP(t) = known as Predicate and these are the conditions that are used to fetch t. \nThus, it generates a set of all tuples t, such that Predicate P(t) is true for t. \nP(t) may have various conditions logically combined with OR (\u2228), AND (\u2227), \nNOT(\u00ac).  \nIt also uses quantifiers: \n\u2203 t \u2208 r (Q(t)) =\u201d there exists\u201d a tuple in t in relation r such that predicate Q(t) \nis true. \n\u2200 t \u2208 r (Q(t)) = Q(t) is true \u201cfor all\u201d tuples in relation r. \n \n \n \n5.2 Tuple Relational Calculus Examples \nTable Customer  \nCustomer Name Street \nCity \nSaurabh \nA7 \nPatiala \nMehak \nB6 \nJalandhar \nSumiti \nD9 \nLudhiana \nRia \nA5 \nPatiala \n \nTable Branch  \nBranch Name \nBranch City \nABC \nPatiala \nDEF \nLudhiana \nGHI \nJalandhar \n \nTable Account \nAccount number \nBranch name \nBalance \n1111 \nABC \n50000 \n1112 \nDEF \n10000 \n1113 \nGHI \n9000 \n1114 \nABC \n7000 \n \nTable Loan \nLoan number \nBranch name \nAmount \nL33 \nABC \n10000 \nL35 \nDEF \n15000 \nL49 \nGHI \n9000 \nL98 \nDEF \n65000 \n \nTable Borrower  \nCustomer name \nLoan number \nSaurabh \nL33 \nMehak \nL49 \nRia \nL98 \n \nTable Depositor  \nCustomer name \nAccount number \nSaurabh \n1111 \nMehak \n1113 \nSuniti \n1114 \n \n \n \n \nExample 1: Find the loan number, branch, and amount of loans greater than or \nequal to 10000 amounts. \n{t| t \u2208 loan \u2227 t[amount]>=10000} \nResulting relation: \nLoan number \nBranch name \nAmount \nL33 \nABC \n10000 \nL35 \nDEF \n15000 \nL98 \nDEF \n65000 \nIn the above query, t[amount] is known as a tuple variable. \n \n8.5. Extended Operation in Relational Algebra \nExtended operators are those operators which can be derived from basic \noperators. There are mainly three types of extended operators in Relational \nAlgebra: \n\uf0b7 Join \n\uf0b7 Intersection \n\uf0b7 Divide  \nThe relations used to understand extended operators are STUDENT, \nSTUDENT_SPORTS, ALL_SPORTS and EMPLOYEE which are shown in \nTable 1, Table 2, Table 3, and Table 4, respectively.  \nSTUDENT Table (Table 1) \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n2 \nRAMESH \nGURGAON \n9652431543 \n18 \n3 \nSUJIT \nROHTAK \n9156253131 \n20 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n                                                 \n  STUDENT_SPORTS (Table 2) \nROLL_NO \nSPORTS \n1 \nBadminton \n2 \nCricket \n2 \nBadminton \n4 \nBadminton \n  \n ALL_SPORTS (Table 3) \nSPORTS \nBadminton \nCricket  \n \n \n \nEMPLOYEE  (Table 4) \nEMP_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n5 \nNARESH \nHISAR \n9782918192 \n22 \n6 \nSWETA \nRANCHI \n9852617621 \n21 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n \nIntersection (?): Intersection on two relations R1 and R2 can only be computed \nif R1 and R2 are union compatible (These two relations should have same \nnumber of attributes and corresponding attributes in two relations have same \ndomain). Intersection operator when applied on two relations as R1? R2 will \ngive a relation with tuples which are in R1 as well as R2. Syntax: \n Relation1? Relation2 \nExample: Find a person who is student as well as employee- STUDENT? \nEMPLOYEE   \nIn terms of basic operators (union and minus): \nSTUDENT ? EMPLOYEE = STUDENT + EMPLOYEE - (STUDENT U \nEMPLOYEE)  \nRESULT: \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n \nConditional Join(?c): Conditional Join is used when you want to join two or \nmore relation based on some conditions. Example: Select students whose \nROLL_NO is greater than EMP_NO of employees \nSTUDENT?c STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE \nIn terms of basic operators (cross product and selection) : \n? (STUDENT.ROLL_NO>EMPLOYEE.EMP_NO)(STUDENT\u00d7EMPLOYEE) \nRESULT: \nROL \nL_NO \nNAME \nADDRESS \nPHONE \nAGE \nEMP_NO NAME ADDRESS \n2 \nRAMESH GURGAON \n9652431543 18 \n1 \nRAM \nDELHI \n3 \nSUJIT \nROHTAK \n9156253131 20 \n1 \nRAM \nDELHI \n4 \nSURESH \nDELHI \n9156768971 18 \n1 \nRAM \nDELHI \n \nEquijoin(?): Equijoin is a special case of conditional join where only equality \ncondition holds between a pair of attributes. As values of two attributes will be \nequal in result of equijoin, only one attribute will be appeared in result. \nExample: Select  students whose ROLL_NO is equal to EMP_NO of \nemployees. \nSTUDENT?STUDENT.ROLL_NO=EMPLOYEE.EMP_NOEMPLOYEE \n \n \nIn terms? of basic operators (cross product, selection and projection) : \n?(STUDENT.ROLL_NO, STUDENT.NAME, STUDENT.ADDRESS, STUDENT.PHONE, STUDENT.AGE \nEMPLOYEE.NAME, \nEMPLOYEE.ADDRESS, \nEMPLOYEE.PHONE, \nEMPLOYEE>AGE)(? \n(STUDENT.ROLL_NO=EMPLOYEE.EMP_NO) (STUDENT\u00d7EMPLOYEE)) \nRESULT: \nRO \nLL_NO \nNAME \nADD \nRESS \nPHONE \nAGE NAME \nADD \nRESS \nPHONE \nAGE\n1 \nRAM \nDELHI 9455123451 18 \nRAM \nDELHI 9455123451 18 \n4 \nSURESH DELHI 9156768971 18 \nSURESH DELHI 9156768971 18 \n \nNatural Join(?): It is a special case of equijoin in which equality condition hold \non all attributes which have same name in relations R and S (relations on which \njoin operation is applied). While applying natural join on two relations, there is \nno need to write equality condition explicitly. Natural Join will also return the \nsimilar attributes only once as their value will be same in resulting relation. \nExample: Select students whose ROLL_NO is equal to ROLL_NO of \nSTUDENT_SPORTS as: \nSTUDENT?STUDENT_SPORTS \nIn terms of basic operators (cross product, selection and projection) : \n?(STUDENT.ROLL_NO, STUDENT.NAME, STUDENT.ADDRESS, STUDENT.PHONE, STUDENT.AGE \nSTUDENT_SPORTS.SPORTS)(? \n(STUDENT.ROLL_NO=STUDENT_SPORTS.ROLL_NO) \n(STUDENT\u00d7STUDENT_SPORTS)) \nRESULT: \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \nSPORTS \n1 \nRAM \nDELHI \n9455123451 \n18 \nBadminton \n2 \nRAMESH \nGURGAON \n9652431543 \n18 \nCricket \n2 \nRAMESH \nGURGAON \n9652431543 \n18 \nBadminton \n4 \nSURESH \nDELHI \n9156768971 \n18 \nBadminton \nNatural Join is by default inner join because the tuples which does not satisfy \nthe conditions of join does not appear in result set. e.g.; The tuple having \n \n \nROLL_NO 3 in STUDENT does not match with any tuple in \nSTUDENT_SPORTS, so it has not been a part of result set.  \nLeft Outer Join(?): When applying join on two relations R and S, some tuples \nof R or S does not appear in result set which does not satisfy the join conditions. \nBut Left Outer Joins gives all tuples of R in the result set. The tuples of R which \ndo not satisfy join condition will have values as NULL for attributes of S. \nExample:Select students whose ROLL_NO is greater than EMP_NO of \nemployees and details of other students as well \nSTUDENT?STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE \nRESULT \nROL\nL_N\nO \nNA\nME \nADD\nRESS \nPHO\nNE \nA\nG\nE \nEMP\n_NO \nNA\nM\nE \nADD\nRES\nS \nPHO\nNE \nA\nG\nE \n2 \nRA\nMES\nH \nGUR\nGAO\nN \n96524\n31543 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n3 \nSUJI\nT \nROH\nTAK \n91562\n53131 \n20 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n4 \nSUR\nESH \nDEL\nHI \n91567\n68971 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \nNUL\nL \nNU\nLL \nNUL\nL \nNUL\nL \nN\nUL\nL \nRight Outer Join(?): When applying join on two relations R and S, some tuples \nof R or S does not appear in result set which does not satisfy the join conditions. \nBut Right Outer Joins gives all tuples of S in the result set. The tuples of S which \ndo not satisfy join condition will have values as NULL for attributes of R. \nExample: Select students whose ROLL_NO is greater than EMP_NO of \nemployees and details of other Employees as well \nSTUDENT?STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE \nRESULT: \n \n \nROL\nL_N\nO \nNA\nME \nADD\nRESS \nPHO\nNE \nA\nG\nE \nEM\nP_N\nO \nNA\nME \nADD\nRES\nS \nPHO\nNE \nA\nG\nE \n2 \nRA\nMES\nH \nGUR\nGAO\nN \n96524\n31543 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n3 \nSUJI\nT \nROH\nTAK \n91562\n53131 \n20 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n4 \nSUR\nESH \nDEL\nHI \n91567\n68971 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n5 \nNA\nRES\nH \nHISA\nR \n97829\n18192 \n22 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n6 \nSW\nETA \nRAN\nCHI \n98526\n17621 \n21 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n4 \nSUR\nESH \nDEL\nHI \n91567\n68971 \n18 \nFull Outer Join(?): When applying join on two relations R and S, some tuples \nof R or S does not appear in result set which does not satisfy the join conditions. \nBut Full Outer Joins gives all tuples of S and all tuples of R in the result set. The \ntuples of S which do not satisfy join condition will have values as NULL for \nattributes of R and vice versa. Example: \nSelect students whose ROLL_NO is greater than EMP_NO of employees and \ndetails of other Employees as well and other Students as well \nSTUDENT?STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE \nRESULT: \n \n \nROL\nL_N\nO \nNA\nME \nADD\nRESS \nPHO\nNE \nA\nG\nE \nEM\nP_N\nO \nNA\nME \nADD\nRES\nS \nPHO\nNE \nA\nG\nE \n2 \nRA\nMES\nH \nGUR\nGAO\nN \n96524\n31543 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n3 \nSUJI\nT \nROH\nTAK \n91562\n53131 \n20 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \n4 \nSUR\nESH \nDEL\nHI \n91567\n68971 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n5 \nNA\nRES\nH \nHISA\nR \n97829\n18192 \n22 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n6 \nSW\nETA \nRAN\nCHI \n98526\n17621 \n21 \nNUL\nL \nNUL\nL \nNUL\nL \nNUL\nL \nN\nUL\nL \n4 \nSUR\nESH \nDEL\nHI \n91567\n68971 \n18 \n1 \nRA\nM \nDEL\nHI \n94551\n23451 \n18 \nNUL\nL \nNU\nLL \nNUL\nL \nNUL\nL \nN\nUL\nL \n \n \n \nAdvantages: \n\uf0b7 Expressive Power: Extended operators allow for more complex queries \nand transformations that cannot be easily expressed using basic relational \nalgebra operations. \n\uf0b7 Data Reduction: Aggregation operators, such as SUM, AVG, COUNT, \nand MAX, can reduce the amount of data that needs to be processed and \ndisplayed. \n\uf0b7 Data Transformation: Extended operators can be used to transform data \ninto different formats, such as pivoting rows into columns or vice versa. \n\uf0b7 More Efficient: Extended operators can be more efficient than \nexpressing the same query in terms of basic relational algebra operations, \nsince they can take advantage of specialized algorithms and \noptimizations. \nDisadvantages: \n\uf0b7 Complexity: Extended operators can be more difficult to understand and \nuse than basic relational algebra operations. They require a deeper \nunderstanding of the underlying data and the operators themselves. \n\uf0b7 Performance: Some extended operators, such as outer joins, can be \nexpensive in terms of performance, especially when dealing with large \ndata sets. \n\uf0b7 Non-standardized: There is no universal set of extended operators, and \ndifferent relational database management systems may implement them \ndifferently or not at all. \n\uf0b7 Data Integrity: Some extended operators, such as aggregate functions, \ncan introduce potential problems with data integrity if not used properly. \nFor example, using AVG on a column that contains null values can result \nin unexpected or incorrect results. \n \n \n \n \n \n \n \n \n \n \n \n \nDatabase Design \n \n \n \n9. Introduction to Database Normalization \n \nDatabase normalization is the process of organizing the attributes of the \ndatabase to reduce or eliminate data redundancy (having the same data but \nat different places).  \nProblems because of data redundancy: Data redundancy unnecessarily \nincreases the size of the database as the same data is repeated in many places. \nInconsistency problems also arise during insert, delete and update operations.  \nFunctional Dependency: Functional Dependency is a constraint between two \nsets of attributes in relation to a database. A functional dependency is denoted \nby an arrow (?). If an attribute A functionally determines B, then it is written as \nA ? B.  \nFor example, employee_id ? name means employee_id functionally determines \nthe name of the employee. As another example in a timetable database, \n{student_id, time} ? {lecture_room}, student ID and time determine the lecture \nroom where the student should be.  \nAdvantages of Functional Dependency \n\uf0b7 The database\u2019s data quality is maintained using it.  \n\uf0b7 It communicates the database design\u2019s facts.  \n\uf0b7 It aids in precisely outlining the limitations and implications of \ndatabases.  \n\uf0b7 It is useful to recognize poor designs.  \n\uf0b7 Finding the potential keys in the relationship is the first step in \nthe normalization procedure. \nIdentifying \npotential \nkeys \nand \nnormalizing the database without functional dependencies is \nimpossible.  \n \nWhat does functionally dependent mean?  \nA function dependency A ? B means for all instances of a particular value of A, \nthere is the same value of B. For example, in the below table A ? B is true, but \nB ? A is not true as there are different values of A for B = 3.  \nA \nB \n1 \n3 \n2 \n3 \n4 \n0 \n1 \n3 \n4 \n0 \n \n \nThe features of database normalization are as follows: \n\uf0b7 Elimination of Data Redundancy: One of the main features of \nnormalization is to eliminate the data redundancy that can occur in a \ndatabase. Data redundancy refers to the repetition of data in different parts \nof the database. Normalization helps in reducing or eliminating this \nredundancy, which can improve the efficiency and consistency of the \ndatabase. \n\uf0b7 Ensuring Data Consistency: Normalization helps in ensuring that the \ndata in the database is consistent and accurate. By eliminating \nredundancy, normalization helps in preventing inconsistencies and \ncontradictions that can arise due to different versions of the same data. \n\uf0b7 Simplification of Data Management: Normalization simplifies the \nprocess of managing data in a database. By breaking down a complex data \nstructure into simpler tables, normalization makes it easier to manage the \ndata, update it, and retrieve it. \n\uf0b7 Improved Database Design: Normalization helps in improving the \noverall design of the database. By organizing the data in a structured and \nsystematic way, normalization makes it easier to design and maintain the \ndatabase. It also makes the database more flexible and adaptable to \nchanging business needs. \n\uf0b7 Avoiding Update Anomalies: Normalization helps in avoiding update \nanomalies, which can occur when updating a single record in a table \naffects multiple records in other tables. Normalization ensures that each \ntable contains only one type of data and that the relationships between the \ntables are clearly defined, which helps in avoiding such anomalies. \n\uf0b7 Standardization: Normalization helps in standardizing the data in the \ndatabase. By organizing the data into tables and defining relationships \nbetween them, normalization helps in ensuring that the data is stored in a \nconsistent and uniform manner. \n \n \n9.1. Normal Forms & Dependency \n \nImportant Points Regarding Normal Forms in DBMS \n\uf0b7 First Normal Form (1NF): This is the most basic level of \nnormalization. In 1NF, each table cell should contain only a single \nvalue, and each column should have a unique name. The first normal \nform helps to eliminate duplicate data and simplify queries. \n\uf0b7 Second Normal Form (2NF): 2NF eliminates redundant data by \nrequiring that each non-key attribute be dependent on the primary key. \nThis means that each column should be directly related to the primary \nkey, and not to other columns. \n \n \n\uf0b7 Third Normal Form (3NF): 3NF builds on 2NF by requiring that all \nnon-key attributes be independent of each other. This means that each \ncolumn should be directly related to the primary key, and not to any \nother columns in the same table. \n\uf0b7 Boyce-Codd Normal Form (BCNF): BCNF is a stricter form of 3NF \nthat ensures that each determinant in a table is a candidate key. In other \nwords, BCNF ensures that each non-key attribute is dependent only on \nthe candidate key. \n\uf0b7 Fourth Normal Form (4NF): 4NF is a further refinement of BCNF \nthat ensures that a table does not contain any multi-valued \ndependencies. \n\uf0b7 Fifth Normal Form (5NF): 5NF is the highest level of normalization \nand involves decomposing a table into smaller tables to remove data \nredundancy and improve data integrity. \n \nNormal forms help to reduce data redundancy, increase data consistency, and \nimprove database performance. However, higher levels of normalization can \nlead to more complex database designs and queries. It is important to strike a \nbalance between normalization and practicality when designing a database. \n \nFirst, Second and Third Normal Form \n\uf076 First Normal Form \nIf a relation contains composite or multi-valued attribute, it violates first normal \nform, or a relation is in first normal form if it does not contain any composite or \nmulti-valued attribute. A relation is in first normal form if every attribute in that \nrelation is singled valued attribute. \n \n\uf0b7 Example 1 \u2013 Relation STUDENT in table 1 is not in 1NF because of \nmulti-valued attribute STUD_PHONE. Its decomposition into 1NF \nhas been shown in table 2. \n \n \n\uf076 Second Normal Form \nTo be in second normal form, a relation must be in first normal form and relation \nmust not contain any partial dependency. A relation is in 2NF if it has No \nPartial Dependency, i.e., no non-prime attribute (attributes which are not part \nof any candidate key) is dependent on any proper subset of any candidate key \nof the table. Partial Dependency \u2013 If the proper subset of candidate key \ndetermines non-prime attribute, it is called partial dependency. \n \n\uf0b7 Example 1 \u2013 Consider table-3 as following below. \nSTUD_NO       COURSE_NO        COURSE_FEE \n1                             C1                                  1000 \n2                             C2                                  1500 \n1                             C4                                  2000 \n4                             C3                                  1000 \n4                             C1                   \n       1000 \n2                             C5                   \n       2000 \n \n{Note that, there are many courses having the same course fee} Here, \nCOURSE_FEE cannot alone decide the value of COURSE_NO or STUD_NO; \nCOURSE_FEE together with STUD_NO cannot decide the value of \nCOURSE_NO; COURSE_FEE together with COURSE_NO cannot decide the \nvalue of STUD_NO; Hence, COURSE_FEE would be a non-prime attribute, as \nit does not belong to the one only candidate key {STUD_NO, COURSE_NO} ; \nBut, COURSE_NO -> COURSE_FEE, i.e., COURSE_FEE is dependent on \nCOURSE_NO, which is a proper subset of the candidate key. Non-prime \nattribute COURSE_FEE is dependent on a proper subset of the candidate key, \nwhich is a partial dependency and so this relation is not in 2NF. To convert the \nabove relation to 2NF, we need to split the table into two tables such as :  \nTable 1: STUD_NO, COURSE_NO     Table 2: COURSE_NO, COURSE_FEE \n       Table 1                                                           Table 2 \nSTUD_NO         COURSE_NO            COURSE_NO        COURSE_FEE                \n1                 \n \nC1                          C1                        1000 \n2             \n \nC2                         C2                        1500 \n1                 \n \nC4                            C3                        1000 \n4                \n \nC3                            C4                        2000 \n3                \n \nC1                            C5                        2000         \n \n \nNOTE: 2NF tries to reduce the redundant data getting stored in memory. For \ninstance, if there are 100 students taking C1 course, we don\u2019t need to store its \nFee as 1000 for all the 100 records, instead, once we can store it in the second \ntable as the course fee for C1 is 1000. \n \n \n \n\uf076 Third Normal Form \nA relation is said to be in third normal form, if we did not have any transitive \ndependency for non-prime attributes. The basic condition with the Third Normal \nForm is that, the relation must be in Second Normal Form. \nBelow mentioned is the basic condition that must be hold in the non-trivial \nfunctional dependency X -> Y: \n\uf0b7 X is a Super Key. \n\uf0b7 Y is a Prime Attribute (this means that element of Y is some part of \nCandidate Key). \n \nBoyce-Codd Normal Form (BCNF), Fourth and Fifth Normal Form \n\uf076 BCNF (Boyce-Codd Normal Form) \nBCNF (Boyce-Codd Normal Form) is just an advanced version of Third Normal \nForm. Here we have some additional rules than Third Normal Form. The basic \ncondition for any relation to be in BCNF is that it must be in Third Normal \nForm. \nWe must focus on some basic rules that are for BCNF: \n\uf0b7 Table must be in Third Normal Form. \n\uf0b7 In relation X->Y, X must be a superkey in a relation. \n\uf076 Fourth Normal Form \nFourth Normal Form contains no non-trivial multivalued dependency except \ncandidate key. The basic condition with Fourth Normal Form is that the relation \nmust be in BCNF. \nThe basic rules are mentioned below. \n\uf0b7 It must be in BCNF. \n\uf0b7 It does not have any multi-valued dependency. \n \n\uf076 Fifth Normal Form \nFifth Normal Form is also called as Projected Normal Form. The basic \nconditions of Fifth Normal Form are mentioned below. \nRelation must be in Fourth Normal Form. \nThe relation must not be further non loss decomposed. \n \n\uf076 How To Find the Highest normal Form of a relation \nSteps to find the highest normal form of relation:  \nStep 1. Find all possible candidate keys of the relation. \nStep 2. Divide all attributes into two categories: prime attributes and non-prime \nattributes. \nStep 3. Check for 1st normal form then 2nd and so on. If it fails to satisfy the \nnth normal form condition, the highest normal form will be n-1. \n \n \n \nExample 1. Find the highest normal form of a relation R(A,B,C,D,E) with \nFD set {A->D, B->A, BC->D, AC->BE}  \n \nStep 1.   As we can see, (AC)+ ={A, C, B, E, D}  but none of its subsets can \ndetermine all attributes of relation, So AC will be the candidate key. A can be \nderived from B, so we can replace A in AC with B. So BC will also be a \ncandidate key. So, there will be two candidate keys {AC, BC}. \nStep 2.  The prime attribute is that attribute which is part of candidate key {A, \nB, C} in this example and others will be non-prime {D, E} in this example. \nStep 3.  The relation R is in 1st normal form as a relational DBMS does not \nallow multi-valued or composite attributes. \nThe relation is not in the 2nd Normal form because A->D is partial dependency \n(A which is a subset of candidate key AC is determining non-prime attribute D) \nand the 2nd normal form does not allow partial dependency. \nSo, the highest normal form will be the 1st Normal Form. \n \nFunction Dependency \nA functional dependency A->B in a relation holds if two tuples having the same \nvalue of attribute A also have the same value for attribute B. For Example, in \nrelation to STUDENT shown in Table 1, Functional Dependencies  \nSTUD_NO->STUD_NAME, STUD_NO->STUD_PHONE hold but  \nSTUD_NAME->STUD_STATE do not hold. \nAdvantages of Functional Dependencies \n\uf0b7 Through the identification and removal of redundant or unneeded data, \nthey aid in the reduction of data redundancy in databases. \n\uf0b7 By guaranteeing that data is correct and consistent throughout the \ndatabase, they enhance data integrity. \n\uf0b7 They make it simpler to add, edit, and remove data, which helps with \ndatabase management. \n \nDisadvantages of Functional Dependencies \n\uf0b7 The process of identifying functional dependencies can be time-\nconsuming and complex, especially in large databases with many \ntables and relationships. \n \n \n\uf0b7 Overly restrictive functional dependencies can result in slow query \nperformance or data inconsistencies, as data that should be related may \nnot be properly linked. \n\uf0b7 Functional dependencies do not consider the semantic meaning of data \nand may not always reflect the true relationships between data \nelements. \n \n \n9.2. Attribute Clouser and Candidate Key \n \nAttribute Closure \nAttribute closure of an attribute set can be defined as set of attributes which can \nbe functionally determined from it. \n  \nHow to find attribute closure of an attribute set? \nTo find attribute closure of an attribute set:  \n\uf0b7 Add elements of attribute set to the result set. \n\uf0b7 Recursively add elements to the result set which can be functionally \ndetermined from the elements of the result set. \nUsing FD set of table 1, attribute closure can be determined as:  \n(STUD_NO) \n+ \n= \n{STUD_NO, \nSTUD_NAME, \nSTUD_PHONE, \nSTUD_STATE, \nSTUD_COUNTRY, \nSTUD_AGE} \n(STUD_STATE) + = {STUD_STATE, STUD_COUNTRY} \n \nAdvantages of Attribute Closure \n\uf0b7 Attribute closures help to identify all possible attributes that can be \nderived from a set of given attributes. \n\uf0b7 They facilitate database design by identifying relationships between \nattributes and tables, which can help to optimize query performance. \n\uf0b7 They ensure data consistency by identifying all possible combinations \nof attributes that can exist in the database. \n \nDisadvantages of Attribute Closure \n\uf0b7 The process of calculating attribute closures can be computationally \nexpensive, especially for large datasets. \n\uf0b7 Attribute closures can become too complex to manage, especially as \nthe number of attributes and tables in a database grows. \n\uf0b7 Attribute closures do not take into account the semantic meaning of \ndata and may not always accurately reflect the relationships between \ndata elements. \n \n \n \n \n \nHow to Find Candidate Keys and Super Keys Using Attribute Closure? \n\uf0b7 If attribute closure of an attribute set contains all attributes of relation, \nthe attribute set will be super key of the relation. \n\uf0b7 If no subset of this attribute set can functionally determine all \nattributes of the relation, the set will be candidate key as well. For \nExample, using FD set of tables 1, \n(STUD_NO, \nSTUD_NAME) \n+ \n= \n{STUD_NO, \nSTUD_NAME, \nSTUD_PHONE, STUD_STATE, STUD_COUNTRY, STUD_AGE}  \n(STUD_NO)+ \n= \n{STUD_NO, \nSTUD_NAME, \nSTUD_PHONE, \nSTUD_STATE, STUD_COUNTRY, STUD_AGE}  \n(STUD_NO, STUD_NAME) will be super key but not candidate key because \nits subset (STUD_NO)+ is equal to all attributes of the relation. So, STUD_NO \nwill be a candidate key.  \n \nPrime and Non-Prime Attributes \nAttributes which are parts of any candidate key of relation are called as prime \nattribute, others are non-prime attributes. For Example, STUD_NO in \nSTUDENT relation is prime attribute, others are non-prime attribute.  \n \nCandidate Key \nCandidate keys play an essential role in Database Management Systems \n(DBMS) by ensuring data integrity and efficient retrieval. A candidate key \nrefers to a set of attributes that can uniquely identify each record in a table. In \nthis article, we will explore the concept of candidate keys, their significance in \nDBMS, and their crucial role in optimizing databases. \n \nWhat is a Candidate Key? \nA candidate key is a minimal set of attributes that uniquely identifies each tuple \nwithin a table. In other words, there should not be any two rows in a table that \ncan have the same values for the columns that are the part of candidate key. It \nis very important for establishing relationships between tables and maintaining \ndata integrity. Candidate keys play a pivotal role in database normalization as \nthey help us to eliminate data redundancy and anomalies. \n \n \n                                     Figure 25: Key in DBMS \nExample of Candidate Key \nLet us try to understand, the concept of the candidate key with an example of a \nstudent table. \nStudent_id \nRoll_no \nName \nMobile_no \nEmail_id \nA1 \n1 \nDipak \n9120 \na@gmail.cpm \nA2 \n2 \nRaja \n8732 \nb@gmail.com \nA3 \n3 \nDipak \n8344 \nc@gmail.com \n \nIn this table, each student can uniquely identify by any of the following attribute: \nStudent_id, Roll_no, Mobile_no, Email_id. So let primary key is Student_id \nand Candidate keys are Student_id, Roll_no, Mobile_no, Email_id. \n \n9.3.  Lossless Decomposition and Dependency Preserving \nDecomposition \n \nWhat is Lossless Decomposition? \nLossless join decomposition is a decomposition of a relation R into relations R1, \nand R2 such that if we perform a natural join of relation R1 and R2, it will return \nthe original relation R. This is effective in removing redundancy from databases \nwhile preserving the original data. \nIn other words by lossless decomposition, it becomes feasible to reconstruct the \nrelation R from decomposed tables R1 and R2 by using Joins. \n0 seconds of 17 seconds Volume 0% \n \n \n  \nOnly 1NF,2NF,3NF, and BCNF are valid for lossless join decomposition. \nIn Lossless Decomposition, we select the common attribute and the criteria for \nselecting a common attribute is that the common attribute must be a candidate \nkey or super key in either relation R1, R2, or both. \nDecomposition of a relation R into R1 and R2 is a lossless-join decomposition \nif at least one of the following functional dependencies is in F+ (Closure of \nfunctional dependencies)  \n \nExample of Lossless Decomposition \n\u2014 Employee (Employee_Id, Ename, Salary, Department_Id, Dname) \nCan be decomposed using lossless decomposition as,  \n\u2014 \nEmployee_desc \n(Employee_Id, \nEname, \nSalary, \nDepartment_Id)  \n\u2014 \nDepartment_desc \n(Department_Id, \nDname) \nAlternatively the lossy decomposition would be as joining these tables is not \npossible so not possible to get back original data. \n\u2013 \nEmployee_desc \n(Employee_Id, \nEname, \nSalary)  \n\u2013 Department_desc (Department_Id, Dname) \nR1 \u2229 R2 \u2192 R1 \n   OR \nR1 \u2229 R2 \u2192 R2 \nIn a database management system (DBMS), a lossless decomposition is a \nprocess of decomposing a relation schema into multiple relations in such a way \nthat it preserves the information contained in the original relation. Specifically, \na lossless decomposition is one in which the original relation can be \nreconstructed by joining the decomposed relations. \nTo achieve lossless decomposition, a set of conditions known as Armstrong\u2019s \naxioms can be used. These conditions ensure that the decomposed relations will \nretain all the information present in the original relation. Specifically, the two \nmost important axioms for lossless decomposition are the reflexivity and the \ndecomposition axiom. \n \nThe reflexivity axiom states that if a set of attributes is a subset of another set \nof attributes, then the larger set of attributes can be inferred from the smaller \nset. The decomposition axiom states that if a relation R can be decomposed into \ntwo relations R1 and R2, then the original relation R can be reconstructed by \ntaking the natural join of R1 and R2. \nThere are several algorithms available for performing lossless decomposition in \nDBMS, such as the BCNF (Boyce-Codd Normal Form) decomposition and \nthe 3NF (Third Normal Form) decomposition. These algorithms use a set of \nrules to decompose a relation into multiple relations while ensuring that the \noriginal relation can be reconstructed without any loss of information. \n \n \nAdvantages of Lossless Decomposition \n\uf0b7 Reduced Data Redundancy: Lossless decomposition helps in reducing \nthe data redundancy that exists in the original relation. This helps in \nimproving the efficiency of the database system by reducing storage \nrequirements and improving query performance. \n\uf0b7 Maintenance and Updates: Lossless decomposition makes it easier to \nmaintain and update the database since it allows for more granular \ncontrol over the data. \n\uf0b7 Improved Data Integrity: Decomposing a relation into smaller \nrelations can help to improve data integrity by ensuring that each \nrelation contains only data that is relevant to that relation. This can help \nto reduce data inconsistencies and errors. \n\uf0b7 Improved Flexibility: Lossless decomposition can improve the \nflexibility of the database system by allowing for easier modification of \nthe schema. \n \nDisadvantages of Lossless Decomposition \n\uf0b7 Increased Complexity: Lossless decomposition can increase the \ncomplexity of the database system, making it harder to understand \nand manage. \n\uf0b7 Increased Processing Overhead: The process of decomposing a \nrelation into smaller relations can result in increased processing \noverhead. This can lead to slower query performance and reduced \nefficiency. \n\uf0b7 Join Operations: Lossless decomposition may require additional \njoin operations to retrieve data from the decomposed relations. This \ncan also result in slower query performance. \n\uf0b7 Costly: Decomposing relations can be costly, especially if the \ndatabase is large and complex. This can require additional resources, \nsuch as hardware and personnel. \n \nDependency Prevention Decomposition \nDependency Preservation \nA Decomposition D = {R1, R2, R3\u2026Rn } of R is dependency preserving wrt a \nset F of Functional dependency if \n(F1 ? F2 ? \u2026 ? Fm) + = F+. \nConsider a relation R \nR ---> F {...with some functional dependency (FD)....} \nR is decomposed or divided into R1 with FD { f1 } and R2 with { f2 }, then \nthere can be three cases: \nf1 U f2 = F -----> Decomposition is dependency preserving.  \nf1 U f2 is a subset of F -----> Not Dependency preserving. \nf1 U f2 is a super set of F -----> This case is not possible. \n \n \nProblem: \nLet a relation R (A, B, C, D) and functional dependency {AB \u2013> C, C \u2013> D, D \n\u2013> A}. Relation R is decomposed into R1(A, B, C) and R2(C, D). Check \nwhether decomposition is dependency preserving or not.  \nSolution: \n0 seconds of 17 seconds Volume 0% \nR1(A, B, C) and R2(C, D) \nLet us find closure of F1 and F2 \nTo find closure of F1, consider all combination of \nABC. i.e., find closure of A, B, C, AB, BC and AC \nNote ABC is not considered as it is always ABC  \nclosure(A) = { A }  // Trivial \nclosure(B) = { B }  // Trivial \nclosure(C) = {C, A, D} but D can't be in closure as D is not present R1. \n           = {C, A} \nC--> A   // Removing C from right side as it is trivial attribute \nclosure(AB) = {A, B, C, D} = {A, B, C} \nAB --> C // Removing AB from right side as these are trivial attributes \nclosure(BC) = {B, C, D, A} = {A, B, C} \nBC --> A  // Removing BC from right side as these are trivial attributes \nclosure(AC) = {A, C, D} \n \nNULL SET \nF1 {C--> A, AB --> C, BC --> A}. \nSimilarly F2 { C--> D } \nIn the original Relation Dependency { AB --> C , C --> D , D --> A}. \nAB --> C is present in F1. \nC --> D is present in F2. \nD --> A is not preserved. \nF1 U F2 is a subset of F. So given decomposition is not dependency \npreserving. \n \n \n \nLossless Join Decomposition \nIf we decompose a relation R into relations R1 and R2, \nDecomposition is lossy if R1 \u22c8 R2 \u2283 R \nDecomposition is lossless if R1 \u22c8 R2 = R \n \nTo check for lossless join decomposition using the FD set, the following \nconditions must hold:  \ni) \nThe Union of Attributes of R1 and R2 must be equal to the \nattribute of R. Each attribute of R must be either in R1 or in R2. \n0 seconds of 17 secondsVolume 0% \n Att(R1) U Att(R2) = Att(R) \nii) \nThe intersection of Attributes of R1 and R2 must not be NULL. \n Att(R1) \u2229 Att(R2) \u2260 \u03a6 \niii) \nThe common attribute must be a key for at least one relation (R1 \nor R2) \n Att(R1) \u2229 Att(R2) -> Att(R1) or Att(R1) \u2229 Att(R2) -> Att(R2) \n \nDependency Preserving Decomposition \nIf we decompose a relation R into relations R1 and R2, All dependencies of R \neither must be a part of R1 or R2 or must be derivable from a combination \nof functional dependency of R1 and R2. For Example, A relation R (A, B, C, D) \nwith FD set{A->BC} is decomposed into R1(ABC) and R2(AD) which is \ndependency preserving because FD A->BC is a part of R1(ABC).  \n \nAdvantages of Lossless Join and Dependency Preserving Decomposition \n\uf0b7 Improved Data Integrity: Lossless join and dependency preserving \ndecomposition help to maintain the data integrity of the original \nrelation by ensuring that all dependencies are preserved. \n\uf0b7 Reduced Data Redundancy: These techniques help to reduce data \nredundancy by breaking down a relation into smaller, more \nmanageable relations. \n\uf0b7 Improved Query Performance: By breaking down a relation into \nsmaller, more focused relations, query performance can be improved. \n\uf0b7 Easier Maintenance and Updates: The smaller, more focused \nrelations are easier to maintain and update than the original relation, \nmaking it easier to modify the database schema and update the data. \n\uf0b7 Better Flexibility: Lossless join and dependency preserving \ndecomposition can improve the flexibility of the database system by \nallowing for easier modification of the schema. \n \n \n \nDisadvantages of Lossless Join and Dependency Preserving Decomposition \n\uf0b7 Increased Complexity: Lossless join and dependency-preserving \ndecomposition can increase the complexity of the database system, \nmaking it harder to understand and manage. \n\uf0b7 Costly: Decomposing relations can be costly, especially if the \ndatabase is large and complex. This can require additional resources, \nsuch as hardware and personnel. \n\uf0b7 Reduced Performance: Although query performance can be \nimproved in some cases, in others, lossless join and dependency-\npreserving decomposition can result in reduced query performance \ndue to the need for additional join operations. \n\uf0b7 Limited Scalability: These techniques may not scale well in larger \ndatabases, as the number of smaller, focused relations can become \nunwieldy. \n \n \n9.4. Equivalence of Function Dependencies \n \nFor understanding the equivalence of Functional Dependencies Sets (FD sets), \nthe basic idea about Attribute Closure is given in this article Given a Relation \nwith different FD sets for that relation, we have to find out whether one FD set \nis a subset of another or both are equal.  \nHow To Find the Relationship Between Two Functional Dependency \nSets?  \nLet FD1 and FD2 be two FD sets for a relation R. \ni) If all FDs of FD1 can be derived from FDs present in FD2, we can \nsay that FD2 \u2283 FD1. \nii) If all FDs of FD2 can be derived from FDs present in FD1, we can \nsay that FD1 \u2283 FD2. \niii) If 1 and 2 both are true, FD1=FD2. \n \nWhy We Need to Compare Functional Dependencies? \nSuppose in the designing process we convert the ER diagram to a relational \nmodel and this task is given to two different engineers.  \nNow those two engineers give two different sets of functional dependencies. So, \nbeing an administrator, we need to ensure that we must have a good set of \nFunctional Dependencies. To ensure this we require to study the equivalence of \nFunctional Dependencies. \n \nAdvantages \n\uf0b7 It can help to identify redundant functional dependencies, which can \nbe eliminated to reduce data redundancy and improve database \nperformance. \n \n \n\uf0b7 It can help to optimize database design by identifying equivalent sets \nof functional dependencies that can be used interchangeably. \n\uf0b7 It can ensure data consistency by identifying all possible \ncombinations of attributes that can exist in the database. \n \nDisadvantages \n\uf0b7 The process of determining the equivalence of functional \ndependencies can be computationally expensive, especially for large \ndatasets. \n\uf0b7 The process may require testing multiple candidates sets of \nfunctional dependencies, which can be time-consuming and complex. \n\uf0b7 The equivalence of functional dependencies may not always \naccurately reflect the semantic meaning of data and may not always \nreflect the true relationships between data elements. \n \nQuestion.1 Let us take an example to show the relationship between two FD \nsets. A relation R(A,B,C,D) having two FD sets FD1 = {A->B, B->C, AB->D} \nand FD2 = {A->B, B->C, A->C, A->D}  \nStep 1: Checking whether all FDs of FD1 are present in FD2 \n\uf0b7 A->B in set FD1 is present in set FD2. \n\uf0b7 B->C in set FD1 is also present in set FD2. \n\uf0b7 AB->D is present in set FD1 but not directly in FD2 but we will \ncheck whether we can derive it or not. For set FD2, (AB)+ = {A, B, \nC, D}. It means that AB can functionally determine A, B, C, and D. \nSo AB->D will also hold in set FD2. \nAs all FDs in set FD1 also hold in set FD2, FD2 \u2283 FD1 is true.  \n \n \n9.5. Canonical Cover of Functional Dependencies \n \nWhenever a user updates the database, the system must check whether any of \nthe functional dependencies are getting violated in this process. If there is a \nviolation of dependencies in the new database state, the system must roll back. \nWorking with a huge set of functional dependencies can cause unnecessary \nadded computational time. This is where the canonical cover comes into play. \nA canonical cover of a set of functional dependencies F is a simplified set of \nfunctional dependencies that has the same closure as the original set F.  \nAn attribute of a functional dependency is said to be extraneous if we can \nremove it without changing the closure of the set of functional dependencies \n \n \n \n \n \n \nCanonical Cover \nIn DBMS, a canonical cover is a set of functional dependencies that is equivalent \nto a given set of functional dependencies but is minimal in terms of the number \nof dependencies. The process of finding the canonical cover of a set of \nfunctional dependencies involves three main steps: \n\uf0b7 Reduction: The first step is to reduce the original set of functional \ndependencies to an equivalent set that has the same closure as the \noriginal set, but with fewer dependencies. This is done by removing \nredundant dependencies and combining dependencies that have \ncommon attributes on the left-hand side. \n\uf0b7 Elimination: The second step is to eliminate any extraneous \nattributes from the left-hand side of the dependencies. An attribute is \nconsidered extraneous if it can be removed from the left-hand side \nwithout changing the closure of the dependencies. \n\uf0b7 Minimization: The final step is to minimize the number of \ndependencies by removing any dependencies that are implied by \nother dependencies in the set. \n\uf0b7 To illustrate the process, let\u2019s consider a set of functional \ndependencies: A -> BC, B -> C, and AB -> C. Here are the steps to \nfind the canonical cover: \n\uf0b7 Reduction: We can reduce the set by removing the redundant \ndependency B -> C and combining the two remaining dependencies \ninto one: A -> B, A -> C. \n\uf0b7 Elimination: We can eliminate the extraneous attribute B from the \ndependency A -> B, resulting in A -> C. \n\uf0b7 Minimization: We can minimize the set by removing the \ndependency AB -> C, which is implied by A -> C. \n\uf0b7 The resulting canonical cover for the original set of functional \ndependencies is A -> C. \nA canonical cover Fc of a set of functional dependencies F such that all the \nfollowing properties are satisfied: \n\uf0b7 F logically implies all dependencies in Fc. \n\uf0b7  Fc logically implies all dependencies in F. \n\uf0b7 No functional dependency in Fc contains an extraneous attribute. \n\uf0b7 Each left side of a functional dependency in Fc is unique. That is, \nthere are no two dependencies \u03b11 \u2192 \u03b21 and \u03b12 \u2192 \u03b22 in such \nthat \u03b11 \u2192 \u03b12. \nThe canonical cover is useful because it provides a simplified representation of \nthe original set of functional dependencies that can be used to determine the key, \nsuperkey, and candidate key for a relation, as well as to check \nfor normalization violations and perform other database design tasks. \n \n \n \n \nHow to Find Canonical Cover? \nBelow mentioned is the algorithm to compute canonical cover for set F. \ni) \nRepeat \nUse the union rule to replace any dependencies in \u03b11 \u2192 \u03b21 and \u03b12 \u2192 \n\u03b22 with \u03b11 \u2192 \u03b21\u03b22 \nii) \nFind a functional dependency \u03b1 \u2192 \u03b2 with an extraneous attribute either \nin \u03b1 or in \u03b2.     \niii) \nIf an extraneous attribute is found, delete it from \u03b1 \u2192 \u03b2. until F does \nnot change. \n \nExample 1: \nConsider the following set F of functional dependencies:  \nF= {A \u2192 BC, B \u2192 C A \u2192 B, AB \u2192 C}. Below mentioned are the steps to \nfind the canonical cover of the functional dependency given above. \nStep 1: There are two functional dependencies with the same attributes on the \nleft: A \u2192 BC, A \u2192 B. These two can be combined to get A \u2192 BC. Now, the \nrevised set F becomes F= {A \u2192 BC, B \u2192 C, AB \u2192 C}. \nStep 2: There is an extraneous attribute in AB \u2192 C because even after removing \nAB \u2192 C from the set F, we get the same closures. This is because B \u2192 C is \nalready a part of F. Now, the revised set F becomes: F= {A \u2192 BC, B \u2192 C} \nStep 3: C is an extraneous attribute in A \u2192 BC, also A \u2192 B is logically implied \nby A \u2192 B and B \u2192 C (by transitivity). F= {A \u2192 B B \u2192 C} \nStep 4: After this step, F does not change anymore. So, hence the required \ncanonical cover is, Fc = {A \u2192 B, B \u2192 C} \n \nFeatures of the Canonical Cover \n\uf0b7 Minimal: The canonical cover is the smallest set of dependencies \nthat can be derived from a given set of dependencies, i.e., it has the \nminimum number of dependencies required to represent the same set \nof constraints. \n\uf0b7 Lossless: The canonical cover preserves all the functional \ndependencies of the original set of dependencies, i.e., it does not lose \nany information. \n\uf0b7 Unique: The canonical cover is unique, i.e., there is only one \ncanonical cover for a given set of dependencies. \n\uf0b7 Deterministic: The canonical cover is deterministic, i.e., it does not \ncontain any redundant or extraneous dependencies. \n\uf0b7 Reduces data redundancy: The canonical cover helps to reduce \ndata redundancy by eliminating unnecessary dependencies that can \nbe inferred from other dependencies. \n\uf0b7 Improves query performance: The canonical cover helps to \nimprove query performance by reducing the number of joins and \nredundant data in the database. \n \n \n \nStructured Query language (SQL) \n \n10. \n SQL Overview \nStructured Query Language is a standard Database language that is used to \ncreate, maintain, and retrieve the relational database. In this article, we will \ndiscuss this in detail about SQL. Following are some interesting facts about \nSQL. Let us focus on that. \nSQL is case insensitive. But it is a recommended practice to use keywords (like \nSELECT, UPDATE, CREATE, etc.) in capital letters and use user-defined \nthings (like table name, column name, etc.) in small letters. \nWe can write comments in SQL using \u201c\u2013\u201d (double hyphen) at the beginning of \nany line. SQL is the programming language for relational databases (explained \nbelow) like MySQL, Oracle, Sybase, SQL Server, Postgrad, etc. Other non-\nrelational databases \n(also \ncalled NoSQL) \ndatabases \nlike \nMongoDB, \nDynamoDB, etc. do not use SQL. \nAlthough there is an ISO standard for SQL, most of the implementations slightly \nvary in syntax. So, we may encounter queries that work in SQL Server but do \nnot work in MySQL. \n \nWhat is Relational Database? \nA relational database means the data is stored as well as retrieved in the form of \nrelations (tables). Table 1 shows the relational database with only one relation \ncalled STUDENT which \nstores ROLL_NO, NAME, ADDRESS, PHONE, and AGE of students. \n \nSTUDENT Table \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n2 \nRAMESH \nGURGAON \n9652431543 \n18 \n3 \nSUJIT \nROHTAK \n9156253131 \n20 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n \nImportant Terminologies \nThese are some important terminologies that are used in terms of relation. \n\uf0b7 Attribute: Attributes are the properties that define a relation. \ne.g.; ROLL_NO, NAME etc. \n\uf0b7 Tuple: Each row in the relation is known as tuple. The above relation \ncontains 4 tuples, one of which is shown as: \n1 \nRAM \nDELHI \n9455123451 \n18 \n\uf0b7 Degree: The number of attributes in the relation is known as degree \nof the relation. The STUDENT relation defined above has degree 5. \n\uf0b7 Cardinality: The number of tuples in a relation is known as \ncardinality. The STUDENT relation defined above has cardinality 4. \n \n \n\uf0b7 Column: Column represents the set of values for a particular \nattribute. The column ROLL_NO is extracted from relation \nSTUDENT. \nROLL_NO \n1 \n2 \n3 \n4 \n \nHow Queries can be Categorized in Relational Database? \nThe queries to deal with relational database can be categorized as: \n\uf0b7 Data Definition Language: It is used to define the structure of the \ndatabase. e.g., CREATE TABLE, ADD COLUMN, DROP COLUMN \nand so on. \n\uf0b7 Data Manipulation Language: It is used to manipulate data in the \nrelations. e.g., INSERT, DELETE, UPDATE and so on. \n\uf0b7 Data Query Language: It is used to extract the data from the \nrelations. e.g., SELECT So first we will consider the Data Query \nLanguage. A generic query to retrieve data from a relational database. \ni) \nSELECT [DISTINCT] Attribute_List FROM R1, R2\u2026.RM \nii) \n[WHERE condition] \niii) \n[GROUP BY (Attributes)[HAVING condition]] \niv) \n[ORDER BY(Attributes)[DESC]]; \n \nDifferent Query Combinations \nCase 1: If we want to retrieve attributes ROLL_NO and Name of all students, \nthe query will be: \nSELECT ROLL_NO, NAME FROM STUDENT; \n \nROLL_NO \nNAME \n1 \nRAM \n2 \nRAMESH \n3 \nSUJIT \n4 \nSURESH \n \nCase 2: If we want to retrieve ROLL_NO and NAME of the students \nwhose ROLL_NO is greater than 2, the query will be: \nSELECT ROLL_NO, NAME FROM STUDENT  \nWHERE ROLL_NO>2; \nROLL_NO \nNAME \n3 \nSUJIT \n4 \nSURESH \n \n \n \nCASE 3: If we want to retrieve all attributes of students, we can write * in \nplace of writing all attributes as: \nSELECT * FROM STUDENT  \nWHERE ROLL_NO>2; \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n3 \nSUJIT \nROHTAK \n9156253131 \n20 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n \nCASE 4: If we want to represent the relation in ascending order by AGE, we \ncan use ORDER BY clause as: \n0 seconds of 0 secondsVolume 0% \nSELECT * FROM STUDENT ORDER BY AGE; \n \nROLL_NO \nNAME \nADDRESS \nPHONE \nAGE \n1 \nRAM \nDELHI \n9455123451 \n18 \n2 \nRAMESH \nGURGAON \n9652431543 \n18 \n4 \nSURESH \nDELHI \n9156768971 \n18 \n3 \nSUJIT \nROHTAK \n9156253131 \n20 \n \nNote: \nORDER BY AGE is equivalent to ORDER BY AGE ASC.  \nIf we want to retrieve the results in descending order of AGE, we can use \nORDER BY AGE DESC.   \n \nCASE 5: If we want to retrieve distinct values of an attribute or group of \nattributes, DISTINCT is used as in: \nSELECT DISTINCT ADDRESS FROM STUDENT; \nADDRESS \nDELHI \nGURGAON \nROHTAK \n \nIf DISTINCT is not used, DELHI will be repeated twice in result set. Before \nunderstanding GROUP BY and HAVING, we need to understand aggregations \nfunctions in SQL. \n \nAggregation Functions \nAggregation functions are used to perform mathematical operations on data \nvalues of a relation. Some of the common aggregation functions used in SQL \nare: \n \n \n\uf0b7 COUNT: Count function is used to count the number of rows in a \nrelation. e.g., \nSELECT COUNT (PHONE) FROM STUDENT; \nCOUNT(PHONE) \n4 \n \n\uf0b7 SUM: SUM function is used to add the values of an attribute in a \nrelation. e.g., \nSELECT SUM(AGE) FROM STUDENT;  \nSUM (AGE) \n74 \nIn the same way, MIN, MAX and AVG can be used.  As we have seen above, \nall aggregation functions return only 1 row. AVERAGE: It gives the average \nvalues of the tuples. It is also defined as sum divided by count values. \nSyntax: \nAVG (attributename) \nOR \nSUM (attributename)/COUNT (attributename) \nThe above-mentioned syntax also retrieves the average value of tuples. \n \n\uf0b7 MAXIMUM: It extracts the maximum value among the set of tuples. \nSyntax: \nMAX (attributename) \n \n\uf0b7 MINIMUM: It extracts the minimum value amongst the set of all the \ntuples. \nSyntax: \nMIN (attributename) \n \n\uf0b7 GROUP BY: Group by is used to group the tuples of a relation based \non an attribute or group of attributes. It is always combined with \naggregation function which is computed on group. e.g., \nSELECT ADDRESS, SUM(AGE) FROM STUDENT \nGROUP BY (ADDRESS); \nIn this query, SUM(AGE) will be computed but not for entire table but for each \naddress. i.e., sum of AGE for address DELHI (18+18=36) and similarly for \nother address as well. The output is: \nADDRESS \nSUM(AGE) \nDELHI \n36 \nGURGAON \n18 \nROHTAK \n20 \n \n \n \nIf we try to execute the query given below, it will result in error because \nalthough we have computed SUM(AGE) for each address, there are more than \n1 ROLL_NO for each address we have grouped. So, it cannot be displayed in \nresult set. We need to use aggregate functions on columns after SELECT \nstatement to make sense of the resulting set whenever we are using GROUP BY. \n \nSELECT ROLL_NO, ADDRESS, SUM(AGE) FROM STUDENT \nGROUP BY (ADDRESS);  \n \nNOTE: An attribute which is not a part of GROUP BY clause can\u2019t be used \nfor selection.  \nAny attribute which is part of GROUP BY CLAUSE can be used for selection \nbut it is not mandatory.  \nBut we could use attributes which are not a part of the GROUP BY clause in \nan aggregate function.  \n \n \n10.1. SQL Commands \n \nLooking for one place where you can find all the SQL commands or SQL \nsublanguage commands like DDL, DQL, DML, DCL, and TCL, then \nbookmark this article. In this write-up, you will explore all the Structured \nQuery Language (SQL) commands with accurate syntax. \n \nStructured Query Language (SQL), as we all know, is the database language by \nthe use of which we can perform certain operations on the existing database, and \nalso, we can use this language to create a database. SQL uses certain commands \nlike CREATE, DROP, INSERT, etc. to carry out the required tasks.  \nSQL commands are like instructions to a table. It is used to interact with the \ndatabase with some operations. It is also used to perform specific tasks, \nfunctions, and queries of data. SQL can perform various tasks like creating a \ntable, adding data to tables, dropping the table, modifying the table, set \npermission for users. \n \nThese SQL commands are mainly categorized into five categories:  \n\uf0b7 DDL \u2013 Data Definition Language \n\uf0b7 DQL \u2013 Data Query Language \n\uf0b7 DML \u2013 Data Manipulation Language \n\uf0b7 DCL \u2013 Data Control Language \n\uf0b7 TCL \u2013 Transaction Control Language \n \n \ni) DDL (Data Definition Language) \nDDL or Data Definition Language consists of the SQL commands that can be \nused to define the database schema. It simply deals with descriptions of the \ndatabase schema and is used to create and modify the structure of database \nobjects in the database. DDL is a set of SQL commands used to create, modify, \nand delete database structures but not data. These commands are normally not \nused by a general user, who should be accessing the database via an application. \nList of DDL commands:  \n\uf0b7 CREATE: This command is used to create the database or its objects \n(like table, index, function, views, store procedure, and triggers). \n\uf0b7 DROP: This command is used to delete objects from the database. \n\uf0b7 ALTER: This is used to alter the structure of the database. \n\uf0b7 TRUNCATE: This is used to remove all records from a table, \nincluding all spaces allocated for the records are removed. \n\uf0b7 COMMENT: This is used to add comments to the data dictionary. \n\uf0b7 RENAME: This is used to rename an object existing in the database. \n \nii) \nDQL (Data Query Language) \nDQL statements are used for performing queries on the data within schema \nobjects. The purpose of the DQL Command is to get some schema relation based \non the query passed to it. We can define DQL as follows it is a component of \nSQL statement that allows getting data from the database and imposing order \nupon it. It includes the SELECT statement. This command allows getting the \ndata out of the database to perform operations with it. When a SELECT is fired \nagainst a table or tables the result is compiled into a further temporary table, \nwhich is displayed or perhaps received by the program i.e., front-end. \nList of DQL:  \n\uf0b7 SELECT: It is used to retrieve data from the database. \n \n \n \niii) \nDML (Data Manipulation Language) \nThe SQL commands that deal with the manipulation of data present in the \ndatabase belong to DML or Data Manipulation Language and this includes most \nof the SQL statements. It is the component of the SQL statement that controls \naccess to data and to the database. Basically, DCL statements are grouped with \nDML statements. \nList of DML commands:  \n\uf0b7 INSERT: It is used to insert data into a table. \n\uf0b7 UPDATE: It is used to update existing data within a table. \n\uf0b7 DELETE: It is used to delete records from a database table. \n\uf0b7 LOCK: Table control concurrency. \n\uf0b7 CALL: Call a PL/SQL or JAVA subprogram. \n\uf0b7 EXPLAIN PLAN: It describes the access path to data. \n \niv) \nDCL (Data Control Language) \nDCL includes commands such as GRANT and REVOKE which mainly deal \nwith the rights, permissions, and other controls of the database system.  \nList of  DCL commands:  \n\uf0b7 GRANT: This command gives users access privileges to the database. \nSyntax:GRANT SELECT, UPDATE ON MY_TABLE TO SOME_USER, ANOT\nHER_USER;   \n\uf0b7 REVOKE: This command withdraws the user\u2019s access privileges given \nby using the GRANT command. \nSyntax: \nREVOKE SELECT, UPDATE ON MY_TABLE FROM USER1, USER2;   \nv) \nTCL (Transaction Control Language) \nTransactions group a set of tasks into a single execution unit. Each transaction \nbegins with a specific task and ends when all the tasks in the group are \nsuccessfully completed. If any of the tasks fail, the transaction fails. Therefore, \na transaction has only two results: success or failure. Hence, the following TCL \ncommands are used to control the execution of a transaction:  \nBEGIN: Opens a Transaction. \nCOMMIT: Commits a Transaction. \nSyntax: \nCOMMIT;   \nROLLBACK: Rollbacks a transaction in case of any error occurs. \nSyntax: \nROLLBACK;   \nSAVEPOINT: Sets a save point within a transaction. \nSyntax:           SAVEPOINT SAVEPOINT_NAME; \n \n \n10.2. SQL | Join \nSQL Join statement is used to combine data or rows from two or more tables \nbased on a common field between them. Different types of Joins are as \nfollows:  \n\uf0b7 INNER JOIN \n\uf0b7 LEFT JOIN \n\uf0b7 RIGHT JOIN \n\uf0b7 FULL JOIN \n\uf0b7 NATURAL JOIN  \nConsider the two tables below as follows:  \nStudent \n StudentCourse  \nCOURSE_ID \nROLL_NO \n1 \n1 \n2 \n2 \n2 \n3 \n3 \n4 \n1 \n5 \n4 \n9 \n5 \n10 \n4 \n11 \n \nThe simplest Join is INNER JOIN.  \ni) \nINNER JOIN \nThe INNER JOIN keyword selects all rows from both the tables if the condition \nis satisfied. This keyword will create the result-set by combining all rows from \nboth the tables where the condition satisfies i.e., value of the common field will \nbe the same.  \nSyntax:  \nSELECT table1.column1, table1.column2, table2.column1,.... \nFROM table1  \nINNER JOIN table2 \nON table1.matching_column = table2.matching_column; \n \n \n \n \ntable1: First table. \ntable2: Second table \nmatching_column: Column common to both the tables. \n \nNote: We can also write JOIN instead of INNER JOIN. JOIN is same as \nINNER JOIN.  \n                           Figure 26: Venn Diagram Inner Join \nExample Queries (INNER JOIN) \nThis query will show the names and age of students enrolled in different \ncourses.   \nSELECT StudentCourse.COURSE_ID, Student.NAME, Student.AGE FROM \nStudent \nINNER JOIN StudentCourse \nON Student.ROLL_NO = StudentCourse.ROLL_NO; \nOutput:   \nCOURSE_ID \nNAME \nAge \n1 \nHarsh \n18 \n2 \nPratik \n19 \n2 \nRiyanka \n20 \n3 \nDeep \n18 \n1 \nSaptarhi \n19 \n \n \nii) \nLEFT JOIN \nThis join returns all the rows of the table on the left side of the join and \nmatches rows for the table on the right side of the join. For the rows for which \nthere is no matching row on the right side, the result-set will contain null. \nLEFT JOIN is also known as LEFT OUTER JOIN. \n \n \nSyntax:  \nSELECT table1.column1,table1.column2,table2.column1,.... \nFROM table1  \nLEFT JOIN table2 \nON table1.matching_column = table2.matching_column; \n \ntable1: First table. \ntable2: Second table \nmatching_column: Column common to both the tables. \n \nNote: We can also use LEFT OUTER JOIN instead of LEFT JOIN, both are \nthe same. \n                                  Figure 27: Venn Diagram (Left Join) \nExample Queries (LEFT JOIN):  \nSELECT Student.NAME,StudentCourse.COURSE_ID  \nFROM Student \nLEFT JOIN StudentCourse  \nON StudentCourse.ROLL_NO = Student.ROLL_NO; \nOutput:   \nNAME \nCOURSE_ID \nHarsh \n1 \nPratik \n2 \nRiyanka \n2 \nDeep \n3 \nSaptarhi \n1 \nDhanraj \nNULL \nRohit \nNULL \nNiraj \nNULL \niii) \nRIGHT JOIN \nRIGHT JOIN is like LEFT JOIN. This join returns all the rows of the table on \nthe right side of the join and matching rows for the table on the left side of the \njoin. For the rows for which there is no matching row on the left side, the \nresult-set will contain null. RIGHT JOIN is also known as RIGHT OUTER \nJOIN.  \n \n \nSyntax:  \nSELECT table1.column1,table1.column2,table2.column1,.... \nFROM table1  \nRIGHT JOIN table2 \nON table1.matching_column = table2.matching_column; \ntable1: First table. \ntable2: Second table \nmatching_column: Column common to both the tables. \n \nNote: We can also use RIGHT OUTER JOIN instead of RIGHT JOIN, both are \nthe same.  \n                            Figure 28: Venn Diagram (Right Join)  \nExample Queries (RIGHT JOIN): \nSELECT Student.NAME,StudentCourse.COURSE_ID  \nFROM Student \nRIGHT JOIN StudentCourse  \nON StudentCourse.ROLL_NO = Student.ROLL_NO; \nOutput:  \nNAME \nCOURSE_ID \nHarsh \n1 \nPratik \n2 \nRiyanka \n2 \nDeep \n3 \nSaptarhi \n1 \nNULL \n4 \nNULL \n5 \nNULL \n4 \n \niv) \nFULL JOIN \nFULL JOIN creates the result-set by combining results of both LEFT JOIN \nand RIGHT JOIN. The result-set will contain all the rows from both tables. \nFor the rows for which there is no matching, the result-set will \ncontain NULL values. \n \n \n                              Figure 29: Venn Diagram (Full Join) \nSyntax:   \nSELECT table1.column1,table1.column2,table2.column1,.... \nFROM table1  \nFULL JOIN table2 \nON table1.matching_column = table2.matching_column; \ntable1: First table. \ntable2: Second table \nmatching_column: Column common to both the tables.  \n \nExample Queries (FULL JOIN):  \nSELECT Student.NAME,StudentCourse.COURSE_ID  \nFROM Student \nFULL JOIN StudentCourse  \nON StudentCourse.ROLL_NO = Student.ROLL_NO; \nOutput:   \nNAME \nCOURSE_ID \nHARSH \n1 \nPRATIK \n2 \nRIYANKA \n2 \nDEEP \n3 \nSAPTARHI \n1 \nDHANRAJ \nNULL \nROHIT \nNULL \nNIRAJ \nNULL \nNULL \n4 \nNULL \n5 \nNULL \n4 \n \n \n \n \n \n \n10.3. Clause in SQL \n \nHaving vs. Where Clause  \nThe difference between the having and where clause in SQL is that where clause \ncannot be used with aggregates, but the having clause can. \nThe where clause works on row\u2019s data, not on aggregated data.  Let us consider \nbelow table \u2018Marks\u2019. \nStudent \nCourse \nScore \na \nc1 \n40 \na \nc2 \n50 \nb \nc3 \n60 \nd \nc1 \n70 \ne \nc2 \n80 \n \nConsider the query. \nSELECT Student, Score FROM Marks WHERE Score >= 40 \nThis would select data row by row basis. \nThe having clause works on aggregated data. \nFor example, output of below query \nSELECT Student, SUM (SCORE AS total FROM Marks GROUP BY Student \nStudent \nTotal \na \n90 \nb \n60 \nc \n70 \nd \n80 \n \nWhen we apply having in above query, we get \nSELECT Student, SUM (score) AS total FROM Marks Group BY Student HAVING \nStudent \nTotal \na \n90 \nc \n80 \n  \nNote:  It is not a predefined rule but  in a good number of the SQL queries, we \nuse WHERE prior to GROUP \n \nBY and HAVING after GROUP BY. The Where clause acts as a pre \nfilter where as Having as a post filter. \n \n \n \n10.4. Database Objects \n \nA database object is any defined object in a database that is used to store or \nreference data. Anything which we make from create command is known as \nDatabase Object. It can be used to hold and manipulate the data. Some of the \nexamples of database objects are : view, sequence, indexes, etc. \n\uf0b7 Table \u2013 Basic unit of storage; composed rows and columns. \n\uf0b7 View \u2013 Logically represents subsets of data from one or more tables. \n\uf0b7 Sequence \u2013 Generates primary key values. \n\uf0b7 Index \u2013 Improves the performance of some queries. \n\uf0b7 Synonym \u2013 Alternative name for an object \nDifferent database Objects: \ni) Table \u2013 This database object is used to create a table in database. \nSyntax:  \nCREATE TABLE [schema.] table (column datatype [DEFAULT expr][, ...]); \nExample: \nCREATE TABLE dept \n           (deptno NUMBER (2), \n            dname VARCHAR2(14), \n            loc VARCHAR2(13)) ; \nOutput: \nDESCRIBE dept; \nName \nNull? \nType \nDEPT_NO \n \nNUMBER(2) \nDNAME \n \nVARCHAR(14) \nLOC \n \nVARCHAR(13) \n \nii) View \u2013 This database object is used to create a view in database. A view \nis a logical table based on a table or another view. A view contains no \ndata of its own but is like a window through which data from tables can \nbe viewed or changed. The tables on which a view is based are called \nbase tables. The view is stored as a SELECT statement in the data \ndictionary. \nSyntax: \nCREATE [OR REPLACE] [FORCE|NOFORCE] VIEW view \n                       [(alias[, alias]...)] \n                       AS subquery \n                       [WITH CHECK OPTION [CONSTRAINT \nconstraint]] \n                       [WITH READ ONLY [CONSTRAINT constraint]]; \n \n \nExample: \nCREATE VIEW salvu50 \n               AS SELECT employee_id ID_NUMBER, last_name \nNAME, \n               salary*12 ANN_SALARY \n               FROM employees \n               WHERE department_id = 50; \nOutput: \nSELECT * \nFROM salvu50; \nID_NUMBER \nNAME \nANN_SALARY \n124 \nMourgous \n69600 \n141 \nRajs \n42000 \n142 \nDavies \n372000 \n143 \nMatos \n312000 \n144 \nVargas \n300000 \n \niii) Sequence \u2013  \nThis database object is used to create a sequence in database. A sequence is a \nuser created database object that can be shared by multiple users to generate \nunique integers. A typical usage for sequences is to create a primary key value, \nwhich must be unique for each row. The sequence is generated and \nincremented (or decremented) by an internal Oracle routine. \nSyntax: \nCREATE SEQUENCE sequence \n                    [INCREMENT BY n] \n                    [START WITH n] \n                    [{MAXVALUE n | NOMAXVALUE}] \n                    [{MINVALUE n | NOMINVALUE}] \n                    [{CYCLE | NOCYCLE}] \n                    [{CACHE n | NOCACHE}]; \nExample: \nCREATE SEQUENCE dept_deptid_seq \n                        INCREMENT BY 10 \n                        START WITH 120 \n                        MAXVALUE 9999 \n                        NOCACHE \n                        NOCYCLE; \nCheck if sequence is created by: \nSELECT sequence_name, min_value, max_value, \n                       increment_by, last_number \n                       FROM   user_sequences; \n \n \niv) Index \u2013  \nThis database object is used to create an index in database. An Oracle server \nindex is a schema object that can speed up the retrieval of rows by using a \npointer. Indexes can be created explicitly or automatically. If you do not have \nan index on the column, then a full table scan occurs. \nAn index provides direct and fast access to rows in a table. Its purpose is to \nreduce the necessity of disk I/O by using an indexed path to locate data \nquickly. The index is used and maintained automatically by the Oracle server. \nOnce an index is created, no direct activity is required by the user. Indexes are \nlogically and physically independent of the table they index. This means that \nthey can be created or dropped at any time and have no effect on the base \ntables or other indexes. \nSyntax: \nCREATE INDEX index \n            ON table (column [, column] ...); \nExample: \nCREATE INDEX emp_last_name_idx \n                ON  employees(last_name); \n \nv) Synonym \u2013  \nThis database object is used to create a index in database. It simplify access to \nobjects by creating a synonym (another name for an object). With synonyms, \nyou can Ease referring to a table owned by another user and shorten lengthy \nobject names. To refer to a table owned by another user, you need to prefix the \ntable name with the name of the user who created it followed by a period. \nCreating a synonym eliminates the need to qualify the object name with the \nschema and provides you with an alternative name for a table, view, sequence, \nprocedure, or other objects. This method can be especially useful with lengthy \nobject names, such as views. \nIn the syntax: \nPUBLIC: creates a synonym accessible to all users \nsynonym : is the name of the synonym to be created \nobject : identifies the object for which the synonym is created \nSyntax: \nCREATE [PUBLIC] SYNONYM synonym FOR object; \nExample: \nCREATE SYNONYM d_sum FOR dept_sum_vu; \n \n \n \n \n \n10.5. Indexing  \n \nIndexing improves database performance by minimizing the number of disc \nvisits required to fulfil a query. It is a data structure technique used to locate and \nquickly access data in databases. Several database fields are used to generate \nindexes. The main key or candidate key of the table is duplicated in the first \ncolumn, which is the Search key. To speed up data retrieval, the values are also \nkept in sorted order. It should be highlighted that sorting the data is not required. \nThe second column is the Data Reference or Pointer which contains a set of \npointers holding the address of the disk block where that particular key value \ncan be found. \nAttributes of Indexing \n\uf0b7 Access Types: This refers to the type of access such as value-based \nsearch, range access, etc. \n\uf0b7 Access Time: It refers to the time needed to find a particular data \nelement or set of elements. \n\uf0b7 Insertion Time: It refers to the time taken to find the appropriate \nspace and insert new data. \n\uf0b7 Deletion Time: Time taken to find an item and delete it as well as \nupdate the index structure. \n\uf0b7 Space Overhead: It refers to the additional space required by the \nindex. \nIn general, there are two types of file organization mechanisms that are followed \nby the indexing methods to store the data:   \nSequential File Organization or Ordered Index File \nIn this, the indices are based on a sorted ordering of the values. These are \ngenerally fast and a more traditional type of storing mechanism. These Ordered \nor Sequential file organizations might store the data in a dense or sparse format. \n\uf0b7 Dense Index \nFor every search key value in the data file, there is an index record. \nThis record contains the search key and a reference to the first data record with \nthat search key value. \n\uf0b7 Sparse Index  \n\uf0b7 The index record appears only for a few items in the data \nfile. Each item points to a block as shown. \n\uf0b7 To locate a record, we find the index record with the largest \nsearch key value less than or equal to the search key value \nwe are looking for. \n\uf0b7 We start at that record pointed to by the index record and \nproceed along with the pointers in the file (that is, \nsequentially) until we find the desired record. \n\uf0b7 Number of Accesses required=log\u2082(n)+1, (here n=number \nof blocks acquired by index file) \n \n \nHash File Organization \nIndices are based on the values being distributed uniformly across a range of \nbuckets. The buckets to which a value is assigned are determined by a function \ncalled a hash function. There are primarily three methods of indexing:   \n\uf0b7 Clustered Indexing: When more than two records are stored in the \nsame file this type of storing is known as cluster indexing. By using \ncluster indexing we can reduce the cost of searching reason being \nmultiple records related to the same thing are stored in one place and \nit also gives the frequent joining of more than two tables (records).  \nThe clustering index is defined on an ordered data file. The data file \nis ordered on a non-key field. In some cases, the index is created on \nnon-primary key columns which may not be unique for each record. \nIn such cases, in order to identify the records faster, we will group \ntwo or more columns together to get the unique values and create an \nindex out of them. This method is known as the clustering index. \nEssentially, records with similar properties are grouped together, and \nindexes for these groupings are formed.  \nStudents studying each semester. \n\uf0b7 Primary Indexing: This is a type of Clustered Indexing wherein the \ndata is sorted according to the search key and the primary key of the \ndatabase table is used to create the index. It is a default format of \nindexing where it induces sequential file organization. As primary \nkeys are unique and are stored in a sorted manner, the performance of \nthe searching operation is quite efficient.  \n\uf0b7 Non-clustered or Secondary Indexing: A non-clustered index just \ntells us where the data lies, i.e., it gives us a list of virtual pointers or \nreferences to the location where the data is stored. Data is not \nphysically stored in the order of the index. Instead, data is present in \nleaf nodes. For e.g., the contents page of a book. Each entry gives us \nthe page number or location of the information stored. The actual \ndata here (information on each page of the book) is not organized but \nwe have an ordered reference (contents page) to where the data \npoints lie. We can have only dense ordering in the non-clustered \nindex as sparse ordering is not possible because data is not physically \norganized accordingly.  \nIt requires more time as compared to the clustered index because \nsome amount of extra work is done in order to extract the data by \nfurther following the pointer. In the case of a clustered index, data is \ndirectly present in front of the index. \n \n \n                                  Figure 30: Non-Cluster Indexing \n\uf0b7 Multilevel Indexing: With the growth of the size of the database, \nindices also grow. As the index is stored in the main memory, a \nsingle-level index might become too large a size to store with \nmultiple disk accesses. The multilevel indexing segregates the main \nblock into various smaller blocks so that the same can be stored in a \nsingle block. The outer blocks are divided into inner blocks which in \nturn are pointed to the data blocks. This can be easily stored in the \nmain memory with fewer overheads.   \n \n10.6. SQL Views \n \nViews in SQL are kind of virtual tables. A view also has rows and columns as \nthey are in a real table in the database. We can create a view by selecting fields \nfrom one or more tables present in the database. A View can either have all the \nrows of a table or specific rows based on certain condition. In this article we \nwill learn about creating, deleting and updating Views.  \nSample Tables: \nTable 1. StudentDetails \nS_ID \nNAME \nADDRESS \n1 \nHarsh \nKolkata \n2 \nAshish \nDurgapur \n3 \nPratik \nDelhi \n4 \nDhanraj \nBihar \n5 \nRam \nRajasthan \n \n \nTable 2. StudentMarks \nID \nNAME \nMARKS \nAGE \n1 \nHarsh \n90 \n19 \n2 \nSuresh \n50 \n20 \n3 \nPratik \n80 \n19 \n4 \nDhanraj \n95 \n21 \n5 \nRam \n85 \n18 \n \nCREATING VIEWS \nWe can create View using CREATE VIEW statement. A View can be created \nfrom a single table or multiple tables. Syntax: \nCREATE VIEW view_name AS \nSELECT column1, column2..... \nFROM table_name \nWHERE condition; \nview_name: Name for the View \ntable_name: Name of the table \ncondition: Condition to select rows \n \nExamples: \nCreating View from a single table: \n\uf0b7 In this example we will create a View named DetailsView from the \ntable StudentDetails. Query: \nCREATE VIEW DetailsView AS \nSELECT NAME, ADDRESS \nFROM StudentDetails \nWHERE S_ID < 5; \n\uf0b7 To see the data in the View, we can query the view in the same \nmanner as we query a table. \nSELECT * FROM DetailsView; \nOutput:   \nNAME \nADDRESS \nHarsh \nKolkata \nAshish \nDurgapur \nPratik \nDelhi \nDhanraj \nBihar \n \n \n \nIn this example, we will create a view named StudentNames from the table \nStudentDetails. Query: \nCREATE VIEW StudentNames AS \nSELECT S_ID, NAME \nFROM StudentDetails \nORDER BY NAME; \nIf we now query the view as, \nSELECT * FROM StudentNames; \nOutput:  \nS_ID \nNAMES \n2 \nAshish \n4 \nDhanraj \n1 \nHarsh \n3 \nPratik \n5 \nRam \n \nCreating View from multiple tables: In this example we will create a View \nnamed MarksView from two tables StudentDetails and StudentMarks. To \ncreate a View from multiple tables we can simply include multiple tables in \nthe SELECT statement. Query: \nCREATE VIEW MarksView AS \nSELECT StudentDetails.NAME, StudentDetails.ADDRESS, \nStudentMarks.MARKS \nFROM StudentDetails, StudentMarks \nWHERE StudentDetails.NAME = StudentMarks.NAME; \n \nTo display data of View MarksView: \nSELECT * FROM MarksView; \nOutput:  \nNAME \nADDRESS \nMARKS \nHarsh \nKolkata \n90 \nPratik \nDelhi \n80 \nDhanraj \nBihar \n95 \nRam \nRajasthan \n85 \n \nLISTING ALL VIEWS IN A DATABASE \nWe can list View using the SHOW FULL TABLES statement or using the \ninformation_schema table. A View can be created from a single table or \nmultiple tables.  \nSyntax (Using SHOW FULL TABLES): \nuse \"database_name\"; \nshow full tables where table_type like \"%VIEW\"; \n \n \nSyntax (Using information_schema) : \nselect * from information_schema.views where table_schema = \n\"database_name\"; \n \nOR \n \nselect table_schema,table_name,view_definition from \ninformation_schema.views where table_schema = \"database_name\"; \n  \nDELETING VIEWS \nWe have learned about creating a View, but what if a created View is not \nneeded any more? Obviously we will want to delete it. SQL allows us to delete \nan existing View. We can delete or drop a View using the DROP \nstatement. Syntax: \nDROP VIEW view_name; \n \nview_name: Name of the View which we want to delete. \nFor example, if we want to delete the View MarksView, we can do this as: \nDROP VIEW MarksView; \n \nUPDATING VIEWS \nThere are certain conditions needed to be satisfied to update a view. If any one \nof these conditions is not met, then we will not be allowed to update the view. \ni) The SELECT statement which is used to create the view should not \ninclude GROUP BY clause or ORDER BY clause. \nii) The SELECT statement should not have the DISTINCT keyword. \niii) The View should have all NOT NULL values. \niv) The view should not be created using nested queries or complex \nqueries. \nv) The view should be created from a single table. If the view is created \nusing multiple tables then we will not be allowed to update the view. \nWe can use the CREATE OR REPLACE VIEW statement to add or remove \nfields from a view. Syntax: \nCREATE OR REPLACE VIEW view_name AS \nSELECT column1,column2,.. \nFROM table_name \nWHERE condition; \n \n \n \n \n \n \nInserting a row in a view: We can insert a row in a View in a same way as we \ndo in a table. We can use the INSERT INTO statement of SQL to insert a row \nin a View.  \nSyntax: \nINSERT INTO view_name(column1, column2 , column3,..)  \nVALUES(value1, value2, value3..); \nview_name: Name of the View \nExample: In the below example we will insert a new row in the View \nDetailsView which we have created above in the example of \u201ccreating views \nfrom a single table\u201d. \nINSERT INTO DetailsView(NAME, ADDRESS) \nVALUES(\"Suresh\",\"Gurgaon\"); \n\uf0b7 If we fetch all the data from DetailsView now as, \nSELECT * FROM DetailsView; \nOutput:  \nNAME \nADDRESS \nHarsh \nKolkata \nAshish \nDurgapur \nPratik \nDelhi \nDhanraj \nBihar \nSuresh \nGurgon \n \nDeleting a row from a View: Deleting rows from a view is also as simple as \ndeleting rows from a table. We can use the DELETE statement of SQL to \ndelete rows from a view. Also deleting a row from a view first delete the row \nfrom the actual table and the change is then reflected in the view.  \nSyntax: \nDELETE FROM view_name \nWHERE condition; \nview_name:Name of view from where we want to delete rows \ncondition: Condition to select rows  \nExample: In this example, we will delete the last row from the view \nDetailsView which we just added in the above example of inserting rows. \nDELETE FROM DetailsView \nWHERE NAME=\"Suresh\"; \nIf we fetch all the data from DetailsView now as, \nSELECT * FROM DetailsView; \nOutput:  \nNAME \nADDRESS \nHarsh \nKolkata \nAshish \nDurgapur \nPratik \nDelhi \nDhanraj \nBihar \n \n \nWITH CHECK OPTION \nThe WITH CHECK OPTION clause in SQL is a very useful clause for views. \nIt is applicable to an updatable view. If the view is not updatable, then there is \nno meaning of including this clause in the CREATE VIEW statement. \n\uf0b7 The WITH CHECK OPTION clause is used to prevent the insertion \nof rows in the view where the condition in the WHERE clause in \nCREATE VIEW statement is not satisfied. \n\uf0b7 If we have used the WITH CHECK OPTION clause in the CREATE \nVIEW statement, and if the UPDATE or INSERT clause does not \nsatisfy the conditions then they will return an error. \nExample: In the below example we are creating a View SampleView from \nStudentDetails Table with WITH CHECK OPTION clause. \nCREATE VIEW SampleView AS \nSELECT S_ID, NAME \nFROM  StudentDetails \nWHERE NAME IS NOT NULL \nWITH CHECK OPTION; \nIn this View if we now try to insert a new row with null value in the NAME \ncolumn then it will give an error because the view is created with the condition \nfor NAME column as NOT NULL. For example,though the View is updatable \nbut then also the below query for this View is not valid: \nINSERT INTO SampleView(S_ID) \nVALUES(6); \nNOTE: The default value of NAME column is null.  \nUses of a View: A good database should contain views due to the given \nreasons: \ni) Restricting data access \u2013 Views provide an additional level of table \nsecurity by restricting access to a predetermined set of rows and \ncolumns of a table. \nii) Hiding data complexity \u2013 A view can hide the complexity that exists in \nmultiple tables join. \niii) Simplify commands for the user \u2013 Views allow the user to select \ninformation from multiple tables without requiring the users to actually \nknow how to perform a join. \niv) Store complex queries \u2013 Views can be used to store complex queries. \nv) Rename Columns \u2013 Views can also be used to rename the columns \nwithout affecting the base tables provided the number of columns in \nview must match the number of columns specified in select statement. \nThus, renaming helps to hide the names of the columns of the base \ntables. \nvi) Multiple view facility \u2013 Different views can be created on the same \ntable for different users. \n \n \n10.7. SQL Indexes \nAn index is a schema object. It is used by the server to speed up the retrieval of \nrows by using a pointer. It can reduce disk I/O(input/output) by using a rapid \npath access method to locate data quickly. \nAn index helps to speed up select queries and where clauses, but it slows down \ndata input, with the update and the insert statements. Indexes can be created or \ndropped with no effect on the data. In this article, we will see how \nto create, delete, and use the INDEX in the database. \n   \nCreating an Index \nSyntax \nCREATE INDEX index \nON TABLE column; \nwhere the index is the name given to that index TABLE is the name of the table \non which that index is created, and column is the name of that column for which \nit is applied. \n  \nFor Multiple Columns \nSyntax: \nCREATE INDEX index \nON TABLE (column1, column2,\u2026..); \n \nFor Unique Indexes \nUnique indexes are used for the maintenance of the integrity of the data present \nin the table as well as for fast performance, it does not allow multiple values to \nenter into the table. \nSyntax: \nCREATE UNIQUE INDEX index \nON TABLE column; \n \nWhen Should Indexes be Created? \n\uf0b7 A column contains a wide range of values. \n\uf0b7 A column does not contain a large number of null values. \n\uf0b7 One or more columns are frequently used together in a where clause \nor a join condition. \n \nWhen Should Indexes be Avoided? \n\uf0b7 The table is small \n\uf0b7 The columns are not often used as a condition in the query \n\uf0b7 The column is updated frequently \n \n \n \n \nRemoving an Index \nRemove \nan \nindex \nfrom \nthe \ndata \ndictionary \nby \nusing \nthe DROP INDEX command.  \nSyntax \nDROP INDEX index; \nTo drop an index, you must be the owner of the index or have the DROP ANY \nINDEX privilege.  \n \nAltering an Index \nTo modify an existing table\u2019s index by rebuilding, or reorganizing the index. \nALTER INDEX IndexName \nON TableName REBUILD; \n \nConfirming Indexes \nYou can check the different indexes present in a particular table given by the \nuser or the server itself and their uniqueness.  \nSyntax: \nSELECT * from USER_INDEXES; \nIt will show you all the indexes present in the server, in which you can locate \nyour own tables too. \n \nRenaming an Index \nYou can use the system-stored procedure sp_rename to rename any index in the \ndatabase. \nSyntax: \nEXEC sp_rename \nindex_name, \nnew_index_name, \nN\u2019INDEX\u2019; \n \nSQL Server Database \nSyntax: \nDROP INDEX TableName.IndexName;   \nWhy SQL Indexing is Important? \nIndexing is an important topic when considering advanced MySQL, although \nmost people know about its definition and usage they don\u2019t understand when \nand where to use it to change the efficiency of our queries or stored procedures \nby a huge margin. \nHere are some scenarios along with their explanation related to Indexing: \n\uf0b7 When executing a query on a table having huge data  \n( > 100000 rows ), MySQL performs a full table scan which takes \nmuch time and the server usually gets timed out. To avoid this always \n \n \ncheck the explain option for the query within MySQL which tells us \nabout the state of execution. It shows which columns are being used \nand whether it will be a threat to huge data. On basis of the columns \nrepeated in a similar order in condition. \n\uf0b7 The order of the index is of huge importance as we can use the \nsaitions, we can create an index for them in the same order to \nmaximize the speed of the query. me index in many scenarios. Using \nonly one index we can utilize it in more than one query which \ndifferent conditions. like for example, in a query, we make a join \nwith a table based on customer_id wards we also join another join \nbased on customer_id and order_date. Then we can simply create a \nsingle index by the order of customer_id, order_date which would be \nused in both cases. This also saves storage. \n\uf0b7 We should also be careful to not make an index for each query as \ncreating indexes also take storage and when the amount of data is \nhuge it will create a problem. Therefore, it\u2019s important to carefully \nconsider which columns to index based on the needs of your \napplication. In general, it\u2019s a good practice to only create indexes on \ncolumns that are frequently used in queries and to avoid creating \nindexes on columns that are rarely used. It\u2019s also a good idea to \nperiodically review the indexes in your database and remove any that \nare no longer needed. \n\uf0b7 Indexes can also improve performance when used in conjunction \nwith sorting and grouping operations. For example, if you frequently \nsort or group data based on a particular column, creating an index on \nthat column can greatly improve performance. The index allows \nMySQL to quickly access and sort or group the data, rather than \nhaving to perform a full table scan. \n\uf0b7 In some cases, MySQL may not use an index even if one exists. This \ncan happen if the query optimizer determines that a full table scan is \nfaster than using the index. \n \nSQL Queries on Cluster and Non-Cluster Indexes \nIndexing is a procedure that returns your requested data faster from the defined \ntable. Without indexing, the SQL server has to scan the whole table for your \ndata. By indexing, the SQL server will do the exact same thing you do when \nsearching for content in a book by checking the index page. In the same way, a \ntable\u2019s index allows us to locate the exact data without scanning the whole table. \nThere are two types of indexing in SQL. \n\uf0b7 Clustered index \n\uf0b7 Non-clustered index \n \n \n \n \nClustered Index \nA clustered index is the type of indexing that establishes a physical sorting order \nof rows. \nSuppose you have a table Student_info which contains ROLL_NO as a primary \nkey, then the clustered index which is self-created on that primary key will sort \nthe Student_info table as per ROLL_NO. A clustered index is like a Dictionary; \nin the dictionary, the sorting order is alphabetical and there is no separate index \npage.  \nExamples: \nCREATE TABLE Student_info \n( \nROLL_NO int(10) primary key, \nNAME varchar(20), \nDEPARTMENT varchar(20), \n); \nINSERT INTO Student_info values(1410110405, 'H Agarwal', 'CSE'); \nINSERT INTO Student_info values(1410110404, 'S Samadder', 'CSE'); \nINSERT INTO Student_info values(1410110403, 'MD Irfan', 'CSE');  \n \nSELECT * FROM Student_info; \nOutput: \nROLL_NO \nNAME \nDEPARTMENT \n1410110403 \nMD Irfan \nCSE \n1410110404 \nS Samadder \nCSE \n1410110405 \nH Agarwal \nCSE \n \nIf we want to create a Clustered index on another column, first we have to \nremove the primary key, and then we can remove the previous index. Note that \ndefining a column as a primary key makes that column the Clustered Index of \nthat table. To make any other column, the clustered index, first we have to \nremove the previous one as follows below.  \n \nSyntax: \n//Drop index \ndrop index table_name.index_name \n//Create Clustered index index \ncreate Clustered index IX_table_name_column_name \non table_name (column_name ASC) \n \nNote: We can create only one clustered index in a table. \n \n \n \n \n \nNon-Clustered Index \nNon-Clustered index is an index structure separate from the data stored in a table \nthat reorders one or more selected columns. The non-clustered index is created \nto improve the performance of frequently used queries not covered by a \nclustered index. It\u2019s like a textbook; the index page is created separately at the \nbeginning of that book.  \nExamples: \nCREATE TABLE Student_info \n( \nROLL_NO int(10), \nNAME varchar(20), \nDEPARTMENT varchar(20), \n); \nINSERT INTO Student_info values(1410110405, 'H Agarwal', 'CSE'); \nINSERT INTO Student_info values(1410110404, 'S Samadder', 'CSE'); \nINSERT INTO Student_info values(1410110403, 'MD Irfan', 'CSE'); \n \nSELECT * FROM Student_info; \nOutput: \nROLL_NO \nNAME \nDEPARTMENT \n1410110405 \nH Agarwal \nCSE \n1410110404 \nS Samadder \nCSE \n1410110403 \nMD Irfan \nCSE \nNote: We can create one or more Non_Clustered index in a table. \nSyntax: \n//Create Non-Clustered index \ncreate NonClustered index IX_table_name_column_name \non table_name (column_name ASC) \nTable: Student_info \nROLL_NO \nNAME \nDEPARTMENT \n1410110405 \nH Agarwal \nCSE \n1410110404 \nS Samadder \nCSE \n1410110403 \nMD Irfan \nCSE \nInput: create NonClustered index IX_Student_info_NAME on Student_info \n(NAME ASC) \nOutput: Index \nNAME \nROW_ADDRESS \nH Agarwal \n1 \nMD Irfan \n3 \nS Samadder \n2 \n \n \n \n \nClustered vs Non-Clustered Index \n\uf0b7 In a table, there can be only one clustered index or one or more than \none non_clustered index. \n\uf0b7 In Clustered index, there is no separate index storage but in Non-\nClustered index, there is separate index storage for the index. \n\uf0b7 Clustered index offers faster data access, on the other hand, the Non-\nclustered index is slower. \nClustered and non-clustered indexes in SQL Server can provide significant \nperformance benefits when querying large tables. Here are some examples of \nSQL queries and the advantages of using clustered and non-clustered indexes: \n \nSELECT Queries with WHERE Clause \n\uf0b7 Clustered Index: When a SELECT query with a WHERE clause is \nexecuted on a table with a clustered index, SQL Server can use the \nclustered index to quickly locate the rows that match the WHERE \ncondition. This can be very efficient for large tables, as it allows the \ndatabase engine to minimize the number of disk reads required to \nretrieve the desired data. \n\uf0b7 Non-Clustered Index: If there is no clustered index on the table or \nthe WHERE clause references columns that are not part of the \nclustered index, SQL Server can use a non-clustered index to find the \nmatching rows. However, this may require additional disk reads if \nthe non-clustered index does not include all the columns required by \nthe query. \n \nUPDATE Queries \n\uf0b7 Clustered Index: When an UPDATE query is executed on a table \nwith a clustered index, SQL Server can use the index to quickly \nlocate and modify the rows that match the query criteria. This can be \nvery efficient for large tables, as it allows the database engine to \nminimize the number of disk writes required to modify the data. \n\uf0b7 Non-Clustered Index: If the UPDATE query references columns \nthat are not part of the clustered index, SQL Server may need to \nperform additional disk writes to update the non-clustered index as \nwell. \n \nJOIN Queries \n\uf0b7 Clustered Index: When performing a JOIN operation between two \nlarge tables, SQL Server can use the clustered index on the join \ncolumn(s) to efficiently match the rows from both tables. This can \nsignificantly reduce the time required to complete the query. \n \n \n \n\uf0b7 Non-Clustered Index: If the JOIN operation references columns that \nare not part of the clustered index, SQL Server can use a non-\nclustered index to find the matching rows. However, this may require \nadditional disk reads and slow down the query. \n\uf0b7 In general, the advantage of using clustered indexes is that they can \nprovide very efficient access to large tables, particularly when \nquerying on the index columns. The advantage of using non-\nclustered indexes is that they can provide efficient access to columns \nthat are not part of the clustered index, or when querying multiple \ntables with JOIN operations. However, non-clustered indexes can \nalso require additional disk reads or writes, which can slow down \nqueries. It is important to carefully design and tune indexes based on \nthe specific query patterns and data access patterns of your \napplication. \n \n \nAdvantages of Indexing \n\uf0b7 Improved Query Performance: Indexing enables faster data \nretrieval from the database. The database may rapidly discover rows \nthat match a specific value or collection of values by generating an \nindex on a column, minimizing the amount of time it takes to \nperform a query. \n\uf0b7 Efficient Data Access: Indexing can enhance data access efficiency \nby lowering the amount of disk I/O required to retrieve data. The \ndatabase can maintain the data pages for frequently visited columns \nin memory by generating an index on those columns, decreasing the \nrequirement to read from disk. \n\uf0b7 Optimized Data Sorting: Indexing can also improve the \nperformance of sorting operations. By creating an index on the \ncolumns used for sorting, the database can avoid sorting the entire \ntable and instead sort only the relevant rows. \n\uf0b7 Consistent Data Performance: Indexing can assist ensure that the \ndatabase performs consistently even as the amount of data in the \ndatabase rises. Without indexing, queries may take longer to run as \nthe number of rows in the table grows, while indexing maintains a \nroughly consistent speed. \n\uf0b7 By ensuring that only unique values are inserted into columns that \nhave been indexed as unique, indexing can also be utilized to ensure \nthe integrity of data. This avoids storing duplicate data in the \ndatabase, which might lead to issues when performing queries or \nreports. \n \n \n \n \nDisadvantages of Indexing \n\uf0b7 Indexing necessitates more storage space to hold the index data \nstructure, which might increase the total size of the database. \n\uf0b7 Increased database maintenance overhead: Indexes must be \nmaintained as data is added, destroyed, or modified in the table, \nwhich might raise database maintenance overhead. \n\uf0b7 Indexing can reduce insert and update performance since the index \ndata structure must be updated each time data is modified. \n\uf0b7 Choosing an index can be difficult: It can be challenging to choose \nthe right indexes for a specific query or application and may call for \na detailed examination of the data and access patterns. \n \nFeatures of Indexing \n\uf0b7 The development of data structures, such as B-trees or hash tables, \nthat provide quick access to certain data items is known as indexing. \nThe data structures themselves are built on the values of the indexed \ncolumns, which are utilized to quickly find the data objects. \n\uf0b7 The most important columns for indexing columns are selected based \non how frequently they are used and the sorts of queries they are \nsubjected to. The cardinality, selectivity, and uniqueness of the \nindexing columns can be taken into account. \n\uf0b7 There are several different index types used by databases, including \nprimary, secondary, clustered, and non-clustered indexes. Based on \nthe needs of the database system, each form of index offers benefits \nand drawbacks. \n\uf0b7 For the database system to function at its best, periodic index \nmaintenance is required. According to changes in the data and usage \npatterns, maintenance work involves building, updating, and \nremoving indexes. \n\uf0b7 Database query optimization involves indexing, which is essential. \nThe query optimizer utilizes the indexes to choose the best execution \nstrategy for a particular query based on the cost of accessing the data \nand the selectivity of the indexing columns. \n\uf0b7 Databases make use of a range of indexing strategies, including \ncovering indexes, index-only scans, and partial indexes. These \ntechniques maximize the utilization of indexes for particular types of \nqueries and data access. \n\uf0b7 When non-contiguous data blocks are stored in an index, it can result \nin index fragmentation, which makes the index less effective. \nRegular index maintenance, such as defragmentation and \nreorganization, can decrease fragmentation. \n \n \n \n \n \nTransaction and Concurrency Control \n \n11. \nConcurrency Control Overview \n \nAn index is a schema object. It is used by the server to speed up the retrieval of \nrows by using a pointer. It can reduce disk I/O(input/output) by using a rapid \npath access method to locate data quickly. \nAn index helps to speed up select queries and where clauses, but it slows down \ndata input, with the update and the insert statements. Indexes can be created or \ndropped with no effect on the data. In this article, we will see how \nto create, delete, and use the INDEX in the database.   \n \nCreating an Index \nSyntax \nCREATE INDEX index \nON TABLE column; \nwhere the index is the name given to that index TABLE is the name of the table \non which that index is created, and column is the name of that column for which \nit is applied.  \n \nFor Multiple Columns \nSyntax: \nCREATE INDEX index \nON TABLE (column1, column2,\u2026..); \n \nFor Unique Indexes \nUnique indexes are used for the maintenance of the integrity of the data present \nin the table as well as for fast performance, it does not allow multiple values to \nenter the table. \nSyntax: \nCREATE UNIQUE INDEX index \nON TABLE column \nWhen Should Indexes be Created? \n\uf0b7 A column contains a wide range of values. \n\uf0b7 A column does not contain a large number of null values. \n\uf0b7 One or more columns are frequently used together in a where clause \nor a join condition. \nWhen Should Indexes be Avoided? \n\uf0b7 The table is small \n\uf0b7 The columns are not often used as a condition in the query \n\uf0b7 The column is updated frequently \n \n \nRemoving an Index \nRemove \nan \nindex \nfrom \nthe \ndata \ndictionary \nby \nusing \nthe DROP INDEX command.  \nSyntax \nDROP INDEX index; \nTo drop an index, you must be the owner of the index or have the DROP ANY \nINDEX privilege.  \n \nAltering an Index \nTo modify an existing table\u2019s index by rebuilding or reorganizing the index. \nALTER INDEX IndexName \nON TableName REBUILD; \n \nConfirming Indexes \nYou can check the different indexes present in a particular table given by the \nuser or the server itself and their uniqueness.  \nSyntax: \nSELECT * from USER_INDEXES; \nIt will show you all the indexes present in the server, in which you can locate \nyour own tables too. \nRenaming an Index \nYou can use the system-stored procedure sp_rename to rename any index in the \ndatabase. \nSyntax: \nEXEC sp_rename \nindex_name, \nnew_index_name, \nN\u2019INDEX\u2019; \nSQL Server Database \nSyntax: \nDROP INDEX TableName.IndexName;   \n \nWhy SQL Indexing is Important? \nIndexing is an important topic when considering advanced MySQL, although \nmost people know about its definition and usage they don\u2019t understand when \nand where to use it to change the efficiency of our queries or stored procedures \nby a huge margin. \nHere are some scenarios along with their explanation related to Indexing: \n\uf0b7 When executing a query on a table having huge data  \n( > 100000 rows ), MySQL performs a full table scan which \ntakes much time and the server usually gets timed out. To avoid \nthis always check the explain option for the query within \n \n \nMySQL which tells us about the state of execution. It shows \nwhich columns are being used and whether it will be a threat to \nhuge data. On basis of the columns repeated in a similar order in \ncondition. \n\uf0b7 The order of the index is of huge importance as we can use the \nsaitions, we can create an index for them in the same order to \nmaximize the speed of the query. me index in many scenarios. \nUsing only one index we can utilize it in more than one query \nwhich different conditions. like for example, in a query, we \nmake a join with a table based on customer_id wards we also \njoin another join based on customer_id and order_date. Then we \ncan simply create a single index by the order of customer_id, \norder_date which would be used in both cases. This also saves \nstorage. \n\uf0b7 We should also be careful to not make an index for each query \nas creating indexes also take storage and when the amount of \ndata is huge it will create a problem. Therefore, it\u2019s important to \ncarefully consider which columns to index based on the needs of \nyour application. In general, it\u2019s a good practice to only create \nindexes on columns that are frequently used in queries and to \navoid creating indexes on columns that are rarely used. It\u2019s also \na good idea to periodically review the indexes in your database \nand remove any that are no longer needed. \n\uf0b7 Indexes can also improve performance when used in \nconjunction with sorting and grouping operations. For example, \nif you frequently sort or group data based on a particular \ncolumn, creating an index on that column can greatly improve \nperformance. The index allows MySQL to quickly access and \nsort or group the data, rather than having to perform a full table \nscan. \n\uf0b7 In some cases, MySQL may not use an index even if one exists. \nThis can happen if the query optimizer determines that a full \ntable scan is faster than using the index. \n \nConcurrency Control Problems \nThere are several problems that arise when numerous transactions are executed \nsimultaneously in a random manner. The database transaction consist of two \nmajor operations \u201cRead\u201d and \u201cWrite\u201d. It is very important to manage these \noperations in the concurrent execution of the transactions in order to maintain \nthe consistency of the data.  \n \n \n \nDirty Read Problem (Write-Read conflict) \nDirty read problem occurs when one transaction updates an item but due to some \nunconditional events that transaction fails but before the transaction performs \nrollback, some other transaction reads the updated value. Thus creates an \ninconsistency in the database. Dirty read problem comes under the scenario of \nWrite-Read conflict between the transactions in the database. \n\uf0b7 The lost update problem can be illustrated with the below scenario \nbetween two transactions T1 and T2. \n\uf0b7 Transaction T1 modifies a database record without committing the \nchanges. \n\uf0b7 T2 reads the uncommitted data changed by T1 \n\uf0b7 T1 performs rollback \n\uf0b7 T2 has already read the uncommitted data of T1 which is no longer \nvalid, thus creating inconsistency in the database. \n \n \nLost Update Problem \nLost update problem occurs when two or more transactions modify the same \ndata, resulting in the update being overwritten or lost by another transaction. \nThe lost update problem can be illustrated with the below scenario between two \ntransactions T1 and T2. \n\uf0b7 T1 reads the value of an item from the database. \n\uf0b7 T2 starts and reads the same database item. \n\uf0b7 T1 updates the value of that data and performs a commit. \n\uf0b7 T2 updates the same data item based on its initial read and performs \ncommit. \n\uf0b7 This results in the modification of T1 gets lost by the T2\u2019s writes which \ncauses a lost update problem in the database. \n \n \nConcurrency Control Protocols \nConcurrency control protocols are the set of rules which are maintained to solve \nthe concurrency control problems in the database. It ensures that the concurrent \ntransactions can execute properly while maintaining the database consistency. \nThe concurrent execution of a transaction is provided with atomicity, \nconsistency, isolation, durability, and serializability via the concurrency control \nprotocols. \n\uf0b7 Locked based concurrency control protocol. \n\uf0b7 Timestamp based concurrency control protocol. \n \n \n \nLocked based Protocol. \nIn locked based protocol, each transaction needs to acquire locks before they \nstart accessing or modifying the data items. There are two types of locks used \nin databases. \n\uf0b7 Shared Lock: Shared lock is also known as read lock which allows \nmultiple transactions to read the data simultaneously. The transaction \nwhich is holding a shared lock can only read the data item, but it cannot \nmodify the data item. \n\uf0b7 Exclusive Lock: Exclusive lock is also known as the write lock. \nExclusive lock allows a transaction to update a data item. Only one \ntransaction can hold the exclusive lock on a data item at a time. While \na transaction is holding an exclusive lock on a data item, no other \ntransaction is allowed to acquire a shared/exclusive lock on the same \ndata item. \n \nThere are two kind of lock based protocol mostly used in database: \n\uf0b7 Two Phase Locking Protocol:  Two phase locking is a widely used \ntechnique which ensures strict ordering of lock acquisition and \nrelease. Two phase locking protocol works in two phases. \n\uf0b7 Growing Phase: In this phase, the transaction starts acquiring locks \nbefore performing any modification on the data items. Once a \ntransaction acquires a lock, that lock can not be released until the \ntransaction reaches the end of the execution. \n\uf0b7 Shrinking Phase : In this phase, the transaction releases all the \nacquired locks once it performs all the modifications on the data \nitem. Once the transaction starts releasing the locks, it can not \nacquire any locks further.  \n\uf0b7 Strict Two Phase Locking Protocol : It is almost similar to the two \nphase locking protocol the only difference is that in two phase \nlocking the transaction can release its locks before it commits, but in \ncase of strict two phase locking the transactions are only allowed to \nrelease the locks only when they performs commits.  \n \nTimestamp based Protocol \n\uf0b7 In this protocol each transaction has a timestamp attached to it. \nTimestamp is nothing but the time in which a transaction enters into \nthe system. \n\uf0b7 The conflicting pairs of operations can be resolved by the timestamp \nordering protocol through the utilization of the timestamp values of \nthe transactions. Therefore, guaranteeing that the transactions take \nplace in the correct order. \n \n \n \n \nAdvantages of Concurrency \nIn general, concurrency means, that more than one transaction can work on a \nsystem. The advantages of a concurrent system are: \n\uf0b7 Waiting Time: It means if a process is in a ready state but still the \nprocess does not get the system to get execute is called waiting time. \nSo, concurrency leads to less waiting time. \n\uf0b7 Response Time: The time wasted in getting the response from the \ncpu for the first time, is called response time. So, concurrency leads \nto less Response Time. \n\uf0b7 Resource Utilization: The amount of Resource utilization in a \nparticular system is called Resource Utilization. Multiple \ntransactions can run parallel in a system. So, concurrency leads to \nmore Resource Utilization. \n\uf0b7 Efficiency: The amount of output produced in comparison to given \ninput is called efficiency. So, Concurrency leads to more Efficiency. \n \nDisadvantages of Concurrency  \n\uf0b7 Overhead: Implementing concurrency control requires additional \noverhead, such as acquiring and releasing locks on database objects. \nThis overhead can lead to slower performance and increased resource \nconsumption, particularly in systems with high levels of concurrency. \n\uf0b7 Deadlocks: Deadlocks can occur when two or more transactions are \nwaiting for each other to release resources, causing a circular \ndependency that can prevent any of the transactions from completing. \nDeadlocks can be difficult to detect and resolve, and can result in \nreduced throughput and increased latency. \n\uf0b7 Reduced concurrency: Concurrency control can limit the number of \nusers or applications that can access the database simultaneously. \nThis can lead to reduced concurrency and slower performance in \nsystems with high levels of concurrency. \n\uf0b7 Complexity: Implementing concurrency control can be complex, \nparticularly in distributed systems or in systems with complex \ntransactional logic. This complexity can lead to increased \ndevelopment and maintenance costs. \n\uf0b7 Inconsistency: In some cases, concurrency control can lead to \ninconsistencies in the database. For example, a transaction that is \nrolled back may leave the database in an inconsistent state, or a long-\nrunning transaction may cause other transactions to wait for extended \nperiods, leading to data staleness and reduced accuracy. \n \n \n \n \n \n \n12. \nACID Properties in DBMS \n \nA transaction is a single logical unit of work that accesses and possibly \nmodifies the contents of a database. Transactions access data using read and \nwrite operations.  \nTo maintain consistency in a database, before and after the transaction, certain \nproperties are followed. These are called ACID properties.  \n                                        Figure 31: ACID Property \nAtomicity: \nBy this, we mean that either the entire transaction takes place at once or does \nnot happen at all. There is no midway i.e. transactions do not occur partially. \nEach transaction is considered as one unit and either runs to completion or is \nnot executed at all. It involves the following two operations.  \n\u2014Abort: If a transaction aborts, changes made to the database are not visible.  \n\u2014Commit: If a transaction commits, changes made are visible.  \nAtomicity is also known as the \u2018All or nothing rule\u2019.  \nConsider the following transaction T consisting of T1 and T2: Transfer of 100 \nfrom account X to account Y.   \n \n \n \nIf the transaction fails after completion of T1 but before completion of T2.( \nsay, after write(X) but before write(Y)), then the amount has been deducted \nfrom X but not added to Y. This results in an inconsistent database state. \nTherefore, the transaction must be executed in its entirety in order to ensure \nthe correctness of the database state.  \nConsistency: \nThis means that integrity constraints must be maintained so that the database is \nconsistent before and after the transaction. It refers to the correctness of a \ndatabase. Referring to the example above,  \nThe total amount before and after the transaction must be maintained.  \nTotal before T occurs = 500 + 200 = 700.  \nTotal after T occurs = 400 + 300 = 700.  \nTherefore, the database is consistent. Inconsistency occurs in \ncase T1 completes but T2 fails. As a result, T is incomplete.  \nIsolation: \nThis property ensures that multiple transactions can occur concurrently \nwithout leading to the inconsistency of the database state. Transactions occur \nindependently without interference. Changes occurring in a particular \ntransaction will not be visible to any other transaction until that particular \nchange in that transaction is written to memory or has been committed. This \nproperty ensures that the execution of transactions concurrently will result in a \nstate that is equivalent to a state achieved these were executed serially in some \norder.  \nLet X= 500, Y = 500.  \nConsider two transactions T and T\u201d.  \nSuppose T has been executed till Read (Y) and then T\u2019\u2019 starts. As a result, \ninterleaving of operations takes place due to which T\u2019\u2019 reads the correct value \nof X but the incorrect value of Y and sum computed by  \nT\u2019\u2019: (X+Y = 50, 000+500=50, 500)  \nis thus not consistent with the sum at end of the transaction:  \nT: (X+Y = 50, 000 + 450 = 50, 450).  \nThis results in database inconsistency, due to a loss of 50 units. Hence, \ntransactions must take place in isolation and changes should be visible only \nafter they have been made to the main memory.  \n \n \nDurability:  \nThis property ensures that once the transaction has completed execution, the \nupdates and modifications to the database are stored in and written to disk and \nthey persist even if a system failure occurs. These updates now become \npermanent and are stored in non-volatile memory. The effects of the transaction, \nthus, are never lost.  \nSome important points: \nProperty \nResponsibility for maintaining properties \nAtomicity \nTransaction Manager \nConsistency \nApplication programmer \nIsolation \nConcurrency Control Manager \nDurability \nRecovery Manager \n \nThe ACID properties, in totality, provide a mechanism to ensure the correctness \nand consistency of a database in a way such that each transaction is a group of \noperations that acts as a single unit, produces consistent results, acts in isolation \nfrom other operations, and updates that it makes are durably stored.  \nACID properties are the four key characteristics that define the reliability and \nconsistency of a transaction in a Database Management System (DBMS). The \nacronym ACID stands for Atomicity, Consistency, Isolation, and Durability. \nHere is a brief description of each of these properties: \n\uf0b7 Atomicity: Atomicity ensures that a transaction is treated as a single, \nindivisible unit of work. Either all the operations within the transaction \nare completed successfully, or none of them are. If any part of the \ntransaction fails, the entire transaction is rolled back to its original \nstate, ensuring data consistency and integrity. \n\uf0b7 Consistency: Consistency ensures that a transaction takes the database \nfrom one consistent state to another consistent state. The database is \nin a consistent state both before and after the transaction is executed. \nConstraints, such as unique keys and foreign keys, must be maintained \nto ensure data consistency. \n\uf0b7 Isolation: Isolation ensures that multiple transactions can execute \nconcurrently without interfering with each other. Each transaction \nmust be isolated from other transactions until it is completed. This \nisolation prevents dirty reads, non-repeatable reads, and phantom \nreads. \n\uf0b7 Durability: Durability ensures that once a transaction is committed, \nits changes are permanent and will survive any subsequent system \nfailures. The transaction\u2019s changes are saved to the database \npermanently, and even if the system crashes, the changes remain intact \nand can be recovered. \n \n \nOverall, ACID properties provide a framework for ensuring data consistency, \nintegrity, and reliability in DBMS. They ensure that transactions are executed \nin a reliable and consistent manner, even in the presence of system failures, \nnetwork issues, or other problems. These properties make DBMS a reliable \nand efficient tool for managing data in modern organizations. \nAdvantages of ACID Properties in DBMS: \n\uf0b7 Data Consistency: ACID properties ensure that the data remains \nconsistent and accurate after any transaction execution. \n\uf0b7 Data Integrity: ACID properties maintain the integrity of the data by \nensuring that any changes to the database are permanent and cannot be \nlost. \n\uf0b7 Concurrency Control: ACID properties help to manage multiple \ntransactions occurring concurrently by preventing interference \nbetween them. \n\uf0b7 Recovery: ACID properties ensure that in case of any failure or crash, \nthe system can recover the data up to the point of failure or crash. \nDisadvantages of ACID Properties in DBMS: \n\uf0b7 Performance: The ACID properties can cause a performance overhead \nin the system, as they require additional processing to ensure data \nconsistency and integrity. \n\uf0b7 Scalability: The ACID properties may cause scalability issues in large \ndistributed systems where multiple transactions occur concurrently. \n\uf0b7 Complexity: Implementing the ACID properties can increase the \ncomplexity of the system and require significant expertise and \nresources. \nOverall, the advantages of ACID properties in DBMS outweigh the \ndisadvantages. They provide a reliable and consistent approach to data. \n\uf0b7 management, ensuring data integrity, accuracy, and reliability. \nHowever, in some cases, the overhead of implementing ACID \nproperties can cause performance and scalability issues. Therefore, it\u2019s \nimportant to balance the benefits of ACID properties against the \nspecific needs and requirements of the system. \n \n \n \n \n \n \n \n \n \n \n13. \nDatabase Recovery Techniques \nDatabase Systems like any other computer system, are subject to failures but the \ndata stored in them must be available as and when required. When a database \nfails it must possess the facilities for fast recovery. It must also have atomicity \ni.e. either transactions are completed successfully and committed (the effect is \nrecorded permanently in the database) or the transaction should have no effect \non the database. \nTypes of Recovery Techniques in DBMS \nDatabase recovery techniques are used in database management systems \n(DBMS) to restore a database to a consistent state after a failure or error has \noccurred. The main goal of recovery techniques is to ensure data integrity and \nconsistency and prevent data loss. \nThere are mainly two types of recovery techniques used in DBMS. \n\uf0b7 Rollback/Undo Recovery Technique \n\uf0b7 Commit/Redo Recovery Technique \n \nRollback/Undo Recovery Technique \nThe rollback/undo recovery technique is based on the principle of backing out \nor undoing the effects of a transaction that has not been completed successfully \ndue to a system failure or error. This technique is accomplished by undoing the \nchanges made by the transaction using the log records stored in the transaction \nlog. The transaction log contains a record of all the transactions that have been \nperformed on the database. The system uses the log records to undo the changes \nmade by the failed transaction and restore the database to its previous state. \nCommit/Redo Recovery Technique \nThe commit/redo recovery technique is based on the principle of reapplying the \nchanges made by a transaction that has been completed successfully to the \ndatabase. This technique is accomplished by using the log records stored in the \ntransaction log to redo the changes made by the transaction that was in progress \nat the time of the failure or error. The system uses the log records to reapply the \nchanges made by the transaction and restore the database to its most recent \nconsistent state. \nIn addition to these two techniques, there is also a third technique \ncalled checkpoint recovery. \nCheckpoint Recovery is a technique used to reduce the recovery time by \nperiodically saving the state of the database in a checkpoint file. In the event of \na failure, the system can use the checkpoint file to restore the database to the \nmost recent consistent state before the failure occurred, rather than going \nthrough the entire log to recover the database. \nOverall, recovery techniques are essential to ensure data consistency and \navailability in Database Management System, and each technique has its own \n \n \nadvantages and limitations that must be considered in the design of a recovery \nsystem. \nDatabase Systems \nThere are both automatic and non-automatic ways for both, backing up data and \nrecovery from any failure situations. The techniques used to recover lost data \ndue to system crashes, transaction errors, viruses, catastrophic failure, incorrect \ncommand execution, etc. are database recovery techniques. So to prevent data \nloss recovery techniques based on deferred updates and immediate updates or \nbacking up data can be used. Recovery techniques are heavily dependent upon \nthe existence of a special file known as a system log. It contains information \nabout the start and end of each transaction and any updates which occur during \nthe transaction. The log keeps track of all transaction operations that affect the \nvalues of database items. This information is needed to recover from transaction \nfailure. \n\uf0b7 The log is kept on disk start_transaction(T): This log entry records \nthat transaction T starts the execution. \n\uf0b7 read_item(T, X): This log entry records that transaction T reads the \nvalue of database item X. \n\uf0b7 write_item(T, X, old_value, new_value): This log entry records that \ntransaction T changes the value of the database item X from old_value \nto new_value. The old value is sometimes known as a before an image \nof X, and the new value is known as an afterimage of X. \n\uf0b7 commit(T): This log entry records that transaction T has completed \nall accesses to the database successfully and its effect can be \ncommitted (recorded permanently) to the database. \n\uf0b7 abort(T): This records that transaction T has been aborted. \n\uf0b7 checkpoint: A checkpoint is a mechanism where all the previous logs \nare removed from the system and stored permanently in a storage disk. \nCheckpoint declares a point before which the DBMS was in a \nconsistent state, and all the transactions were committed. \nA transaction T reaches its commit point when all its operations that access the \ndatabase have been executed successfully i.e. the transaction has reached the \npoint at which it will not abort (terminate without completing). Once \ncommitted, the transaction is permanently recorded in the database. \nCommitment always involves writing a commit entry to the log and writing the \nlog to disk. At the time of a system crash, the item is searched back in the log \nfor all transactions T that have written a start_transaction(T) entry into the log \nbut have not written a commit(T) entry yet; these transactions may have to be \nrolled back to undo their effect on the database during the recovery process. \n\uf0b7 Undoing: If a transaction crashes, then the recovery manager may \nundo transactions i.e. reverse the operations of a transaction. This \ninvolves examining a transaction for the log entry write_item(T, x, \nold_value, new_value) and setting the value of item x in the database \n \n \nto old-value. There are two major techniques for recovery from non-\ncatastrophic transaction failures: deferred updates and immediate \nupdates. \n\uf0b7 Deferred Update: This technique does not physically update the \ndatabase on disk until a transaction has reached its commit point. \nBefore reaching commit, all transaction updates are recorded in the \nlocal transaction workspace. If a transaction fails before reaching its \ncommit point, it will not have changed the database in any way so \nUNDO is not needed. It may be necessary to REDO the effect of the \noperations that are recorded in the local transaction workspace, \nbecause their effect may not yet have been written in the database. \nHence, a deferred update is also known as the No-undo/redo \nalgorithm. \n\uf0b7 Immediate Update: In the immediate update, the database may be \nupdated by some operations of a transaction before the transaction \nreaches its commit point. However, these operations are recorded in a \nlog on disk before they are applied to the database, making recovery \nstill possible. If a transaction fails to reach its commit point, the effect \nof its operation must be undone i.e., the transaction must be rolled back \nhence we require both undo and redo. This technique is known \nas undo/redo algorithm. \n\uf0b7 Caching/Buffering: In these one or more disk pages that include data \nitems to be updated are cached into main memory buffers and then \nupdated in memory before being written back to disk. A collection of \nin-memory buffers called the DBMS cache is kept under the control of \nDBMS for holding these buffers. A directory is used to keep track of \nwhich database items are in the buffer. A dirty bit is associated with \neach buffer, which is 0 if the buffer is not modified else 1 if modified. \n\uf0b7 Shadow Paging: It provides atomicity and durability. A directory with \nn entries is constructed, where the ith entry points to the ith database \npage on the link. When a transaction began executing the current \ndirectory is copied into a shadow directory. When a page is to be \nmodified, a shadow page is allocated in which changes are made and \nwhen it is ready to become durable, all pages that refer to the original \nare updated to refer new replacement page. \n\uf0b7 Backward Recovery: The term \u201cRollback\u201d and \u201cUNDO\u201d can also \nrefer to backward recovery. When a backup of the data is not available \nand previous modifications need to be undone, this technique can be \nhelpful. With the backward recovery method, unused modifications \nare removed, and the database is returned to its prior condition. All \nadjustments made during the previous traction are reversed during the \nbackward recovery. In other words, it reprocesses valid transactions \nand undoes the erroneous database updates. \n \n \n\uf0b7 Forward Recovery: \u201cRoll forward \u201cand \u201cREDO\u201d refers to \nforwarding recovery. When a database needs to be updated with all \nchanges verified, this forward recovery technique is helpful. Some \nfailed transactions in this database are applied to the database to roll \nthose modifications forward. In other words, the database is restored \nusing preserved data and valid transactions counted by their past saves. \n \nBackup Techniques \nThere are different types of Backup Techniques. Some of them are listed below. \n\uf0b7 Full database Backup: In this full database including data and \ndatabase, Meta information needed to restore the whole database, \nincluding full-text catalogs are backed up in a predefined time series. \n\uf0b7 Differential Backup: It stores only the data changes that have \noccurred since the last full database backup. When some data has \nchanged many times since the last full database backup, a differential \nbackup stores the most recent version of the changed data. For this \nfirst, we need to restore a full database backup. \n\uf0b7 Transaction Log Backup: In this, all events that have occurred in the \ndatabase, like a record of every single statement executed is backed \nup. It is the backup of transaction log entries and contains all \ntransactions that had happened to the database. Through this, the \ndatabase can be recovered to a specific point in time. It is even possible \nto perform a backup from a transaction log if the data files are \ndestroyed and not even a single committed transaction is lost. \n \nLog based recovery \nThe atomicity property of DBMS states that either all the operations of \ntransactions must be performed or none. The modifications done by an aborted \ntransaction should not be visible to the database and the modifications done by \nthe committed transaction should be visible. To achieve our goal of atomicity, \nthe user must first output stable storage information describing the \nmodifications, without modifying the database itself. This information can help \nus ensure that all modifications performed by committed transactions are \nreflected in the database. This information can also help us ensure that no \nmodifications made by an aborted transaction persist in the database. \n \n \n                               Figure 32: Log Based Recovery  \n \nLog and log records \nThe log is a sequence of log records, recording all the updated activities in the \ndatabase. In stable storage, logs for each transaction are maintained. Any \noperation which is performed on the database is recorded on the log. Prior to \nperforming any modification to the database, an updated log record is created to \nreflect that modification. An update log record represented as: <Ti, Xj, V1, V2> \nhas these fields: \ni) Transaction identifier: Unique Identifier of the transaction that \nperformed the write operation. \nii) Data item: Unique identifier of the data item written. \niii) Old value: Value of data item prior to write. \niv) New value: Value of data item after write operation. \n \nOther types of log records are: \ni) <Ti start>: It contains information about when a transaction Ti starts. \nii) <Ti commit>: It contains information about when a transaction Ti \ncommits. \niii) <Ti abort>: It contains information about when a transaction Ti \naborts. \n \nUndo and Redo Operations \nBecause all database modifications must be preceded by the creation of a log \nrecord, the system has available both the old value prior to the modification of \nthe data item and new value that is to be written for data item. This allows \nsystem to perform redo and undo operations as appropriate: \ni) Undo: using a log record sets the data item specified in log record to \nold value. \n \n \nii) Redo: using a log record sets the data item specified in log record to \nnew value. \n \nThe database can be modified using two approaches \u2013 \ni) Deferred Modification Technique: If the transaction does not \nmodify the database until it has partially committed, it is said to use \ndeferred modification technique. \nii) Immediate Modification Technique: If database modification occur \nwhile the transaction is still active, it is said to use immediate \nmodification technique. \n \nRecovery using Log records \nAfter a system crash has occurred, the system consults the log to determine \nwhich transactions need to be redone and which need to be undone. \ni) Transaction Ti needs to be undone if the log contains the record <Ti \nstart> but does not contain either the record <Ti commit> or the record \n<Ti abort>. \nii) Transaction Ti needs to be redone if log contains record <Ti start> and \neither the record <Ti commit> or the record <Ti abort>. \n \nAdvantages of Log based Recovery \n\uf0b7 Durability: In the event of a breakdown, the log file offers a \ndependable and long-lasting method of recovering data. It guarantees \nthat in the event of a system crash, no committed transaction is lost. \n\uf0b7 Faster Recovery: Since log-based recovery recovers databases by \nreplaying committed transactions from the log file, it is typically faster \nthan alternative recovery methods. \n\uf0b7 Incremental Backup: Backups can be made in increments using log-\nbased recovery. Just the changes made since the last backup are kept \nin the log file, rather than creating a complete backup of the database \neach time. \n\uf0b7 Lowers the Risk of Data Corruption: By making sure that all \ntransactions are correctly committed or canceled before they are \nwritten to the database, log-based recovery lowers the risk of data \ncorruption. \n \nDisadvantages of Log based Recovery \n\uf0b7 Additional overhead: Maintaining the log file incurs an additional \noverhead on the database system, which can reduce the performance \nof the system. \n\uf0b7 Complexity: Log-based recovery is a complex process that requires \ncareful management and administration. If not managed properly, it \ncan lead to data inconsistencies or loss. \n \n \n\uf0b7 Storage space: The log file can consume a significant amount of \nstorage space, especially in a database with many transactions. \n\uf0b7 Time-Consuming: The process of replaying the transactions from the \nlog file can be time-consuming, especially if there are many \ntransactions to recover. \n \nWhy Recovery is Required in Database? \nHere are some of the reasons why recovery is needed in DBMS. \n\uf0b7 System failures: The DBMS can experience various types of failures, \nsuch as hardware failures, software bugs, or power outages, which can \nlead to data corruption or loss. Recovery mechanisms can help restore \nthe database to a consistent state after such failures. \n\uf0b7 Transaction failures: Transactions can fail due to various reasons, \nsuch as network failures, deadlock, or errors in application logic. \nRecovery mechanisms can help roll back or undo the effects of such \nfailed transactions to ensure data consistency. \n\uf0b7 Human errors: Human errors such as accidental deletion, updating or \noverwriting data, or incorrect data entry can cause data \ninconsistencies. Recovery mechanisms can help recover the lost or \ncorrupted data and restore it to the correct state. \n\uf0b7 Security \nbreaches: Security \nbreaches \nsuch \nas \nhacking \nor \nunauthorized access can compromise the integrity of data. Recovery \nmechanisms can help restore the database to a consistent state and \nprevent further data breaches. \n\uf0b7 Hardware upgrades: When a DBMS is upgraded to a new hardware \nsystem, the migration process can potentially lead to data loss or \ncorruption. Recovery mechanisms can help ensure that the data is \nsuccessfully migrated and the integrity of the database is maintained. \n\uf0b7 Natural disasters: Natural disasters such as earthquakes, floods, or \nfires can damage the hardware on which the database is stored, leading \nto data loss. Recovery mechanisms can help restore the data from \nbackups and minimize the impact of the disaster. \n\uf0b7 Compliance regulations: Many industries have regulations that \nrequire businesses to retain data for a certain period. Recovery \nmechanisms can help ensure that the data is available for compliance \npurposes even if it was deleted or lost accidentally. \n\uf0b7 Data corruption: Data corruption can occur due to various reasons \nsuch as hardware failure, software bugs, or viruses. Recovery \nmechanisms can help restore the database to a consistent state and \nrecover any lost or corrupted data. \n \n \n \n \n \n14. \nTransaction and Isolation Levels \n \nAs we know, to maintain consistency in a database, it follows ACID properties. \nAmong these four properties (Atomicity, Consistency, Isolation, and Durability) \nIsolation determines how transaction integrity is visible to other users and \nsystems. It means that a transaction should take place in a system in such a way \nthat it is the only transaction that is accessing the resources in a database \nsystem.  \nIsolation levels define the degree to which a transaction must be isolated from \nthe data modifications made by any other transaction in the database system. A \ntransaction isolation level is defined by the following phenomena:  \n\uf0b7 Dirty Read \u2013 A Dirty read is a situation when a transaction reads data \nthat has not yet been committed. For example, Let\u2019s say transaction 1 \nupdates a row and leaves it uncommitted, meanwhile, Transaction 2 \nreads the updated row. If transaction 1 rolls back the change, \ntransaction 2 will have read data that is considered never to have \nexisted. \n\uf0b7 Non-Repeatable read \u2013 Non-Repeatable read occurs when a \ntransaction reads the same row twice and gets a different value each \ntime. For example, suppose transaction T1 reads data. Due to \nconcurrency, another transaction T2 updates the same data and \ncommit, now if transaction T1 rereads the same data, it will retrieve a \ndifferent value. \n\uf0b7 Phantom Read \u2013 Phantom Read occurs when two same queries are \nexecuted, but the rows retrieved by the two, are different. For example, \nsuppose transaction T1 retrieves a set of rows that satisfy some search \ncriteria. Now, Transaction T2 generates some new rows that match the \nsearch criteria for transaction T1. If transaction T1 re-executes the \nstatement that reads the rows, it gets a different set of rows this time. \n \nBased on these phenomena, The SQL standard defines four isolation \nlevels:   \n\uf0b7 Read Uncommitted \u2013 Read Uncommitted is the lowest isolation level. \nIn this level, one transaction may read not yet committed changes made \nby other transactions, thereby allowing dirty reads. At this level, \ntransactions are not isolated from each other. \n\uf0b7 Read Committed \u2013 This isolation level guarantees that any data read is \ncommitted at the moment it is read. Thus it does not allow dirty read. The \ntransaction holds a read or write lock on the current row, and thus prevents \nother transactions from reading, updating, or deleting it. \n \n \n\uf0b7 Repeatable Read \u2013 This is the most restrictive isolation level. The \ntransaction holds read locks on all rows it references and writes locks on \nreferenced rows for update and delete actions. Since other transactions \ncannot read, update or delete these rows, consequently it avoids non-\nrepeatable read. \n\uf0b7 Serializable \u2013 This is the highest isolation level. A serializable execution \nis guaranteed to be serializable. Serializable execution is defined to be an \nexecution of operations in which concurrently executing transactions \nappears to be serially executing. \nThe Table given below clearly depicts the relationship between isolation levels, \nread phenomena, and locks: \nAnomaly Serializable is not the same as Serializable. That is, it is necessary, but \nnot sufficient that a Serializable schedule should be free of all three phenomena \ntypes.  \n \nTransaction isolation levels are used in database management systems (DBMS) \nto control the level of interaction between concurrent transactions.  \n \nThe four standard isolation levels are: \n\uf0b7 Read Uncommitted: This is the lowest level of isolation where a \ntransaction can see uncommitted changes made by other transactions. \nThis can result in dirty reads, non-repeatable reads, and phantom reads. \n\uf0b7 Read Committed: In this isolation level, a transaction can only see \nchanges made by other committed transactions. This eliminates dirty \nreads but can still result in non-repeatable reads and phantom reads. \n\uf0b7 Repeatable Read: This isolation level guarantees that a transaction will \nsee the same data throughout its duration, even if other transactions \ncommit changes to the data. However, phantom reads are still possible. \n\uf0b7 Serializable: This is the highest isolation level where a transaction is \nexecuted as if it were the only transaction in the system. All transactions \nmust be executed sequentially, which ensures that there are no dirty reads, \nnon-repeatable reads, or phantom reads. \n \n \n \nThe choice of isolation level depends on the specific requirements of the \napplication. Higher isolation levels offer stronger data consistency but can also \nresult in longer lock times and increased contention, leading to decreased \nconcurrency and performance. Lower isolation levels provide more concurrency \nbut can result in data inconsistencies. \nIn addition to the standard isolation levels, some DBMS may also support \nadditional custom isolation levels or features such as snapshot isolation and \nmulti-version concurrency control (MVCC) that provide alternative solutions to \nthe problems addressed by the standard isolation levels. \nAdvantages of Transaction Isolation Levels: \nImproved concurrency: Transaction isolation levels can improve concurrency \nby allowing multiple transactions to run concurrently without interfering with \neach other. \nControl over data consistency: Isolation levels provide control over the level \nof data consistency required by a particular application. \nReduced data anomalies: The use of isolation levels can reduce data anomalies \nsuch as dirty reads, non-repeatable reads, and phantom reads. \nFlexibility: The use of different isolation levels provides flexibility in designing \napplications that require different levels of data consistency. \nDisadvantages of Transaction Isolation Levels: \n\uf0b7 Increased overhead: The use of isolation levels can increase overhead \nbecause the database management system must perform additional checks \nand acquire more locks. \n\uf0b7 Decreased concurrency: Some isolation levels, such as Serializable, can \ndecrease concurrency by requiring transactions to acquire more locks, \nwhich can lead to blocking. \n\uf0b7 Limited support: Not all database management systems support all \nisolation levels, which can limit the portability of applications across \ndifferent systems. \n\uf0b7 Complexity: The use of different isolation levels can add complexity to \nthe design of database applications, making them more difficult to \nimplement and maintain. \n \n \n \n \n \n \n \n \n \n \n14.1. Types of Schedules in DBMS \n \nSchedule, as the name suggests, is a process of lining the transactions and \nexecuting them one by one. When there are multiple transactions that are \nrunning in a concurrent manner and the order of operation is needed to be set so \nthat the operations do not overlap each other, Scheduling is brought into play \nand the transactions are timed accordingly. The basics of Transactions and \nSchedules is discussed in Concurrency Control (Introduction), and Transaction \nIsolation Levels in DBMS articles.  \nHere we will discuss various types of schedules. \n                                   Figure 33: Types of Schedule in DBMS \n \n1) Serial Schedules: \nSchedules in which the transactions are executed non-interleaved, i.e., a serial \nschedule is one in which no transaction starts until a running transaction has \nended are called serial schedules. \nExample:  \nConsider the following schedule involving two transactions T1 and T2. \nT1 \nT2 \nR(A) \n \nW(A) \n \nR(B) \n \n \nW(B) \n \nR(A) \n \nR(B) \n \n \n \n \nwhere R(A) denotes that a read operation is performed on some data item \u2018A\u2019 \nThis is a serial schedule since the transactions perform serially in the order  \nT1 \u2014> T2 \n \n2) Non-Serial Schedule: \nThis is a type of Scheduling where the operations of multiple transactions are \ninterleaved. This might lead to a rise in the concurrency problem. The \ntransactions are executed in a non-serial manner, keeping the end result correct \nand same as the serial schedule. Unlike the serial schedule where one \ntransaction must wait for another to complete all its operation, in the non-serial \nschedule, the other transaction proceeds without waiting for the previous \ntransaction to complete. This sort of schedule does not provide any benefit of \nthe concurrent transaction. It can be of two types namely, Serializable and \nNon-Serializable Schedule. \nThe Non-Serial Schedule can be divided further into Serializable and Non-\nSerializable. \n3) Serializable: \nThis is used to maintain the consistency of the database. It is mainly used in \nthe non-serial scheduling to verify whether the scheduling will lead to any \ninconsistency or not. On the other hand, a serial schedule does not need the \nserializability because it follows a transaction only when the previous \ntransaction is complete. The non-serial schedule is said to be in a serializable \nschedule only when it is equivalent to the serial schedules, for an n number of \ntransactions. Since concurrency is allowed in this case thus, multiple \ntransactions can execute concurrently. A serializable schedule helps in \nimproving both resource utilization and CPU throughput. These are of two \ntypes: \n\uf0b7 Conflict Serializable: \nA schedule is called conflict serializable if it can be transformed into a \nserial schedule by swapping non-conflicting operations. Two operations \nare said to be conflicting if all conditions satisfy: \n\uf0b7 They belong to different transactions \n\uf0b7 They operate on the same data item \n\uf0b7 At Least one of them is a write operation \n \n\uf0b7 View Serializable: \nA Schedule is called view serializable if it is view equal to a serial \nschedule (no overlapping transactions). A conflict schedule is a view \nserializable but if the serializability contains blind writes, then the view \nserializable does not conflict serializable. \n \n \n4) Non-Serializable: \nThe non-serializable schedule is divided into two types, \nRecoverable and Non-recoverable Schedule. \n\uf0b7 Recoverable Schedule: \nSchedules in which transactions commit only after all transactions \nwhose changes they read commit are called recoverable schedules. In \nother words, if some transaction Tj is reading value updated or written \nby some other transaction Ti, then the commit of Tj must occur after the \ncommit of Ti. \nExample \u2013  \nConsider the following schedule involving two transactions T1 and T2. \nT1 \nT2 \nR(A) \n \nW(A) \n \n \nW(A) \n \nR(A) \ncommit \n \n \ncommit \n \nThis is a recoverable schedule since T1 commits before T2, that makes the value \nread by T2 correct. \nThere can be three types of recoverable schedule: \na) Cascading Schedule: \nAlso called Avoids cascading aborts/rollbacks (ACA). When there is a failure \nin one transaction and this leads to the rolling back or aborting other \ndependent transactions, then such scheduling is referred to as Cascading \nrollback or cascading abort.  Example: \n                                  Figure 34: Cascading Abort \n \n \nb) Cascadeless Schedule: \nSchedules in which transactions read values only after all transactions whose \nchanges they are going to read commit are called cascadeless schedules. Avoids \nthat a single transaction abort leads to a series of transaction rollbacks. A \nstrategy to prevent cascading aborts is to disallow a transaction from reading \nuncommitted changes from another transaction in the same schedule. \nIn other words, if some transaction Tj wants to read value updated or written by \nsome other transaction Ti, then the commit of Tj must read it after the commit \nof Ti. \nExample:  \nConsider the following schedule involving two transactions T1 and T2. \nT1 \nT2 \nR(A) \n \nW(A) \n \n \nW(A) \ncommit \n \n \nR(A) \n \ncommit \nThis schedule is cascadeless. Since the updated value of A is read by T2 only \nafter the updating transaction i.e. T1 commits. \n \nc) Strict Schedule: \nA schedule is strict if for any two transactions Ti, Tj, if a write operation of \nTi precedes a conflicting operation of Tj (either read or write), then the commit \nor abort event of Ti also precedes that conflicting operation of Tj. \nIn other words, Tj can read or write updated or written value of Ti only after \nTi commits/aborts. \nExample:  \nConsider the following schedule involving two transactions T1 and T2. \nT1 \nT2 \nR(A) \n \n \nR(A) \nW(A) \n \ncommit \n \n \nW(A) \n \nR(A) \n \ncommit \nThis is a strict schedule since T2 reads and writes A which is written by T1 only \nafter the commit of T1. \n \n \n \n \n \n\uf0b7 Non-Recoverable Schedule: \nExample: Consider the following schedule involving two transactions \nT1 and T2. \nT1 \nT2 \nR(A) \n \nW(A) \n \n \nW(A) \n \nR(A) \n \ncommit \nabort \n \n \nT2 read the value of A written by T1, and committed. T1 later aborted, therefore \nthe value read by T2 is wrong, but since T2 committed, this schedule is non-\nrecoverable. \nNote \u2013 It can be seen that: \n\uf0b7 Cascadeless schedules are stricter than recoverable schedules or are a \nsubset of recoverable schedules. \n\uf0b7 Strict schedules are stricter than cascadeless schedules or are a subset \nof cascadeless schedules. \n\uf0b7 Serial schedules satisfy constraints of all recoverable, cascadeless and \nstrict schedules and hence is a subset of strict schedules. \n \nThe relation between various types of schedules can be depicted as: \n                        Figure 35: Relation between Schedules \n \n \n \n14.2. Types of Schedules based Recoverability \nwe are going to deal with the types of Schedules based on the Recoverability in \nDatabase Management Systems (DBMS). Generally, there are three types of \nschedules given as follows: \n \nSchedules Based on Recoverability \n\uf0b7 Recoverable Schedule: A schedule is recoverable if it allows for the \nrecovery of the database to a consistent state after a transaction failure. \nIn a recoverable schedule, a transaction that has updated the database \nmust commit before any other transaction reads or writes the same \ndata. If a transaction fails before committing, its updates must be rolled \nback, and any transactions that have read its uncommitted data must \nalso be rolled back. \n\uf0b7 Cascadeless Schedule: A schedule is cascaded less if it does not result \nin a cascading rollback of transactions after a failure. In a cascade-less \nschedule, a transaction that has read uncommitted data from another \ntransaction cannot commit before that transaction commits. If a \ntransaction fails before committing, its updates must be rolled back, \nbut any transactions that have read its uncommitted data need not be \nrolled back. \n\uf0b7 Strict Schedule: A schedule is strict if it is both recoverable and \ncascades. In a strict schedule, a transaction that has read uncommitted \ndata from another transaction cannot commit before that transaction \ncommits, and a transaction that has updated the database must commit \nbefore any other transaction reads or writes the same data. If a \ntransaction fails before committing, its updates must be rolled back, \nand any transactions that have read its uncommitted data must also be \nrolled back. \nThese types of schedules are important because they affect the consistency and \nreliability of the database system. It is essential to ensure that schedules are \nrecoverable, cascaded, or strict to avoid inconsistencies and data loss in the \ndatabase. \n \nRecoverable Schedule \nA schedule is said to be recoverable if it is recoverable as the name suggests. \nOnly reads are allowed before write operations on the same data. Only reads \n(Ti->Tj) are permissible.  \n \nExample: \nS1: R1(x), W1(x), R2(x), R1(y), R2(y),  \n         W2(x), W1(y), C1, C2;  \n \n \nThe given schedule follows the order of Ti->Tj => C1->C2. Transaction T1 is \nexecuted before T2 hence there is no chance of conflict occurring. R1(x) appears \nbefore W1(x) and transaction T1 is committed before T2 i.e. completion of the \nfirst transaction performed the first update on data item x, hence given schedule \nis recoverable.  \n \nLet us see an example of an unrecoverable schedule to clear the concept more. \nS2: R1(x), R2(x), R1(z), R3(x), R3(y), W1(x),  \n       W3(y), R2(y), W2(z), W2(y), C1, C2, C3;  \nTi->Tj => C2->C3 but W3(y) executed before W2(y) which leads to conflicts \nthus it must be committed before the T2 transaction. So given schedule is \nunrecoverable. if Ti->Tj => C3->C2 is given in the schedule then it will \nbecome a recoverable schedule.  \n \nNote: A committed transaction should never be rollback. It means that reading \nvalue from uncommitted transaction and commit it will enter the current \ntransaction into inconsistent or unrecoverable state this is called Dirty Read \nproblem.  \n \nCascadeless Schedule \nWhen no read or write-write occurs before the execution of the transaction \nthen the corresponding schedule is called a cascadeless schedule.  \nExample: \nS3: R1(x), \n R2(z), R3(x), R1(z), R2(y), R3(y), W1(x), C1,  \n         W2(z), W3(y), W2(y), C3, C2; \n  \nIn this schedule W3(y) and W2(y) overwrite conflicts and there is no read, \ntherefore given schedule is cascade less schedule.  \n \nSpecial Case: A committed transaction desired to abort. As given below all the \ntransactions are reading committed data hence it\u2019s cascadeless schedule.  \n \nStrict Schedule \nIf the schedule contains no read or write before commit, then it is known as a \nstrict schedule. A strict schedule is strict in nature.  \nExample: \nS4: R1(x), R2(x), R1(z), R3(x), R3(y),  \n        W1(x), C1, W3(y), C3, R2(y), W2(z), W2(y), C2;  \nIn this schedule, no read-write or write-write conflict arises before committing \nhence its strict schedule:  \n \n \n \nCascading Abort:  \nCascading Abort can also be rollback. If transaction T1 aborts as T2 read data \nthat is written by T1 it is not committed. Hence its cascading rollback.  \n \nCo-Relation between Strict, Cascadeless, and Recoverable schedules \nBelow is the picture showing the correlation between Strict Schedules, \nCascadeless Schedules, and Recoverable Schedules. \nSo, we can conclude that: \n\uf0b7 Strict schedules are all recoverable and cascade schedules. \n\uf0b7 All cascade-less schedules are recoverable. \n \n14.3. Conflict Serializability \nin Concurrency control, serial schedules have less resource utilization and low \nthroughput. To improve it, two or more transactions are run concurrently. \nHowever, concurrency of transactions may lead to inconsistency in the database. \nTo avoid this, we need to check whether these concurrent schedules are \nserializable or not. \nConflict Serializable \nConcurrency serializability, also known as conflict serializability, is a type of \nconcurrency control that guarantees that the outcome of concurrent transactions \nis the same as if the transactions were executed consecutively. \nConflict serializable schedules: A schedule is called conflict serializable if it \ncan be transformed into a serial schedule by swapping non-conflicting \noperations. \nNon-conflicting operations: When two operations operate on separate data \nitems or the same data item but at least one of them is a read operation, they are \nsaid to be non-conflicting. \n \nConflicting Operations \nTwo operations are said to be conflicting if all conditions are satisfied:  \n\uf0b7 They belong to different transactions \n\uf0b7 They operate on the same data item \n\uf0b7 At Least one of them is a write operation \n \nExample: \nConflicting operations pair (R1(A), W2(A)) because they belong to two \ndifferent transactions on the same data item A and one of them is a write \noperation. \nSimilarly, (W1(A), W2(A)) and (W1(A), R2(A)) pairs are also conflicting. \nOn the other hand, the (R1(A), W2(B)) pair is non-conflicting because they \noperate on different data items. \nSimilarly, ((W1(A), W2(B)) pair is non-conflicting. \n \n \nConsider the following schedule:  \nS1: R1(A), W1(A), R2(A), W2(A), R1(B), W1(B), R2(B), W2(B) \nIf Oi and Oj are two operations in a transaction and Oi< Oj (Oi is executed \nbefore Oj), same order will follow in the schedule as well. Using this property, \nwe can get two transactions of schedule S1:  \nT1: R1(A), W1(A), R1(B), W1(B) \nT2: R2(A), W2(A), R2(B), W2(B) \n \nPossible Serial Schedules are: T1->T2 or T2->T1 \n-> Swapping non-conflicting operations R2(A) and R1(B) in S1, the schedule \nbecomes,  \nS11: R1(A), W1(A), R1(B), W2(A), R2(A), W1(B), R2(B), W2(B) \n-> Similarly, swapping non-conflicting operations W2(A) and W1(B) in S11, \nthe schedule becomes,  \nS12: R1(A), W1(A), R1(B), W1(B), R2(A), W2(A), R2(B), W2(B) \nS12 is a serial schedule in which all operations of T1 are performed before \nstarting any operation of T2. Since S has been transformed into a serial schedule \nS12 by swapping non-conflicting operations of S1, S1 is conflict serializable. \nLet us take another Schedule:  \nS2: R2(A), W2(A), R1(A), W1(A), R1(B), W1(B), R2(B), W2(B) \nTwo transactions will be:   \nT1: R1(A), W1(A), R1(B), W1(B) \nT2: R2(A), W2(A), R2(B), W2(B) \n \nPossible Serial Schedules are: T1->T2 or T2->T1  \nOriginal Schedule is as:   \nS2: R2(A), W2(A), R1(A), W1(A), R1(B), W1(B), R2(B), W2(B) \nSwapping non-conflicting operations R1(A) and R2(B) in S2, the schedule \nbecomes,  \nS21: R2(A), W2(A), R2(B), W1(A), R1(B), W1(B), R1(A), W2(B) \nSimilarly, swapping non-conflicting operations W1(A) and W2(B) in S21, the \nschedule becomes,   \nS22: R2(A), W2(A), R2(B), W2(B), R1(B), W1(B), R1(A), W1(A) \nIn schedule S22, all operations of T2 are performed first, but operations of T1 \nare not in order (order should be R1(A), W1(A), R1(B), W1(B)). So S2 is not \nconflict serializable. \n \nConflict Equivalent \nTwo schedules are said to be conflict equivalent when one can be transformed \nto another by swapping non-conflicting operations. In the example discussed \nabove, S11 is conflict equivalent to S1 (S1 can be converted to S11 by swapping \nnon-conflicting operations). Similarly, S11 is conflict equivalent to S12, and so \non. \n \n \nNote 1: Although S2 is not conflict serializable, still it is conflict equivalent to \nS21 and S21 because S2 can be converted to S21 and S22 by swapping non-\nconflicting operations. \nNote 2: The schedule which is conflict serializable is always conflict equivalent \nto one of the serial schedules. S1 schedule discussed above (which is conflict \nserializable) is equivalent to the serial schedule (T1->T2). \n \n14.4. Precedence Graph for Testing Conflict Serializability \nA Precedence Graph or Serialization Graph is used commonly to test the \nConflict Serializability of a schedule. It is a directed Graph (V, E) consisting of \na set of nodes V = {T1, T2, T3\u2026\u2026\u2026. Tn} and a set of directed edges E = {e1, \ne2, e3\u2026\u2026\u2026\u2026\u2026\u2026em}. The graph contains one node for each Transaction Ti. \nAn edge ei is of the form Tj \u2013> Tk where Tj is the starting node of ei and Tk is \nthe ending node of ei. An edge ei is constructed between nodes Tj to Tk if one \nof the operations in Tj appears in the schedule before some conflicting operation \nin Tk. The Algorithm can be written as: \n\uf0b7 Create a node T in the graph for each participating transaction in the \nschedule. \n\uf0b7 For the conflicting operation read_item(X) and write_item(X) \u2013 If a \nTransaction Tj executes a read_item (X) after Ti executes a write_item \n(X), draw an edge from Ti to Tj in the graph. \n\uf0b7 For the conflicting operation write_item(X) and read_item(X) \u2013 If a \nTransaction Tj executes a write_item (X) after Ti executes a read_item \n(X), draw an edge from Ti to Tj in the graph. \n\uf0b7 For the conflicting operation write_item(X) and write_item(X) \u2013 If a \nTransaction Tj executes a write_item (X) after Ti executes a \nwrite_item (X), draw an edge from Ti to Tj in the graph. \n\uf0b7 Schedule S is serializable if there is no cycle in the precedence graph. \n \nIf there is no cycle in the precedence graph, it means we can construct a serial \nschedule S\u2019 which is conflict equivalent to schedule S. The serial schedule S\u2019 \ncan be found by Topological Sorting of the acyclic precedence graph. Such \nschedules can be more than 1. For example, Consider the schedule S: \n S: r1(x) r1(y) w2(x) w1(x) r2(y) \n \nWhat are the Steps to Construct a Precedence Graph? \nStep 1: Draw a node for each transaction in the schedule. \n \nStep 2: For each pair of conflicting operations (i.e., operations on the same data \nitem by different transactions), draw an edge from the transaction that \nperformed the first operation to the transaction that performed the second \noperation. The edge represents a dependency between the two transactions. \n \n \nStep 3: If there are multiple conflicting operations between two transactions, \ndraw multiple edges between the corresponding nodes. \n \nStep 4: If there are no conflicting operations between two transactions, do not \ndraw an edge between them. \n \nStep 5: Once all the edges have been added to the graph, check if the graph \ncontains any cycles. If the graph contains cycles, then the schedule is not conflict \nserializable. Otherwise, the schedule is conflict serializable. \n \nThe precedence graph provides a visual representation of the dependencies \nbetween transactions in a schedule and allows us to determine whether the \nschedule is a conflict serializable or not. By constructing the precedence graph, \nwe can identify the transactions that have conflicts and reorder them to produce \na conflict serializable schedule, which is a schedule that can be transformed into \na serial schedule by swapping non-conflicting operations. \n \nAdvantages of Precedence Graphs for Testing Conflict Serializability \n\uf0b7 Simple to comprehend: Because precedence graphs show the \nconnections between transactions visually, they are simple to \ncomprehend. \n\uf0b7 Quick analysis: You can rapidly ascertain whether or not a series of \ntransactions can be conflict serialized by using precedence graphs. \n\uf0b7 Finding anomalies: Anomalies like cycles or deadlocks that might \nnot be seen right away might be found using precedence graphs. \n\uf0b7 Assists with optimization: By identifying transactions that can be \ncarried out in parallel, precedence graphs can be utilized to enhance a \ndatabase system\u2019s performance. \n \nDisadvantages of Precedence Graphs for Testing Conflict Serializability \n\uf0b7 Complex for large systems: It can be challenging to discern \ndependencies between transactions in large database systems due to \nthe complexity of precedence graphs. \n\uf0b7 Potential for inaccurate results: It is possible that some conflicts \nbetween transactions will be unnoticed by precedence graphs. \n\uf0b7 Require Manual efforts: Building precedence graphs by hand can be \nlabour-intensive and time-consuming, particularly in the case of big \nsystems. \n\uf0b7 Limited applicability: Data races and deadlocks cannot be detected \nwith precedence graphs; they are only useful for assessing conflict \nserializability. \n \n \n \n \n \n14.5. Recoverability  \nRecoverability is a property of database systems that ensures that, in the event \nof a failure or error, the system can recover the database to a consistent state. \nRecoverability guarantees that all committed transactions are durable and that \ntheir effects are permanently stored in the database, while the effects of \nuncommitted transactions are undone to maintain data consistency. \nThe recoverability property is enforced through the use of transaction logs, \nwhich record all changes made to the database during transaction processing. \nWhen a failure occurs, the system uses the log to recover the database to a \nconsistent state, which involves either undoing the effects of uncommitted \ntransactions or redoing the effects of committed transactions. \nThere are several levels of recoverability that can be supported by a database \nsystem: \n\uf0b7 No-undo logging: This level of recoverability only guarantees that \ncommitted transactions are durable, but does not provide the ability to \nundo the effects of uncommitted transactions. \n\uf0b7 Undo logging: This level of recoverability provides the ability to undo \nthe effects of uncommitted transactions but may result in the loss of \nupdates made by committed transactions that occur after the failed \ntransaction. \n\uf0b7 Redo logging: This level of recoverability provides the ability to redo the \neffects of committed transactions, ensuring that all committed updates are \ndurable and can be recovered in the event of failure. \n\uf0b7 Undo-redo logging: This level of recoverability provides both undo and \nredo capabilities, ensuring that the system can recover to a consistent state \nregardless of whether a transaction has been committed or not. \nIn addition to these levels of recoverability, database systems may also use \ntechniques such as checkpointing and shadow paging to improve recovery \nperformance and reduce the overhead associated with logging. \nRecoverable Schedules: \nSchedules in which transactions commit only after all transactions whose \nchanges they read commit are called recoverable schedules. In other words, if \nsome transaction Tj is reading value updated or written by some other \ntransaction Ti, then the commit of Tj must occur after the commit of Ti.  \nExample 1:    S1: R1(x), W1(x), R2(x), R1(y), R2(y),  \n         W2(x), W1(y), C1, C2;  \nGiven schedule follows order of Ti->Tj => C1->C2. Transaction T1 is executed \nbefore T2 hence there is no chances of conflict occur. R1(x) appears before \nW1(x) and transaction T1 is committed before T2 i.e. completion of first \ntransaction performed first update on data item x, hence given schedule is \nrecoverable.  \n \n \n14.6. Cascadeless in DBMS \n Generally, there are 3 types of schedule based on recoverbility given as follows: \n\uf0b7 Recoverable schedule: \nTransactions must be committed in order. Dirty Read problem and \nLost Update problem may occur. \n\uf0b7 Cascadeless Schedule: \nDirty Read not allowed, means reading the data written by an \nuncommitted transaction is not allowed. Lost Update problem may \noccur. \n\uf0b7 Strict schedule: \nNeither Dirty read nor Lost Update problem allowed, means reading \nor writing the data written by an uncommitted transaction is not \nallowed. \n \nCascading Rollback: \nIf in a schedule, failure of one transaction causes several other dependent \ntransactions to rollback or abort, then such a schedule is called as a Cascading \nRollback or Cascading Abort or Cascading Schedule. It simply leads to the \nwastage of CPU time. \n \nThese Cascading Rollbacks occur because of Dirty Read problems. \nFor example, transaction T1 writes uncommitted x that is read by Transaction \nT2. Transaction T2 writes uncommitted x that is read by Transaction T3. \nSuppose at this point T1 fails. \nT1 must be rolled back, since T2 is dependent on T1, T2 must be rolled back, \nand since T3 is dependent on T2, T3 must be rolled back. \nBecause of T1 rollback, all T2, T3, and T4 should also be rollback (Cascading \ndirty read problem). \nThis phenomenon, in which a single transaction failure leads to a series of \ntransaction rollbacks is called Cascading rollback. \n \n \n \n \n \n \nCascadeless Schedule: \nThis schedule avoids all possible Dirty Read Problem. \nIn Cascadeless Schedule, if a transaction is going to perform read operation on a \nvalue, it has to wait until the transaction who is performing write on that value \ncommits. That means there must not be Dirty Read. Because Dirty Read Problem \ncan cause Cascading Rollback, which is inefficient. \nCascadeless Schedule avoids cascading aborts/rollbacks (ACA). Schedules in \nwhich transactions read values only after all transactions whose changes they are \ngoing to read commit are called cascadeless schedules. Avoids that a single \ntransaction abort leads to a series of transaction rollbacks. A strategy to prevent \ncascading aborts is to disallow a transaction from reading uncommitted changes \nfrom another transaction in the same schedule. \nIn other words, if some transaction Tj wants to read value updated or written by \nsome other transaction Ti, then the commit of Tj must read it after the commit of \nTi. \n \n \n                                  Figure 36: No Sirty Read Problem \n \nNote: Cascadeless schedule allows only committed read operations. However, it \nallows uncommitted write operations. \n \n \n \n \n \n14.7. Concurrency Control \nConcurrently control is a very important concept of DBMS which ensures the \nsimultaneous execution or manipulation of data by several processes or user \nwithout resulting in data inconsistency. Concurrency Control deals \nwith interleaved execution of more than one transaction. \n \nWhat is Transaction?  \nA transaction is a collection of operations that performs a single logical function \nin a database application. Each transaction is a unit of both atomicity and \nconsistency. Thus, we require that transactions do not violate any database \nconsistency constraints. That is, if the database was consistent when a \ntransaction started, the database must be consistent when the transaction \nsuccessfully terminates. However, during the execution of a transaction, it may \nbe necessary temporarily to allow inconsistency, since either the debit of A or \nthe credit of B must be done before the other. This temporary inconsistency, \nalthough necessary, may lead to difficulty if a failure occurs. \nIt is the programmer\u2019s responsibility to define properly the various transactions, \nso that each preserves the consistency of the database. For example, the \ntransaction to transfer funds from the account of department A to the account of \ndepartment B could be defined to be composed of two separate programs: one \nthat debits account A, and another that credits account B. The execution of these \ntwo programs one after the other will indeed preserve consistency. However, \neach program by itself does not transform the database from a consistent state \nto a new consistent state. Thus, those programs are not transactions. \nThe concept of a transaction has been applied broadly in database systems and \napplications. While the initial use of transactions was in financial applications, \nthe concept is now used in real-time applications in telecommunication, as well \nas in the management of long-duration activities such as product design or \nadministrative workflows. \nA set of logically related operations is known as a transaction. The main \noperations of a transaction are: \n\uf0b7 Read(A): Read operations Read(A) or R(A) reads the value of A from \nthe database and stores it in a buffer in the main memory. \n\uf0b7 Write (A): Write operation Write(A) or W(A) writes the value back to \nthe database from the buffer.  \n(Note: It doesn\u2019t always need to write it to a database back it just writes the \nchanges to buffer this is the reason where dirty read comes into the picture)  \nLet us take a debit transaction from an account that consists of the following \noperations: \n\uf0b7 R(A); \n\uf0b7 A=A-1000; \n\uf0b7 W(A); \n \n \nAssume A\u2019s value before starting the transaction is 5000. \n\uf0b7 The first operation reads the value of A from the database and stores it \nin a buffer. \n\uf0b7 the Second operation will decrease its value by 1000. So buffer will \ncontain 4000. \n\uf0b7 the Third operation will write the value from the buffer to the database. \nSo A\u2019s final value will be 4000. \nBut it may also be possible that the transaction may fail after executing some of \nits operations. The failure can be because of hardware, software or power, etc. \nFor example, if the debit transaction discussed above fails after executing \noperation 2, the value of A will remain 5000 in the database which is not \nacceptable by the bank. To avoid this, Database has two important operations:  \n\uf0b7 Commit: After all instructions of a transaction are successfully \nexecuted, the changes made by a transaction are made permanent in \nthe database. \n\uf0b7 Rollback: If a transaction is not able to execute all operations \nsuccessfully, all the changes made by a transaction are undone. \n \nProperties of a Transaction \n\uf0b7 Atomicity: As a transaction is a set of logically related operations, either \nall of them should be executed or none. A debit transaction discussed \nabove should either execute all three operations or none. If the debit \ntransaction fails after executing operations 1 and 2 then its new value of \n4000 will not be updated in the database which leads to inconsistency. \n\uf0b7 Consistency: If operations of debit and credit transactions on the same \naccount are executed concurrently, it may leave the database in an \ninconsistent state. \n \nIsolation: The result of a transaction should not be visible to others before the \ntransaction is committed. For example, let us assume that A\u2019s balance is Rs. \n5000 and T1 debits Rs. 1000 from A. A\u2019s new balance will be 4000. If T2 credits \nRs. 500 to A\u2019s new balance, A will become 4500, and after this T1 fails. Then \nwe have to roll back T2 as well because it is using the value produced by T1. So \ntransaction results are not made visible to other transactions before it commits. \n \nDurable: Once the database has committed a transaction, the changes made by \nthe transaction should be permanent. e.g.; If a person has credited $500000 to \nhis account, the bank can\u2019t say that the update has been lost. To avoid this \nproblem, multiple copies of the database are stored at different locations. \n \nWhat is a Schedule?  \nA schedule is a series of operations from one or more transactions.  \nA schedule can be of two types:  \n \n \n1) Serial Schedule: When one transaction completely executes before \nstarting another transaction, the schedule is called a serial schedule. A \nserial schedule is always consistent. e.g.; If a schedule S has debit \ntransaction T1 and credit transaction T2, possible serial schedules are T1 \nfollowed by T2 (T1->T2) or T2 followed by T1 ((T2->T1). A serial \nschedule has low throughput and less resource utilization. \n2) Concurrent Schedule: When operations of a transaction are interleaved \nwith operations of other transactions of a schedule, the schedule is called \na Concurrent schedule. e.g.; the Schedule of debit and credit transactions \nshown in Table 1 is concurrent. But concurrency can lead to inconsistency \nin the database.  The above example of a concurrent schedule is also \ninconsistent. \n \nDifference between Serial Schedule and Serializable Schedule \n          Serial Schedule \n           Serializable Schedule \nIn Serial schedule, transactions will \nbe executed one after other. \nIn Serializable schedule transaction \nare executed concurrently. \nSerial schedule are less efficient. \nSerializable \nschedule \nare \nmore \nefficient. \nIn serial schedule only one transaction \nexecuted at a time. \nIn Serializable schedule multiple \ntransactions can be executed at a time. \nSerial schedule takes more time for \nexecution. \nIn Serializable schedule execution is \nfast.  \n \nConcurrency Control in DBMS \n\uf0b7 Executing a single transaction at a time will increase the waiting time \nof the other transactions which may result in delay in the overall \nexecution. Hence for increasing the overall throughput and efficiency \nof the system, several transactions are executed. \n\uf0b7 Concurrently control is a very important concept of DBMS which \nensures the simultaneous execution or manipulation of data by several \nprocesses or user without resulting in data inconsistency. \n\uf0b7 Concurrency control provides a procedure that is able to control \nconcurrent execution of the operations in the database.  \n\uf0b7 The fundamental goal of database concurrency control is to ensure that \nconcurrent execution of transactions does not result in a loss of \ndatabase consistency. The concept of serializability can be used to \nachieve this goal, since all serializable schedules preserve consistency \nof the database. However, not all schedules that preserve consistency \nof the database are serializable. \nIn general it is not possible to perform an automatic analysis of low-level \noperations by transactions and check their effect on database consistency \nconstraints. However, there are simpler techniques. One is to use the database \n \n \nconsistency constraints as the basis for a split of the database into sub databases \non which concurrency can be managed separately. \nAnother is to treat some operations besides read and write as fundamental low-\nlevel operations and to extend concurrency control to deal with them. \n \nConcurrency Control Problems \nThere are several problems that arise when numerous transactions are executed \nsimultaneously in a random manner. The database transaction consist of two \nmajor operations \u201cRead\u201d and \u201cWrite\u201d. It is very important to manage these \noperations in the concurrent execution of the transactions in order to maintain \nthe consistency of the data.  \nDirty Read Problem(Write-Read conflict) \nDirty read problem occurs when one transaction updates an item but due to some \nunconditional events that transaction fails but before the transaction performs \nrollback, some other transaction reads the updated value. Thus creates an \ninconsistency in the database. Dirty read problem comes under the scenario of  \n \nWrite-Read conflict between the transactions in the database. \n\uf0b7 The lost update problem can be illustrated with the below scenario \nbetween two transactions T1 and T2. \n\uf0b7 Transaction T1 modifies a database record without committing the \nchanges. \n\uf0b7 T2 reads the uncommitted data changed by T1. \n\uf0b7 T1 performs rollback. \n\uf0b7 T2 has already read the uncommitted data of T1 which is no longer valid, \nthus creating inconsistency in the database. \n \nLost Update Problem \nLost update problem occurs when two or more transactions modify the same \ndata, resulting in the update being overwritten or lost by another transaction. \nThe lost update problem can be illustrated with the below scenario between two \ntransactions T1 and T2. \n\uf0b7 T1 reads the value of an item from the database. \n\uf0b7 T2 starts and reads the same database item. \n\uf0b7 T1 updates the value of that data and performs a commit. \n\uf0b7 T2 updates the same data item based on its initial read and performs \ncommit. \n\uf0b7 This results in the modification of T1 gets lost by the T2\u2019s writes which \ncauses a lost update problem in the database. \n \nConcurrency Control Protocols \nConcurrency control protocols are the set of rules which are maintained in order \nto solve the concurrency control problems in the database. It ensures that the \n \n \nconcurrent transactions can execute properly while maintaining the database \nconsistency. The concurrent execution of a transaction is provided with \natomicity, consistency, isolation, durability, and serializability via the \nconcurrency control protocols. \n\uf0b7 Locked based concurrency control protocol \n\uf0b7 Timestamp based concurrency control protocol \n \nLocked based Protocol \nIn locked based protocol, each transaction needs to acquire locks before they \nstart accessing or modifying the data items. There are two types of locks used \nin databases. \n\uf0b7 Shared Lock : Shared lock is also known as read lock which allows \nmultiple transactions to read the data simultaneously. The transaction \nwhich is holding a shared lock can only read the data item but it can \nnot modify the data item. \n\uf0b7 Exclusive Lock : Exclusive lock is also known as the write lock. \nExclusive lock allows a transaction to update a data item. Only one \ntransaction can hold the exclusive lock on a data item at a time. While \na transaction is holding an exclusive lock on a data item, no other \ntransaction is allowed to acquire a shared/exclusive lock on the same \ndata item. \n \nThere are two kinds of lock-based protocol mostly used in database: \n\uf0b7 Two Phase Locking Protocol:  Two phase locking is a widely used \ntechnique which ensures strict ordering of lock acquisition and release. \nTwo phase locking protocol works in two phases. \n\uf0b7 Growing Phase: In this phase, the transaction starts \nacquiring locks before performing any modification on the \ndata items. Once a transaction acquires a lock, that lock \ncannot be released until the transaction reaches the end of the \nexecution. \n\uf0b7 Shrinking Phase: In this phase, the transaction releases all \nthe acquired locks once it performs all the modifications on \nthe data item. Once the transaction starts releasing the locks, \nit cannot acquire any locks further.  \n\uf0b7 Strict Two Phase Locking Protocol : It is almost similar to the two \nphase locking protocol the only difference is that in two phase locking \nthe transaction can release its locks before it commits, but in case of \nstrict two phase locking the transactions are only allowed to release \nthe locks only when they performs commits.  \n \n \n \nTimestamp based Protocol. \nIn this protocol each transaction has a timestamp attached to it. Timestamp is \nnothing but the time in which a transaction enters the system. \nThe conflicting pairs of operations can be resolved by the timestamp ordering \nprotocol through the utilization of the timestamp values of the transactions. \nTherefore, guaranteeing that the transactions take place in the correct order. \n \nAdvantages of Concurrency \nIn general, concurrency means, that more than one transaction can work on a \nsystem. The advantages of a concurrent system are: \n\uf0b7 Waiting Time: It means if a process is in a ready state but still the \nprocess does not get the system to get execute is called waiting time. \nSo, concurrency leads to less waiting time. \n\uf0b7 Response Time: The time wasted in getting the response from the \nCPU for the first time, is called response time. So, concurrency leads \nto less Response Time. \n\uf0b7 Resource Utilization: The amount of Resource utilization in a \nparticular system is called Resource Utilization. Multiple transactions \ncan run parallel in a system. So, concurrency leads to more Resource \nUtilization. \n\uf0b7 Efficiency: The amount of output produced in comparison to given \ninput is called efficiency. So, Concurrency leads to more Efficiency. \n \nDisadvantages of Concurrency  \n\uf0b7 Overhead: Implementing concurrency control requires additional \noverhead, such as acquiring and releasing locks on database objects. \nThis overhead can lead to slower performance and increased resource \nconsumption, particularly in systems with high levels of concurrency. \n\uf0b7 Deadlocks: Deadlocks can occur when two or more transactions are \nwaiting for each other to release resources, causing a circular \ndependency that can prevent any of the transactions from completing. \nDeadlocks can be difficult to detect and resolve and can result in \nreduced throughput and increased latency. \n\uf0b7 Reduced concurrency: Concurrency control can limit the number of \nusers or applications that can access the database simultaneously. This \ncan lead to reduced concurrency and slower performance in systems \nwith high levels of concurrency. \n\uf0b7 Complexity: Implementing concurrency control can be complex, \nparticularly in distributed systems or in systems with complex \ntransactional logic. This complexity can lead to increased \ndevelopment and maintenance costs. \n \n \n \n14.8. Concurrency Control Techniques \nConcurrency control is provided in a database to: \n\uf0b7 enforce isolation among transactions. \n\uf0b7 preserve database consistency through consistency preserving \nexecution of transactions. \n\uf0b7 resolve read-write and write-read conflicts. \n \nVarious concurrency control techniques are: \ni) \nTwo-phase locking Protocol \nii) Time stamp ordering Protocol \niii) Multi version concurrency control \niv) Validation concurrency control  \n \nThese are briefly explained below.  \n \ni) \nTwo-Phase Locking Protocol:  \nLocking is an operation which secures permission to read, OR permission to \nwrite a data item. Two phase locking is a process used to gain ownership of \nshared resources without creating the possibility of deadlock. The 3 activities \ntaking place in the two-phase update algorithm are: \n\uf0b7 Lock Acquisition \n\uf0b7 Modification of Data \n\uf0b7 Release Lock  \nTwo phase locking prevents deadlock from occurring in distributed systems by \nreleasing all the resources it has acquired, if it is not possible to acquire all the \nresources required without waiting for another process to finish using a lock. \nThis means that no process is ever in a state where it is holding some shared \nresources, and waiting for another process to release a shared resource which it \nrequires. This means that deadlock cannot occur due to resource contention. A \ntransaction in the Two-Phase Locking Protocol can assume one of the 2 phases: \n1st Phase: Growing Phase: In this phase a transaction can only acquire locks \nbut cannot release any lock. The point when a transaction acquires all the locks \nit needs is called the Lock Point. \n \n2nd Phase: Shrinking Phase: In this phase a transaction can only release locks \nbut cannot acquire any. \n \n \niv) \nTime Stamp Ordering Protocol:  \nA timestamp is a tag that can be attached to any transaction or any data item, \nwhich denotes a specific time on which the transaction or the data item had been \nused in any way. A timestamp can be implemented in 2 ways. One is to directly \n \n \nassign the current value of the clock to the transaction or data item. The other is \nto attach the value of a logical counter that keeps increment as new timestamps \nare required. \n \n The timestamp of a data item can be of 2 types: \n\uf0b7 W-timestamp(X): This means the latest time when the data item X \nhas been written into. \n\uf0b7 R-timestamp(X): This means the latest time when the data item X has \nbeen read from. These 2 timestamps are updated each time a successful \nread/write operation is performed on the data item X. \n \n \niii) \nMult version Concurrency Control:  \nMult version schemes keep old versions of data item to increase concurrency.  \nMult version 2 phase locking: Each successful write results in the creation of \na new version of the data item written. Timestamps are used to label the \nversions. When a read(X) operation is issued, select an appropriate version of X \nbased on the timestamp of the transaction.  \n \n \niv) \n Validation Concurrency Control:  \nThe optimistic approach is based on the assumption that the majority of the \ndatabase operations do not conflict. The optimistic approach requires neither \nlocking nor time stamping techniques. Instead, a transaction is executed without \nrestrictions until it is committed. Using an optimistic approach, each transaction \nmoves through 2 or 3 phases, referred to as read, validation and write. \n\uf0b7 During read phase, the transaction reads the database, executes the \nneeded computations and makes the updates to a private copy of  the \ndatabase values. All update operations of the transactions are recorded \nin a temporary update file, which is not accessed by the remaining \ntransactions. \n\uf0b7 During the validation phase, the transaction is validated to ensure that \nthe changes made will not affect the integrity and consistency of the \ndatabase. If the validation test is positive, the transaction goes to a \nwrite phase. If the validation test is negative, he transaction is restarted \nand the changes are discarded. \n\uf0b7 During the write phase, the changes are permanently applied to the \ndatabase. \n \n \n \n \n \n15. \nDeadlock and Starvation \nDEADLOCK \nIn a database management system (DBMS), a deadlock occurs when two or \nmore transactions are waiting for each other to release resources, such as locks \non database objects, that they need to complete their operations. As a result, \nnone of the transactions can proceed, leading to a situation where they are stuck \nor \u201cdeadlocked.\u201d \nDeadlocks can happen in multi-user environments when two or more \ntransactions are running concurrently and try to access the same data in a \ndifferent order. When this happens, one transaction may hold a lock on a \nresource that another transaction needs, while the second transaction may hold \na lock on a resource that the first transaction needs. Both transactions are then \nblocked, waiting for the other to release the resource they need. \nDBMSs often use various techniques to detect and resolve deadlocks \nautomatically. These techniques include timeout mechanisms, where a \ntransaction is forced to release its locks after a certain period of time, and \ndeadlock detection algorithms, which periodically scan the transaction log for \ndeadlock cycles and then choose a transaction to abort to resolve the deadlock. \nIt is also possible to prevent deadlocks by careful design of transactions, such \nas always acquiring locks in the same order or releasing locks as soon as \npossible. Proper design of the database schema and application can also help to \nminimize the likelihood of deadlocks. \nIn a database, a deadlock is an unwanted situation in which two or more \ntransactions are waiting indefinitely for one another to give up locks. Deadlock \nis said to be one of the most feared complications in DBMS as it brings the \nwhole system to a Halt.  \n \nExample \u2013 let us understand the concept of Deadlock with an example:  \nSuppose, Transaction T1 holds a lock on some rows in the students table \nand needs to update some rows in the Grades table. Simultaneously, \nTransaction T2 holds locks on those very rows (Which T1 needs to update) in \nthe Grades table but needs to update the rows in the student table held by \nTransaction T1.  \nNow, the main problem arises. Transaction T1 will wait for transaction T2 to \ngive up the lock, and similarly, transaction T2 will wait for transaction T1 to \ngive up the lock. As a consequence, All activity comes to a halt and remains at \na standstill forever unless the DBMS detects the deadlock and aborts one of the \ntransactions.  \n  \n \n \n                                    Figure 36: Deadlock in DBMS \nDeadlock Avoidance: When a database is stuck in a deadlock, It is always \nbetter to avoid the deadlock rather than restarting or aborting the database. The \ndeadlock avoidance method is suitable for smaller databases whereas the \ndeadlock prevention method is suitable for larger databases.  \nOne method of avoiding deadlock is using application-consistent logic. In the \nabove-given example, Transactions that access Students and Grades should \nalways access the tables in the same order. In this way, in the scenario \ndescribed above, Transaction T1 simply waits for transaction T2 to release the \nlock on Grades before it begins. When transaction T2 releases the lock, \nTransaction T1 can proceed freely.  \nAnother method for avoiding deadlock is to apply both the row-level locking \nmechanism and the READ COMMITTED isolation level. However, It does \nnot guarantee to remove deadlocks completely.  \nDeadlock Detection: When a transaction waits indefinitely to obtain a lock, \nThe database management system should detect whether the transaction is \ninvolved in a deadlock or not.  \nWait-for-graph is one of the methods for detecting the deadlock situation. \nThis method is suitable for smaller databases. In this method, a graph is drawn \nbased on the transaction and its lock on the resource. If the graph created has a \nclosed loop or a cycle, then there is a deadlock.  \nFor the above-mentioned scenario, the Wait-For graph is drawn below: \n                      Figure 37: Deadlock Situation \n \n \nDeadlock prevention: For a large database, the deadlock prevention method is \nsuitable. A deadlock can be prevented if the resources are allocated in such a \nway that a deadlock never occurs. The DBMS analyses the operations whether \nthey can create a deadlock situation or not, If they do, that transaction is never \nallowed to be executed.  \nDeadlock prevention mechanism proposes two schemes:  \n\uf0b7 Wait-Die Scheme:  \nIn this scheme, If a transaction requests a resource that is locked by another \ntransaction, then the DBMS simply checks the timestamp of both transactions \nand allows the older transaction to wait until the resource is available for \nexecution.  \nSuppose, there are two transactions T1 and T2, and Let the timestamp of any \ntransaction T be TS (T). Now, If there is a lock on T2 by some other transaction \nand T1 is requesting resources held by T2, then DBMS performs the following \nactions:  \nChecks if TS (T1) < TS (T2) \u2013 if T1 is the older transaction and T2 has held \nsome resource, then it allows T1 to wait until resource is available for \nexecution. That means if a younger transaction has locked some resource and \nan older transaction is waiting for it, then an older transaction is allowed to \nwait for it till it is available. If T1 is an older transaction and has held some \nresource with it and if T2 is waiting for it, then T2 is killed and restarted later \nwith random delay but with the same timestamp. i.e. if the older transaction \nhas held some resource and the younger transaction waits for the resource, \nthen the younger transaction is killed and restarted with a very minute delay \nwith the same timestamp.  \n\uf0b7 Wound Wait Scheme:  \nIn this scheme, if an older transaction requests for a resource held by a younger \ntransaction, then an older transaction forces a younger transaction to kill the \ntransaction and release the resource. The younger transaction is restarted with a \nminute delay but with the same timestamp. If the younger transaction is \nrequesting a resource that is held by an older one, then the younger transaction \nis asked to wait till the older one releases it. \nThe following table lists the differences between Wait \u2013 Die and Wound -Wait \nscheme prevention schemes: \nWait \u2013 Die  \nWound -Wait \nIt is based on a non-pre-emptive technique. \nIt is based on a pre-emptive technique. \nIn this, older transactions must wait for the \nyounger one to release its data items. \nIn this, older transactions never wait for \nyounger transactions.  \nThe number of aborts and rollbacks is higher \nin these techniques. \nIn this, the number of aborts and rollback is \nlesser. \n \n \n \nApplications: \n\uf0b7 Delayed Transactions: Deadlocks can cause transactions to be delayed, \nas the resources they need are being held by other transactions. This can \nlead to slower response times and longer wait times for users. \n\uf0b7 Lost Transactions: In some cases, deadlocks can cause transactions to \nbe lost or aborted, which can result in data inconsistencies or other issues. \n\uf0b7 Reduced Concurrency: Deadlocks can reduce the level of concurrency \nin the system, as transactions are blocked waiting for resources to become \navailable. This can lead to slower transaction processing and reduced \noverall throughput. \n\uf0b7 Increased Resource Usage: Deadlocks can result in increased resource \nusage, as transactions that are blocked waiting for resources to become \navailable continue to consume system resources. This can lead to \nperformance degradation and increased resource contention. \n\uf0b7 Reduced User Satisfaction: Deadlocks can lead to a perception of poor \nsystem performance and can reduce user satisfaction with the application. \nThis can have a negative impact on user adoption and retention. \nFeatures of deadlock in a DBMS: \n\uf0b7 Mutual Exclusion: Each resource can be held by only one transaction at \na time, and other transactions must wait for it to be released. \n\uf0b7 Hold and Wait: Transactions can request resources while holding on to \nresources already allocated to them. \n\uf0b7 No Preemption: Resources cannot be taken away from a transaction \nforcibly, and the transaction must release them voluntarily. \n\uf0b7 Circular Wait: Transactions are waiting for resources in a circular chain, \nwhere each transaction is waiting for a resource held by the next \ntransaction in the chain. \n\uf0b7 Indefinite Blocking: Transactions are blocked indefinitely, waiting for \nresources to become available, and no transaction can proceed. \n\uf0b7 System Stagnation: Deadlock leads to system stagnation, where no \ntransaction can proceed, and the system is unable to make any progress. \n\uf0b7 Inconsistent Data: Deadlock can lead to inconsistent data if transactions \nare unable to complete and leave the database in an intermediate state. \n\uf0b7 Difficult to Detect and Resolve: Deadlock can be difficult to detect and \nresolve, as it may involve multiple transactions, resources, and \ndependencies. \n \n \n \n \nDisadvantages: \n\uf0b7 System downtime: Deadlock can cause system downtime, which can \nresult in loss of productivity and revenue for businesses that rely on the \nDBMS. \n\uf0b7 Resource waste: When transactions are waiting for resources, these \nresources are not being used, leading to wasted resources and decreased \nsystem efficiency. \n\uf0b7 Reduced concurrency: Deadlock can lead to a decrease in system \nconcurrency, which can result in slower transaction processing and \nreduced throughput. \n\uf0b7 Complex resolution: Resolving deadlock can be a complex and time-\nconsuming process, requiring system administrators to intervene and \nmanually resolve the deadlock. \n\uf0b7 Increased system overhead: The mechanisms used to detect and resolve \ndeadlock, such as timeouts and rollbacks, can increase system overhead, \nleading to decreased performance. \n \nSTARVATION \nStarvation or Livelock is the situation when a transaction has to wait for an \nindefinite period of time to acquire a lock.  \nReasons for Starvation: \n\uf0b7 If the waiting scheme for locked items is unfair. ( priority queue ) \n\uf0b7 Victim selection (the same transaction is selected as a victim \nrepeatedly ) \n\uf0b7 Resource leak. \n\uf0b7 Via denial-of-service attack. \nStarvation can be best explained with the help of an example \u2013  \nSuppose there are 3 transactions namely T1, T2, and T3 in a database that is \ntrying to acquire a lock on data item \u2018 I \u2018. Now, suppose the scheduler grants the \nlock to T1(maybe due to some priority), and the other two transactions are \nwaiting for the lock. As soon as the execution of T1 is over, another transaction \nT4 also comes over and requests a lock on data item I. Now, this time the \nscheduler grants lock to T4, and T2, T3 has to wait again. In this way, if new \ntransactions keep on requesting the lock, T2 and T3 may have to wait for an \nindefinite period of time, which leads to Starvation.  \nSolutions to starvation: \n\uf0b7 Increasing Priority: Starvation occurs when a transaction has to wait for \nan indefinite time, In this situation, we can increase the priority of that \nparticular transaction/s. But the drawback with this solution is that it may \nhappen that the other transaction may have to wait longer until the highest \npriority transaction comes and proceeds. \n \n \n\uf0b7 Modification in Victim Selection algorithm: If a transaction has been a \nvictim of repeated selections, then the algorithm can be modified by \nlowering its priority over other transactions. \n\uf0b7 First Come First Serve approach: A fair scheduling approach i.e FCFS \ncan be adopted, In which the transaction can acquire a lock on an item in \nthe order, in which the requested lock. \n\uf0b7 Wait-die and wound wait scheme: These are the schemes that use the \ntimestamp ordering mechanism of transactions.  \n\uf0b7 Timeout Mechanism: A timeout mechanism can be implemented in \nwhich a transaction is only allowed to wait for a certain amount of time \nbefore it is aborted or restarted. This ensures that no transaction waits \nindefinitely, and prevents the possibility of starvation. \n\uf0b7 Resource Reservation: A resource reservation scheme can be used to \nallocate resources to a transaction before it starts execution. This ensures \nthat the transaction has access to the necessary resources and reduces the \nchances of waiting for a resource indefinitely. \n\uf0b7 Preemption: Preemption involves the forcible removal of a lock from a \ntransaction that has been waiting for a long time, in favor of another \ntransaction that has a higher priority or has been waiting for a shorter \ntime. Preemption ensures that no transaction waits indefinitely, and \nprevents the possibility of starvation. \n\uf0b7 Dynamic Lock Allocation: In this approach, locks are allocated \ndynamically based on the current state of the system. The system may \nanalyze the current lock requests and allocate locks in such a way that \nprevents deadlocks and reduces the chances of starvation. \n\uf0b7 Parallelism: By allowing multiple transactions to execute in parallel, the \nsystem can ensure that no transaction waits indefinitely, and reduces the \nchances of starvation. This approach requires careful consideration of the \npotential for conflicts and race conditions between transactions. \nIn a database management system (DBMS), starvation occurs when a \ntransaction or process is not able to get the resources it needs to proceed and is \ncontinuously delayed or blocked. This can happen when other transactions or \nprocesses are given priority over the one that is experiencing starvation. \nIn DBMSs, resources such as locks, memory, and CPU time are typically shared \namong multiple transactions or processes. If some transactions or processes are \ngiven priority over others, it is possible for one or more transactions or processes \nto experience starvation. \nFor example, if a transaction is waiting for a lock that is held by another \ntransaction, it may be blocked indefinitely if the other transaction never releases \nthe lock. This can lead to the first transaction experiencing starvation if it is \ncontinuously blocked and unable to proceed. \n \n \nDBMSs typically use various techniques to prevent or mitigate starvation, \nsuch as: \nResource allocation policies: DBMSs can use policies to allocate resources in a \nfair manner, ensuring that no transaction or process is consistently given priority \nover others. \nPriority-based scheduling: DBMSs can use scheduling algorithms that take into \naccount the priority of transactions or processes, ensuring that high-priority \ntransactions or processes are executed before low-priority ones. \nTimeout mechanisms: DBMSs can use timeout mechanisms to prevent \ntransactions or processes from being blocked indefinitely, by releasing \nresources if a transaction or process waits for too long. \nResource management: DBMSs can also use techniques such as resource quotas \nand limits to prevent any single transaction or process from monopolizing \nresources, thus reducing the likelihood of starvation. \nDisadvantages of Starvation: \n\uf0b7 Decreased performance: Starvation can cause decreased performance in \na DBMS by preventing transactions from making progress and causing a \nbottleneck. \n\uf0b7 Increased response time: Starvation can increase response time for \ntransactions that are waiting for resources, leading to poor user experience \nand decreased productivity. \n\uf0b7 Inconsistent data: If a transaction is unable to complete due to \nstarvation, it may leave the database in an inconsistent state, which can \nlead to data corruption and other problems. \n\uf0b7 Difficulty in troubleshooting: Starvation can be difficult to troubleshoot \nbecause it may not be immediately apparent which transaction is causing \nthe problem. \n\uf0b7 Potential for deadlock: If multiple transactions are competing for the \nsame resources, starvation can lead to deadlock, where none of the \ntransactions can proceed, causing a complete system failure. \n \n16. \nLock Based Protocol \nIn a database management system (DBMS), lock-based concurrency control \n(BCC) is used to control the access of multiple transactions to the same data \nitem. This protocol helps to maintain data consistency and integrity across \nmultiple users. In the protocol, transactions gain locks on data items to control \ntheir access and prevent conflicts between concurrent transactions. \nFirst things first, I hope you are familiar with some of the concepts relating to \nTransaction. \n \n \n\uf0b7 What is a Recoverable Schedule? \n\uf0b7 What are Cascading Rollbacks and Cascadeless schedules? \n\uf0b7 Determining if a schedule is Conflict Serializable. \nNow, we all know the four properties a transaction must follow. Yes, you got \nthat right, I mean the ACID properties. Concurrency control techniques are used \nto ensure that the Isolation (or non-interference) property of concurrently \nexecuting transactions is maintained. \nA trivial question I would like to pose in front of you, (I know you must know \nthis but still) why you think that we should have interleaving execution of \ntransactions if it may lead to problems such as Irrecoverable Schedule, \nInconsistency, and many more threats. Why not just let it be Serial schedules \nand we may live peacefully, with no complications at all? \nYes, the performance affects the efficiency too much which is not acceptable. \nHence a Database may provide a mechanism that ensures that the schedules are \neither conflict or view serializable and recoverable (also preferably \ncascadeless). Testing for a schedule for Serializability after it has been executed \nis too late! So, we need Concurrency Control Protocols that ensure \nSerializability. \n \nConcurrency Control Protocols \nallow concurrent schedules but ensure that the schedules are conflict/view \nserializable and are recoverable and maybe even cascadeless. These protocols \ndo not examine the precedence graph as it is being created, instead a protocol \nimposes a discipline that avoids non-serializable schedules. Different \nconcurrency control protocols provide different advantages between the amount \nof concurrency they allow and the amount of overhead that they impose.  \n \nNow, let\u2019s get going: Different categories of protocols: \n\uf0b7 Lock Based Protocol \n\uf0b7 Basic 2-PL \n\uf0b7 Conservative 2-PL \n\uf0b7 Strict 2-PL \n\uf0b7 Rigorous 2-PL \n\uf0b7 Graph Based Protocol \n\uf0b7 Time-Stamp Ordering Protocol \n\uf0b7 Multiple Granularity Protocol \n\uf0b7 Multi-version Protocol \n \nFor GATE we\u2019ll be focusing on the First three protocols. \n \n \n \n \n \n \nLock Based Protocols \nA lock is a variable associated with a data item that describes a status of data \nitem with respect to possible operation that can be applied to it. They \nsynchronize the access by concurrent transactions to the database items. It is \nrequired in this protocol that all the data items must be accessed in a mutually \nexclusive manner. Let me introduce you to two common locks which are used \nand some terminology followed in this protocol. \n\uf0b7 Shared Lock (S): also known as Read-only lock. As the name suggests it \ncan be shared between transactions because while holding this lock the \ntransaction does not have the permission to update data on the data item. \nS-lock is requested using lock-S instruction. \n\uf0b7 Exclusive Lock (X): Data item can be both read as well as written. This \nis Exclusive and cannot be held simultaneously on the same data item. X-\nlock is requested using lock-X instruction. \nLock Compatibility Matrix: \nA transaction may be granted a lock on an item if the requested lock is \ncompatible with locks already held on the item by other transactions. \nAny number of transactions can hold shared locks on an item, but if any \ntransaction holds an exclusive(X) on the item no other transaction may hold any \nlock on the item. \nIf a lock cannot be granted, the requesting transaction is made to wait till all \nincompatible locks held by other transactions have been released. Then the lock \nis granted. \n \nUpgrade / Downgrade locks \nA transaction that holds a lock on an item Ais allowed under certain condition \nto change the lock state from one state to another. Upgrade: A S(A) can be \nupgraded to X(A) if Ti is the only transaction holding the S-lock on element A. \nDowngrade: We may downgrade X(A) to S(A) when we feel that we no longer \nwant to write on data-item A. As we were holding X-lock on A, we need not \ncheck any conditions. \nSo, by now we are introduced with the types of locks and how to apply them. \nBut wait, just by applying locks if our problems could have been avoided then \nlife would have been so simple! If you have done Process Synchronization under \nOS you must be familiar with one consistent problem, starvation, and Deadlock! \nWe will be discussing them shortly, but just so you know we have to apply \nLocks, but they must follow a set of protocols to avoid such undesirable \nproblems. Shortly we will use 2-Phase Locking (2-PL) which will use the \n \n \nconcept of Locks to avoid deadlock. So, applying simple locking, we may not \nalways produce Serializable results, it may lead to Deadlock Inconsistency. \n \nProblem With Simple Locking \nConsider the Partial Schedule: \n \nT1 \nT2 \n1 \nlock-X(B) \n \n2 \nread(B) \n \n3 \nB:=B-50 \n \n4 \nwrite(B) \n \n5 \n \nlock-S(A) \n6 \n \nread(A) \n7 \n \nlock-S(B) \n8 \nlock-X(A) \n \n \n \n \n \nDeadlock \nIn deadlock consider the above execution phase. Now, T1 holds an Exclusive \nlock over B, and T2 holds a Shared lock over A. Consider Statement \n7, T2 requests for lock on B, while in Statement 8 T1 requests lock on A. This \nas you may notice imposes a Deadlock as none can proceed with their \nexecution. \nStarvation \nIt is also possible if concurrency control manager is badly designed. For \nexample: A transaction may be waiting for an X-lock on an item, while a \nsequence of other transactions request and are granted an S-lock on the same \nitem. This may be avoided if the concurrency control manager is properly \ndesigned. \n \n \n17. \nTwo Phase Locking (2-PL) \n there are two types of Locks available Shared S(a) and Exclusive X(a). \nImplementing this lock system without any restrictions gives us the Simple \nLock-based protocol (or Binary Locking), but it has its own disadvantages, \nthey do not guarantee Serializability. Schedules may follow the preceding rules \nbut a non-serializable schedule may result. \nTo \nguarantee \nserializability, \nwe \nmust \nfollow \nsome \nadditional \nprotocols concerning the positioning of locking and unlocking operations in \nevery transaction. This is where the concept of Two-Phase Locking(2-\nPL) comes into the picture, 2-PL ensures serializability. Now, let\u2019s dig deep! \n \n \n \nTwo Phase Locking \nA transaction is said to follow the Two-Phase Locking protocol if Locking and \nUnlocking can be done in two phases. \n\uf0b7 Growing Phase: New locks on data items may be acquired but none \ncan be released. \n\uf0b7 Shrinking Phase: Existing locks may be released but no new locks \ncan be acquired. \nNote:  \nIf lock conversion is allowed, then upgrading of lock( from S(a) to X(a) ) is \nallowed in the Growing Phase, and downgrading of lock (from X(a) to S(a)) \nmust be done in the shrinking phase. \nLet\u2019s see a transaction implementing 2-PL. \n \nT1 \nT2 \n1 \nlock-S(A) \n \n2 \n \nlock-S(A) \n3 \nlock-X(B) \n \n4 \n\u2026\u2026\u2026. \n\u2026\u2026\u2026. \n5 \nUnlock(A) \n \n6 \n \nLock-X(C) \n7 \nUnlock(B) \n \n8 \n \nUnlock(A) \n9 \n \nUnlock(C) \n10 \n\u2026\u2026\u2026. \n\u2026\u2026\u2026. \n \nThis is just a skeleton transaction that shows how unlocking and locking work \nwith 2-PL. Note for: \nTransaction T1 \n\uf0b7 The growing Phase is from steps 1-3 \n\uf0b7 The shrinking Phase is from steps 5-7 \n\uf0b7 Lock Point at 3 \nTransaction T2 \n\uf0b7 The growing Phase is from steps 2-6 \n\uf0b7 The shrinking Phase is from steps 8-9 \n\uf0b7 Lock Point at 6 \n \nLock Point \nThe Point at which the growing phase ends, i.e., when a transaction takes the \nfinal lock, it needs to carry on its work. Now look at the schedule, you\u2019ll surely \nunderstand. I have said that 2-PL ensures serializability, but there are still some \ndrawbacks of 2-PL. Let us glance at the drawbacks. \n\uf0b7 Cascading Rollback is possible under 2-PL. \n\uf0b7 Deadlocks and Starvation are possible. \n \n \nCategories of two-Phase Locking \nNow that we are familiar with what is Two-Phase Locking (2-PL) and the basic \nrules which should be followed which ensures serializability. Moreover, we \ncame across problems with 2-PL, Cascading Aborts, and Deadlocks. Now, we \nturn towards the enhancements made on 2-PL which try to make the protocol \nnearly error-free. Briefly, we allow some modifications to 2-PL to improve it.  \nThere are three categories:  \n\uf0b7 Strict 2-PL \n\uf0b7 Rigorous 2-PL \n\uf0b7 Conservative 2-PL \nNow recall the rules followed in Basic 2-PL, over that we make some extra \nmodifications. Let\u2019s now see what are the modifications and what drawbacks \nthey solve.  \nStrict 2-PL \u2013 \nThis requires that in addition to the lock being 2-Phase all Exclusive(X) \nlocks held by the transaction be released until after the Transaction Commits.  \nFollowing Strict 2-PL ensures that our schedule is: \n\uf0b7 Recoverable \n\uf0b7 Cascadeless \nHence, it gives us freedom from Cascading Abort which was still there in Basic \n2-PL and moreover guarantee Strict Schedules but still, Deadlocks are \npossible!  \nRigorous 2-PL \u2013 \nThis requires that in addition to the lock being 2-Phase all Exclusive(X) and \nShared(S) locks held by the transaction be released until after the Transaction \nCommits. Following Rigorous 2-PL ensures that our schedule is: \n\uf0b7 Recoverable \n\uf0b7 Cascadeless \nHence, it gives us freedom from Cascading Abort which was still there in Basic \n2-PL and moreover guarantee Strict Schedules but still, Deadlocks are \npossible!  \nNote: The difference between Strict 2-PL and Rigorous 2-PL is that Rigorous is \nmore restrictive, it requires both Exclusive and Shared locks to be held until \nafter the Transaction commits and this is what makes the implementation of \nRigorous 2-PL easier.   \n \nConservative 2-PL \u2013 \nA.K.A Static 2-PL, this protocol requires the transaction to lock all the items it \naccesses before the Transaction begins execution by predeclaring its read-set \nand write-set. If any of the predeclared items needed cannot be locked, the \n \n \ntransaction does not lock any of the items, instead, it waits until all the items are \navailable for locking.  \nHowever, it is difficult to use in practice because of the need to predeclare the \nread-set and the write-set which is not possible in many situations. In practice, \nthe most popular variation of 2-PL is Strict 2-PL.  \nVenn Diagram below shows the classification of schedules that are rigorous and \nstrict. The universe represents the schedules that can be serialized as 2-PL. Now \nas the diagram suggests, and it can also be logically concluded, if a schedule is \nRigorous then it is Strict. We can also think in another way, say we put a \nrestriction on a schedule which makes it strict, adding another to the list of \nrestrictions make it Rigorous. Take a moment to again analyze the diagram and \nyou\u2019ll definitely get it.  \n                            Figure 38: Conservative \nImage \u2013 Venn Diagram showing categories of languages under 2-PL \nNow, let\u2019s see the schedule below, tell me if this schedule can be locked using \n2-PL, and if yes, show how and what class of 2-PL does your answer belongs \nto.  \n \nT1 \nT2 \n1 \nRead(A) \n  \n2 \n  \nRead(A) \n3 \nRead(B) \n  \n4 \nWrite(B) \n  \n5 \nCommit \n  \n6 \n  \nRead(B) \n7 \n  \nWrite(B) \n6 \n  \nCommit \n \n \n \n \nYes, the schedule is conflict serializable, so we can try implementing 2-PL. So, \nlet\u2019s try\u2026   \nSolution: \n \nT1 \nT2 \n1 \nLock-S(A) \n  \n2 \nRead(A) \n  \n3 \n  \nLock-S(A) \n4 \n  \nRead(A) \n5 \nLock-X(B) \n  \n6 \nRead(B) \n  \n7 \nWrite(B) \n  \n8 \nCommit \n  \n9 \nUnlock(A) \n  \n10 \nUnlock(B) \n  \n11 \n  \nLock-X(B) \n12 \n  \nRead(B) \n13 \n  \nWrite(B) \n14 \n  \nCommit \n15 \n  \nUnlock(A) \n16 \n  \nUnlock(B) \n  \nNow, this is one way I choose to implement the locks on A and B. You may try \na different sequence but remember to follow the 2-PL protocol. With that said, \nobserve that our locks are released after Commit operation so this satisfies Strict \n2-PL protocol.  \nBy now, I guess you must\u2019ve got the idea of how to differentiate between types \nof 2-PL. Remember the theory as problems come in the examination sometimes \njust based on theoretical knowledge. Next, we\u2019ll look at some examples of \nConservative 2-PL and how does it differ from the above two types of 2-PL. \nWhat makes it Deadlock free and also so difficult to implement. Then we\u2019ll \nconclude the topic of 2-PL. Shortly we\u2019ll move on to another type of Lock-based \nProtocol- Graph-Based Protocols. \n \n \n18. Timestamp Ordering Protocol \nConcurrency Control can be implemented in different ways. One way to \nimplement it is by using Locks. Now, let us discuss Time Stamp Ordering \nProtocol.  \nAs earlier introduced, Timestamp is a unique identifier created by the DBMS \nto identify a transaction. They are usually assigned in the order in which they \nare submitted to the system. Refer to the timestamp of a transaction T as TS(T).  \n \nTimestamp Ordering Protocol \u2013  \nThe main idea for this protocol is to order the transactions based on their \nTimestamps. A schedule in which the transactions participate is then \nserializable and the only equivalent serial schedule permitted has the \ntransactions in the order of their Timestamp Values. Stating simply, the \nschedule is equivalent to the particular Serial Order corresponding to \nthe order of the Transaction timestamps. An algorithm must ensure that, for \neach item accessed by Conflicting Operations in the schedule, the order in \nwhich the item is accessed does not violate the ordering. To ensure this, use \ntwo Timestamp Values relating to each database item X.  \n\uf0b7 W_TS(X) is the largest timestamp of any transaction that \nexecuted write(X) successfully. \n\uf0b7 R_TS(X) is the largest timestamp of any transaction that \nexecuted read(X) successfully. \n \nBasic Timestamp Ordering \u2013  \nEvery transaction is issued a timestamp based on when it enters the system. \nSuppose, if an old transaction Ti has timestamp TS(Ti), a new transaction Tj is \nassigned timestamp TS(Tj) such that TS(Ti) < TS(Tj). The protocol manages \nconcurrent execution such that the timestamps determine the serializability \norder. The timestamp ordering protocol ensures that any conflicting read and \nwrite operations are executed in timestamp order. Whenever some \nTransaction T tries to issue a R_item(X) or a W_item(X), the Basic TO \nalgorithm compares the timestamp of T with R_TS(X) & W_TS(X) to ensure \nthat the Timestamp order is not violated. This describes the Basic TO protocol \nin the following two cases.  \n\uf0b7 Whenever a Transaction T issues a W_item(X) operation, check the \nfollowing conditions:  \n\uf0b7 If R_TS(X) > TS(T) and if W_TS(X) > TS(T), then abort and \nrollback T and reject the operation. else, \n\uf0b7 Execute W_item(X) operation of T and set W_TS(X) to \nTS(T). \n\uf0b7 Whenever a Transaction T issues a R_item(X) operation, check the \nfollowing conditions:  \n \n \n\uf0b7 If W_TS(X) > TS(T), then abort and reject T and reject the \noperation, else \n\uf0b7 If W_TS(X) <= TS(T), then execute the R_item(X) operation \nof T and set R_TS(X) to the larger of TS(T) and current \nR_TS(X).  \nWhenever the Basic TO algorithm detects two conflicting operations that occur \nin an incorrect order, it rejects the latter of the two operations by aborting the \nTransaction that issued it. Schedules produced by Basic TO are guaranteed to \nbe conflict serializable. Already discussed that using Timestamp can ensure that \nour schedule will be deadlock free.  \nOne drawback of the Basic TO protocol is that Cascading Rollback is still \npossible. Suppose we have a Transaction T1 and T2 has used a value written by \nT1. If T1 is aborted and resubmitted to the system then, T2 must also be aborted \nand rolled back. So the problem of Cascading aborts still prevails.  \nLet\u2019s gist the Advantages and Disadvantages of Basic TO protocol:  \n\uf0b7 Timestamp Ordering protocol ensures serializability since the \nprecedence graph will be of the form:  \n                               Figure 39: Precedence Graph for TS ordering \n\uf0b7 Timestamp protocol ensures freedom from deadlock as no transaction \never waits. \n\uf0b7 But the schedule may not be cascade free, and may not even be \nrecoverable. \n \nStrict Timestamp Ordering \u2013  \nA variation of Basic TO is called Strict TO ensures that the schedules are both \nStrict and Conflict Serializable. In this variation, a Transaction T that issues a \nR_item(X) or W_item(X) such that TS(T) > W_TS(X) has its read or write \noperation delayed until the Transaction T\u2018 that wrote the values of X has \ncommitted or aborted.  \n  \n \n \nAdvantages: \n\uf0b7 High Concurrency: Timestamp-based concurrency control allows for a \nhigh degree of concurrency by ensuring that transactions do not interfere \nwith each other. \n\uf0b7 Efficient: The technique is efficient and scalable, as it does not require \nlocking and can handle a large number of transactions. \n\uf0b7 No Deadlocks: Since there are no locks involved, there is no possibility \nof deadlocks occurring. \n\uf0b7 Improved \nPerformance: By \nallowing \ntransactions \nto \nexecute \nconcurrently, the overall performance of the database system can be \nimproved. \nDisadvantages: \n\uf0b7 Limited Granularity: The granularity of timestamp-based concurrency \ncontrol is limited to the precision of the timestamp. This can lead to \nsituations where transactions are unnecessarily blocked, even if they do \nnot conflict with each other. \n\uf0b7 Timestamp Ordering: In order to ensure that transactions are executed \nin the correct order, the timestamps need to be carefully managed. If not \nmanaged properly, it can lead to inconsistencies in the database. \n\uf0b7 Timestamp Synchronization: Timestamp-based concurrency control \nrequires that all transactions have synchronized clocks. If the clocks are \nnot synchronized, it can lead to incorrect ordering of transactions. \n\uf0b7 Timestamp Allocation: Allocating unique timestamps for each \ntransaction can be challenging, especially in distributed systems where \ntransactions may be initiated at different locations. \n \nTimestamp and Deadlock Prevention schemes \nDeadlock occurs when each transaction T in a schedule of two or \nmore transactions waiting for some item locked by some other transaction T\u2018 in \nthe set. Thus, both end up in a deadlock situation, waiting for the other to release \nthe lock on the item. Deadlocks are a common problem and we have introduced \nthe problem while solving the Concurrency Control by the introduction \nof Locks. Deadlock avoidance is a major issue and some protocols were \nsuggested \nto \navoid \nthem, \nlike Conservative \n2-PL and \nGraph-\nBased protocols but some drawbacks are still there.  \nHere, we will discuss a new concept of Transaction Timestamp TS(Ti). A \ntimestamp is a unique identifier created by the DBMS to identify a transaction. \nThey are usually assigned in the order in which they are submitted to the system, \nso a timestamp may be thought of as the transaction start time.  \nThere may be different ways of generating timestamps such as  \n \n \n\uf0b7 A simple counter that increments each time its value is assigned to a \ntransaction. They may be numbered 1, 2, 3\u2026. Though we\u2019ll have to \nreset the counter from time to time to avoid overflow. \n\uf0b7 Using the current date/time from the system clock. Just ensuring that \nno two transactions are given the same value in the same clock tick, \nwe will always get a unique timestamp. This method is widely used. \nDeadlock Prevention Schemes based on Timestamp: \nAs discussed, Timestamps are unique identifiers assigned to each transaction. \nThey are based on the order in which Transactions are started. Say if T1 starts \nbefore T2 then TS(T1) will be less than (<) TS(T2).  \nThere are two schemes to prevent deadlock called wound-wait and wait-die. Say \nthere are two transactions Ti and Tj, now say Ti tries to lock an item X but \nitem X is already locked by some Tj, now in such a conflicting situation the two \nschemes which prevent deadlock. We\u2019ll use this context shortly.  \n\uf0b7 Wait_Die: An older transaction is allowed to wait for a younger \ntransaction, whereas a younger transaction requesting an item held by \nan \nolder \ntransaction \nis \naborted \nand \nrestarted.  \nFrom the context above, if TS(Ti) < TS(Tj), then (Ti older than Tj) \nTi is allowed to wait; otherwise abort Ti (Ti younger than Tj) and \nrestart it later with the same timestamp. \n\uf0b7 Wound_Wait: It is just the opposite of the Wait_Die technique. Here, \na younger transaction is allowed to wait for an older one, whereas if \nan older transaction requests an item held by the younger transaction, \nwe \npreempt \nthe \nyounger \ntransaction \nby \naborting \nit.  \nFrom the context above, if TS(Ti) < TS(Tj), then (Ti older than Tj) \nTj is aborted (i.e., Ti wounds Tj) and restarts it later with the same \nTimestamp; otherwise (Ti younger than Tj) Ti is allowed to wait. \nThus, both schemes end up aborting the younger of the two transactions that \nmay be involved in a deadlock. It is done on the basis of the assumption that \naborting the younger transaction will waste less processing which is logical. In \nsuch a case there cannot be a cycle since we are waiting linearly in both cases.  \n \nAnother group of protocols prevents deadlock but does not require Timestamps. \nThey are discussed below:  \n\uf0b7 No-waiting Algorithm: This follows a simple approach, if a \nTransaction is unable to obtain a lock, it is immediately aborted and \nthen restarted after a certain time delay without checking if a deadlock \nwill occur or not. Here, no Transaction ever waits so there is no \npossibility \nfor \ndeadlock.  \nThis method is somewhat not practical. It may cause the transaction to \nabort and restart unnecessarily. \n \n \n\uf0b7 Cautious Waiting: If Ti tries to lock an item X but is not able to do \nbecause X is locked by some Tj. In such a conflict, if Tj is not waiting \nfor some other locked item, then Ti is allowed to wait, \notherwise, abort Ti. \n \nAnother approach, to deal with deadlock is deadlock detection, we can use Wait-\nfor-Graph. This uses a similar approach when we used to check for cycles while \nchecking for serializability.  \nStarvation: One problem that may occur when we use locking is starvation \nwhich occurs when a transaction cannot proceed for an indefinite period of time \nwhile other transactions in the system continue normally. This may occur if the \nwaiting scheme for locked items is unfair, giving priority to some transactions \nover others. We may have some solutions for Starvation. One is using a first \ncome first serve queue; transactions are enabled to lock an item in the order in \nwhich they originally requested the lock. This is a widely used mechanism to \nreduce starvation. Our Concurrency Control Manager is responsible to schedule \nthe transactions, so it employs different methods to overcome them. \nTimestamp-based concurrency control and deadlock prevention schemes are \ntwo important techniques used in database management systems to ensure \ntransaction correctness and concurrency. \nIn addition, timestamp-based schemes use a validation procedure to check \nwhether a transaction has read data that has been modified by another \ntransaction after the first transaction has read it. If such a conflict is detected, \nthe transaction is rolled back to ensure consistency and correctness. \nDeadlock prevention schemes are used to prevent situations where two or more \ntransactions are waiting for each other to release locks, resulting in a deadlock. \nDeadlocks occur when two transactions hold exclusive locks on resources that \nthe other transaction needs to proceed. To prevent deadlocks, DBMSs use \nseveral schemes, including: \nTimeout-based schemes: Transactions are allowed to hold a lock for a limited \ntime. If a transaction exceeds its allotted time, it is rolled back, allowing other \ntransactions to proceed. \nWait-die schemes: If a transaction requests a lock held by another transaction, \nthe requesting transaction waits if its timestamp is older than the timestamp of \nthe transaction holding the lock. If the timestamp of the requesting transaction \nis newer, it rolls back and is restarted with a new timestamp. \nWound-wait schemes: If a transaction requests a lock held by another \ntransaction, the requesting transaction is granted the lock if its timestamp is \nnewer than the timestamp of the transaction holding the lock. If the timestamp \nof the requesting transaction is older, the transaction holding the lock is rolled \nback and restarted with a new timestamp. \n \n \nFile Structures \n \n19. \n File Organization in DBMS \nA database consists of a huge amount of data. The data is grouped within a table \nin RDBMS, and each table has related records. A user can see that the data is \nstored in the form of tables, but this huge amount of data is stored in physical \nmemory in the form of files.  \n \nWhat is a File? \nA file is named a collection of related information that is recorded on secondary \nstorage such as magnetic disks, magnetic tapes, and optical disks.  \n \nWhat is File Organization? \nFile Organization refers to the logical relationships among various records that \nconstitute the file, particularly with respect to the means of identification and \naccess to any specific record. In simple terms, Storing the files in a certain order \nis called File Organization. File Structure refers to the format of the label and \ndata blocks and of any logical control record.  \nThe Objective of File Organization \n\uf0b7 It helps in the faster selection of records i.e. it makes the process faster. \n\uf0b7 Different Operations like inserting, deleting, and updating different \nrecords are faster and easier. \n\uf0b7 It prevents us from inserting duplicate records via various operations. \n\uf0b7 It helps in storing the records or the data very efficiently at a minimal \ncost. \n \n19.1. Types of File Organization \nTypes of File Organizations \nVarious methods have been introduced to Organize files. These methods have \nadvantages and disadvantages on the basis of access or selection. Thus it is all \nupon the programmer to decide the best-suited file Organization method \naccording to his requirements.  \nSome types of File Organizations are:  \n\uf0b7 Sequential File Organization \n\uf0b7 Heap File Organization  \n\uf0b7 Hash File Organization  \n\uf0b7 B+ Tree File Organization  \n\uf0b7 Clustered File Organization \n\uf0b7 ISAM (Indexed Sequential Access Method) \n  \n \n \nWe will be discussing each of the file Organizations in further sets of this article \nalong with the differences and advantages/ disadvantages of each file \nOrganization method.  \n \n1) Sequential File Organization \nThe easiest method for file Organization is the Sequential method. In this \nmethod, the file is stored one after another in a sequential manner. There are two \nways to implement this method: \n\uf0b7 Pile File Method \nThis method is quite simple, in which we store the records in a sequence i.e. one \nafter the other in the order in which they are inserted into the tables.  \nInsertion of the new record: Let the R1, R3, and so on up to R5 and R4 be four \nrecords in the sequence. Here, records are nothing but a row in any table. \nSuppose a new record R2 has to be inserted in the sequence, then it is simply \nplaced at the end of the file.  \n\uf0b7 Sorted File Method \nIn this method, As the name itself suggests whenever a new record has to be \ninserted, it is always inserted in a sorted (ascending or descending) manner. The \nsorting of records may be based on any primary key or any other key.  \nInsertion of the new record: Let us assume that there is a preexisting sorted \nsequence of four records R1, R3, and so on up to R7 and R8. Suppose a new \nrecord R2 must be inserted in the sequence, then it will be inserted at the end of \nthe file and then it will sort the sequence.  \nAdvantages of Sequential File Organization \n\uf0b7 Fast and efficient method for huge amounts of data. \n\uf0b7 Simple design. \n\uf0b7 Files can be easily stored in magnetic tapes i.e. cheaper storage \nmechanism. \nDisadvantages of Sequential File Organization \n\uf0b7 Time wastage as we cannot jump on a particular record that is required, \nbut we have to move in a sequential manner which takes our time. \n\uf0b7 The sorted file method is inefficient as it takes time and space for \nsorting records.  \n \n2) Heap File Organization \nHeap File Organization works with data blocks. In this method, records are \ninserted at the end of the file, into the data blocks. No Sorting or Ordering is \nrequired in this method. If a data block is full, the new record is stored in some \nother block,  \n \n \nHere the other data block need not be the very next data block, but it can be any \nblock in the memory. It is the responsibility of DBMS to store and manage the \nnew records.  \nInsertion of the new record: Suppose we have four records in the heap R1, R5, \nR6, R4, and R3, and suppose a new record R2 has to be inserted in the heap \nthen, since the last data block i.e data block 3 is full it will be inserted in any of \nthe data blocks selected by the DBMS, let\u2019s say data block 1. \nIf we want to search, delete or update data in the heap file Organization we will \ntraverse the data from the beginning of the file till we get the requested record. \nThus if the database is very huge, searching, deleting, or updating the record \nwill take a lot of time. \nAdvantages of Heap File Organization \n\uf0b7 Fetching and retrieving records is faster than sequential records but \nonly in the case of small databases. \n\uf0b7 When there is a huge number of data that needs to be loaded into \nthe database at a time, then this method of file Organization is best \nsuited. \nDisadvantages of Heap File Organization \n\uf0b7 The problem of unused memory blocks. \n\uf0b7 Inefficient for larger databases. \n \n19.2. Hashing in DBMS \nHashing in DBMS is a technique to quickly locate a data record in a database \nirrespective of the size of the database. For larger databases containing \nthousands and millions of records, the indexing data structure technique \nbecomes very inefficient because searching a specific record through indexing \nwill consume more time. This doesn\u2019t align with the goals of DBMS, especially \nwhen performance and date retrieval time are minimized. So, to counter this \nproblem hashing technique is used. In this article, we will learn about various \nhashing techniques. \nWhat is Hashing? \nThe hashing technique utilizes an auxiliary hash table to store the data records \nusing a hash function. There are 2 key components in hashing: \n\uf0b7 Hash Table: A hash table is an array or data structure, and its size is \ndetermined by the total volume of data records present in the database. \nEach memory location in a hash table is called a \u2018bucket\u2018 or hash \nindice and stores a data record\u2019s exact location and can be accessed \nthrough a hash function. \n\uf0b7 Bucket: A bucket is a memory location (index) in the hash table that \nstores the data record. These buckets generally store a disk block \nwhich further stores multiple records. It is also known as the hash \nindex. \n \n \n\uf0b7 Hash Function: A hash function is a mathematical equation or \nalgorithm that takes one data record\u2019s primary key as input and \ncomputes the hash index as output. \n \nHash Function \nA hash function is a mathematical algorithm that computes the index or the \nlocation where the current data record is to be stored in the hash table so that it \ncan be accessed efficiently later. This hash function is the most crucial \ncomponent that determines the speed of fetching data. \nWorking of Hash Function \nThe hash function generates a hash index through the primary key of the data \nrecord. \nNow, there are 2 possibilities: \n1. The hash index generated isn\u2019t already occupied by any other value. So, the \naddress of the data record will be stored here. \n2. The hash index generated is already occupied by some other value. This is \ncalled collision so to counter this, a collision resolution technique will be \napplied. \n3. Now whenever we query a specific record, the hash function will be applied \nand returns the data record comparatively faster than indexing because we can \ndirectly reach the exact location of the data record through the hash function \nrather than searching through indices one by one. \nExample: \n                                       Figure 40: Hashing \nTypes of Hashing in DBMS \nThere are two primary hashing techniques in DBMS. \n1) Static Hashing \nIn static hashing, the hash function always generates the same bucket\u2019s address. \nFor example, if we have a data record for employee_id = 107, the hash function \nis mod-5 which is \u2013 H(x) % 5, where x = id. Then the operation will take place \nlike this: \n \n \nH(106) % 5 = 1. \nThis indicates that the data record should be placed or searched in the 1st \nbucket (or 1st hash index) in the hash table. \nExample: \n                              Figure 41: Static Hashing Technique \nThe primary key is used as the input to the hash function and the hash function \ngenerates the output as the hash index (bucket\u2019s address) which contains the \naddress of the actual data record on the disk block. \nStatic Hashing has the following Properties. \n\uf0b7 Data Buckets: The number of buckets in memory remains constant. \nThe size of the hash table is decided initially and it may also implement \nchaining that will allow handling some collision issues though, it\u2019s \nonly a slight optimization and may not prove worthy if the database \nsize keeps fluctuating. \n\uf0b7 Hash function: It uses the simplest hash function to map the data \nrecords to its appropriate bucket. It is generally modulo-hash function \n\uf0b7 Efficient for known data size: It\u2019s very efficient in terms when we \nknow the data size and its distribution in the database. \n\uf0b7 It is inefficient and inaccurate when the data size dynamically varies \nbecause we have limited space and the hash function always generates \nthe same value for every specific input. When the data size fluctuates \nvery often it\u2019s not at all useful because collision will keep happening \nand it will result in problems like \u2013 bucket skew, insufficient buckets \netc. \nTo resolve this problem of bucket overflow, techniques such as \u2013 chaining and \nopen addressing are used. Here\u2019s a brief info on both: \n \n\uf0b7 Chaining \nChaining is a mechanism in which the hash table is implemented using an array \nof type nodes, where each bucket is of node type and can contain a long chain \nof linked lists to store the data records. So, even if a hash function generates the \nsame value for any data record it can still be stored in a bucket by adding a new \nnode. \n \n \nHowever, this will give rise to the problem bucket skew that is, if the hash \nfunction keeps generating the same value again and again then the hashing will \nbecome inefficient as the remaining data buckets will stay unoccupied or store \nminimal data. \n\uf0b7 Open Addressing/Closed Hashing \nThis is also called closed hashing this aims to solve the problem of collision by \nlooking out for the next empty slot available which can store data. It uses \ntechniques like linear probing, quadratic probing, double hashing, etc. \n \n2) Dynamic Hashing \nDynamic hashing is also known as extendible hashing, used to handle database \nthat frequently changes data sets. This method offers us a way to add and remove \ndata buckets on demand dynamically. This way as the number of data records \nvaries, the buckets will also grow and shrink in size periodically whenever a \nchange is made. \nProperties of Dynamic Hashing \n\uf0b7 The buckets will vary in size dynamically periodically as changes are \nmade offering more flexibility in making any change. \n\uf0b7 Dynamic Hashing aids in improving overall performance by \nminimizing or completely preventing collisions. \n\uf0b7 It has the following major components: Data bucket, Flexible hash \nfunction, and directories \n\uf0b7 A flexible hash function means that it will generate more dynamic \nvalues and will keep changing periodically asserting to the \nrequirements of the database. \n\uf0b7 Directories are containers that store the pointer to buckets. If bucket \noverflow or bucket skew-like problems happen to occur, then bucket \nsplitting is done to maintain efficient retrieval time of data records. \nEach directory will have a directory id. \n\uf0b7 Global Depth: It is defined as the number of bits in each directory id. \nThe more the number of records, the more bits are there. \nWorking of Dynamic Hashing \nExample: If global depth: k = 2, the keys will be mapped accordingly to the \nhash index. K bits starting from LSB will be taken to map a key to the buckets. \nThat leave \ns us with the following 4 possibilities: 00, 11, 10, 01. \n \n \n \n \n \n \n \n19.3. All about B-Tree \nThe limitations of traditional binary search trees can be frustrating. Meet the B-\nTree, the multi-talented data structure that can handle massive amounts of data \nwith ease. When it comes to storing and searching large amounts of data, \ntraditional binary search trees can become impractical due to their poor \nperformance and high memory usage. B-Trees, also known as B-Tree or \nBalanced Tree, are a type of self-balancing tree that was specifically designed \nto overcome these limitations. \nUnlike traditional binary search trees, B-Trees are characterized by the large \nnumber of keys that they can store in a single node, which is why they are also \nknown as \u201clarge key\u201d trees. Each node in a B-Tree can contain multiple keys, \nwhich allows the tree to have a larger branching factor and thus a shallower \nheight. This shallow height leads to less disk I/O, which results in faster search \nand insertion operations. B-Trees are particularly well suited for storage systems \nthat have slow, bulky data access such as hard drives, flash memory, and CD-\nROMs. \nB-Trees maintains balance by ensuring that each node has a minimum number \nof keys, so the tree is always balanced. This balance guarantees that the time \ncomplexity for operations such as insertion, deletion, and searching is always \nO(log n), regardless of the initial shape of the tree. \n \nTime Complexity of B-Tree:  \nSr. No. \nAlgorithm \nTime Complexity \n1. \nSearch \nO(log n) \n2. \nInsert \nO(log n) \n3. \nDelete \nO(log n) \nNote: \u201cn\u201d is the total number of elements in the B-tree \n \nProperties of B-Tree:  \n\uf0b7 All leaves are at the same level. \n\uf0b7 B-Tree is defined by the term minimum degree \u2018t\u2018. The value of \u2018t\u2018 \ndepends upon disk block size. \n\uf0b7 Every node except the root must contain at least t-1 keys. The root may \ncontain a minimum of 1 key. \n\uf0b7 All nodes (including root) may contain at most (2*t \u2013 1) keys. \n\uf0b7 Number of children of a node is equal to the number of keys in it \nplus 1. \n\uf0b7 All keys of a node are sorted in increasing order. The child between \ntwo keys k1 and k2 contains all keys in the range from k1 and k2. \n\uf0b7 B-Tree grows and shrinks from the root which is unlike Binary Search \nTree. Binary Search Trees grow downward and also shrink from \ndownward. \n \n \n\uf0b7 Like other balanced Binary Search Trees, the time complexity to \nsearch, insert, and delete is O(log n). \n\uf0b7 Insertion of a Node in B-Tree happens only at Leaf Node. \nFollowing is an example of a B-Tree of minimum order 5  \nNote: that in practical B-Trees, the value of the minimum order is much more \nthan 5.  \n  \n                               Figure 42: B-Tree \nWe can see in the above diagram that all the leaf nodes are at the same level and \nall non-leafs have no empty sub-tree and have keys one less than the number of \ntheir children. \n \nTraversal in B-Tree:  \nTraversal is also similar to Inorder traversal of Binary Tree. We start from the \nleftmost child, recursively print the leftmost child, then repeat the same process \nfor the remaining children and keys. In the end, recursively print the rightmost \nchild.  \n \nSearch Operation in B-Tree:  \nSearch is similar to the search in Binary Search Tree. Let the key to be searched \nis k.  \n\uf0b7 Start from the root and recursively traverse down.  \n\uf0b7 For every visited non-leaf node,  \n\uf0b7 If the node has the key, we simply return the node.  \n\uf0b7 Otherwise, we recur down to the appropriate child (The child \nwhich is just before the first greater key) of the node.  \n\uf0b7 If we reach a leaf node and don\u2019t find k in the leaf node, then return \nNULL. \nSearching a B-Tree is similar to searching a binary tree. The algorithm is similar \nand goes with recursion. At each level, the search is optimized as if the key \nvalue is not present in the range of the parent then the key is present in another \nbranch. As these values limit the search they are also known as limiting values \nor separation values. If we reach a leaf node and don\u2019t find the desired key then \nit will display NULL. \n \n \n \n \nApplications of B-Trees: \n\uf0b7 It is used in large databases to access data stored on the disk \n\uf0b7 Searching for data in a data set can be achieved in significantly less \ntime using the B-Tree \n\uf0b7 With the indexing feature, multilevel indexing can be achieved. \n\uf0b7 Most of the servers also use the B-tree approach. \n\uf0b7 B-Trees are used in CAD systems to organize and search geometric \ndata. \n\uf0b7 B-Trees are also used in other areas such as natural language \nprocessing, computer networks, and cryptography. \n \nAdvantages of B-Trees: \n\uf0b7  B-Trees have a guaranteed time complexity of O(log n) for basic \noperations like insertion, deletion, and searching, which makes them \nsuitable for large data sets and real-time applications. \n\uf0b7  B-Trees are self-balancing. \n\uf0b7 High-concurrency and high-throughput. \n\uf0b7 Efficient storage utilization. \n \nDisadvantages of B-Trees: \n\uf0b7 B-Trees are based on disk-based data structures and can have a high \ndisk usage. \n\uf0b7 Not the best for all cases. \n\uf0b7 Slow in comparison to other data structures. \n \nInsertion Operation in B-Tree \nA new key is always inserted at the leaf node. Let the key to be inserted be k. \nLike BST, we start from the root and traverse down till we reach a leaf node. \nOnce we reach a leaf node, we insert the key in that leaf node. Unlike BSTs, we \nhave a predefined range on the number of keys that a node can contain. So before \ninserting a key to the node, we make sure that the node has extra space.  \nHow to make sure that a node has space available for a key before the key is \ninserted? We use an operation called splitChild() that is used to split a child of \na node. See the following diagram to understand split. In the following diagram, \nchild y of x is being split into two nodes y and z. Note that the splitChild \noperation moves a key up and this is the reason B-Trees grow up, unlike BSTs \nwhich grow down.  \n \n \n \n \n \nInsertion  \n1) Initialize x as root.  \n2) While x is not leaf, do following  \n..a) Find the child of x that is going to be traversed next. Let the child be y.  \n..b) If y is not full, change x to point to y.  \n..c) If y is full, split it and change x to point to one of the two parts of y. If k is \nsmaller than mid key in y, then set x as the first part of y. Else second part of \ny. When we split y, we move a key from y to its parent x.  \n3) The loop in step 2 stops when x is leaf. x must have space for 1 extra key as \nwe have been splitting all nodes in advance. So simply insert k to x.  \nNote that the algorithm follows the Cormen book. It is actually a proactive \ninsertion algorithm where before going down to a node, we split it if it is full. \nThe advantage of splitting before is, we never traverse a node twice. If we don\u2019t \nsplit a node before going down to it and split it only if a new key is inserted \n(reactive), we may end up traversing all nodes again from leaf to root. This \nhappens in cases when all nodes on the path from the root to leaf are full. So \nwhen we come to the leaf node, we split it and move a key up. Moving a key up \nwill cause a split in parent node (because the parent was already full). This \ncascading effect never happens in this proactive insertion algorithm. There is a \ndisadvantage of this proactive insertion though, we may do unnecessary splits.  \nLet us understand the algorithm with an example tree of minimum degree \u2018t\u2019 \nas 3 and a sequence of integers 10, 20, 30, 40, 50, 60, 70, 80 and 90 in an \ninitially empty B-Tree. \nInitially root is NULL. Let us first insert 10.  \nLet us now insert 20, 30, 40 and 50. They all will be inserted in root because the \nmaximum number of keys a node can accommodate is 2*t \u2013 1 which is 5. \n \n \nLet us now insert 60. Since root node is full, it will first split into two, then 60 \nwill be inserted into the appropriate child.  \n \nLet us now insert 70 and 80. These new keys will be inserted into the appropriate \nleaf without any split.  \nLet us now insert 90. This insertion will cause a split. The middle key will go \nup to the parent.  \n \nDelete Operation \nB Trees is a type of data structure commonly known as a Balanced Tree that \nstores multiple data items very easily. B Trees are one of the most useful data \nstructures that provide ordered access to the data in the database. In this article, \nwe will see the delete operation in the B-Tree. B-Trees are self-balancing trees. \n \n \nDeletion Process in B-Trees \nDeletion from a B-tree is more complicated than insertion because we can delete \na key from any node-not just a leaf\u2014and when we delete a key from an internal \nnode, we will have to rearrange the node\u2019s children.  \nAs in insertion, we must make sure the deletion doesn\u2019t violate the B-tree \nproperties. Just as we had to ensure that a node didn\u2019t get too big due to \ninsertion, we must ensure that a node doesn\u2019t get too small during deletion \n(except that the root is allowed to have fewer than the minimum number t-1 of \nkeys). Just as a simple insertion algorithm might have to back up if a node on \nthe path to where the key was to be inserted was full, a simple approach to \ndeletion might have to back up if a node (other than the root) along the path to \nwhere the key is to be deleted has the minimum number of keys. \nThe deletion procedure deletes the key k from the subtree rooted at x. This \nprocedure guarantees that whenever it calls itself recursively on a node x, the \nnumber of keys in x is at least the minimum degree t. Note that this condition \nrequires one more key than the minimum required by the usual B-tree \nconditions, so sometimes a key may have to be moved into a child node before \nrecursion descends to that child. This strengthened condition allows us to delete \na key from the tree in one downward pass without having to \u201cback up\u201d (with \none exception, which we\u2019ll explain). You should interpret the following \nspecification for deletion from a B-tree with the understanding that if the root \nnode x ever becomes an internal node having no keys (this situation can occur \nin cases 2c and 3b then we delete x, and x\u2019s only child x.c1 becomes the new \nroot of the tree, decreasing the height of the tree by one and preserving the \nproperty that the root of the tree contains at least one key (unless the tree is \nempty). \nVarious Cases of Deletion \nCase 1: If the key k is in node x and x is a leaf, delete the key k from x. \nCase 2: If the key k is in node x and x is an internal node, do the following. \n\uf0b7 If the child y that precedes k in node x has at least t keys, then find the \npredecessor k0 of k in the sub-tree rooted at y. Recursively delete k0, \nand replace k with k0 in x. (We can find k0 and delete it in a single \ndownward pass.) \n\uf0b7 If y has fewer than t keys, then, symmetrically, examine the child z \nthat follows k in node x. If z has at least t keys, then find the successor \nk0 of k in the subtree rooted at z. Recursively delete k0, and replace k \nwith k0 in x. (We can find k0 and delete it in a single downward pass.) \n\uf0b7 Otherwise, if both y and z have only t-1 keys, merge k and all of z into \ny, so that x loses both k and the pointer to z, and y now contains 2t-1 \nkeys. Then free z and recursively delete k from y. \nCase 3: If the key k is not present in internal node x, determine the root x.c(i) \nof the appropriate subtree that must contain k, if k is in the tree at all. If x.c(i) \n \n \nhas only t-1 keys, execute steps 3a or 3b as necessary to guarantee that we \ndescend to a node containing at least t keys. Then finish by recursing on the \nappropriate child of x. \n\uf0b7 If x.c(i) has only t-1 keys but has an immediate sibling with at least t \nkeys, give x.c(i) an extra key by moving a key from x down into x.c(i), \nmoving a key from x.c(i) \u2019s immediate left or right sibling up into x, \nand moving the appropriate child pointer from the sibling into x.c(i). \n\uf0b7 If x.c(i) and both of x.c(i)\u2019s immediate siblings have t-1 keys, merge \nx.c(i) with one sibling, which involves moving a key from x down into \nthe new merged node to become the median key for that node. \nSince most of the keys in a B-tree are in the leaves, deletion operations are most \noften used to delete keys from leaves. The recursive delete procedure then acts \nin one downward pass through the tree, without having to back up. When \ndeleting a key in an internal node, however, the procedure makes a downward \npass through the tree but may have to return to the node from which the key was \ndeleted to replace the key with its predecessor or successor (cases 2a and 2b). \n \n19.4. All about B+ Tree \nB + Tree is a variation of the B-tree data structure. In a B + tree, data pointers \nare stored only at the leaf nodes of the tree. In a B+ tree structure of a leaf node \ndiffers from the structure of internal nodes. The leaf nodes have an entry for \nevery value of the search field, along with a data pointer to the record (or to the \nblock that contains this record). The leaf nodes of the B+ tree is linked together \nto provide ordered access to the search field to the records. Internal nodes of a \nB+ tree is used to guide the search. Some search field values from the leaf nodes \nare repeated in the internal nodes of the B+ tree. \n \nFeatures of B+ Trees \n\uf0b7 Balanced: B+ Trees are self-balancing, which means that as data is \nadded or removed from the tree, it automatically adjusts itself to \nmaintain a balanced structure. This ensures that the search time \nremains relatively constant, regardless of the size of the tree. \n\uf0b7 Multi-level: B+ Trees are multi-level data structures, with a root node \nat the top and one or more levels of internal nodes below it. The leaf \nnodes at the bottom level contain the actual data. \n\uf0b7 Ordered: B+ Trees maintain the order of the keys in the tree, which \nmakes it easy to perform range queries and other operations that \nrequire sorted data. \n\uf0b7 Fan-out: B+ Trees have a high fan-out, which means that each node \ncan have many child nodes. This reduces the height of the tree and \nincreases the efficiency of searching and indexing operations. \n \n \n\uf0b7 Cache-friendly: B+ Trees are designed to be cache-friendly, which \nmeans that they can take advantage of the caching mechanisms in \nmodern computer architectures to improve performance. \n\uf0b7 Disk-oriented: B+ Trees are often used for disk-based storage \nsystems because they are efficient at storing and retrieving data from \ndisk. \n \nWhy Use B+ Tree? \n\uf0b7 B+ Trees are the best choice for storage systems with sluggish data \naccess because they minimize I/O operations while facilitating \nefficient disc access. \n\uf0b7 B+ Trees are a good choice for database systems and applications \nneeding quick data retrieval because of their balanced structure, which \nguarantees predictable performance for a variety of activities and \nfacilitates effective range-based queries. \n \nDifference Between B+ Tree and B Tree \nParameters \nB+ Tree \n B Tree \nStructure \nSeparate leaf nodes for data \nstorage and internal nodes for \nindexing \nNodes store both keys and \ndata values \nLeaf Nodes \nLeaf nodes form a linked list \nfor \nefficient \nrange-based \nqueries \nLeaf nodes do not form a \nlinked list \nOrder \nHigher order (more keys) \n Lower order (fewer keys) \nKey Duplication \nTypically \nallows \nkey \nduplication in leaf nodes \nUsually does not allow \nkey duplication \nDisk Access \nBetter disk access due to \nsequential reads in a linked list \nstructure \nMore disk I/O due to non-\nsequential reads in internal \nnodes \nApplications \nDatabase systems, file systems, \nwhere \nrange \nqueries \nare \ncommon \nIn-memory \ndata \nstructures, \ndatabases, \ngeneral-purpose use \nPerformance \nBetter performance for range \nqueries and bulk data retrieval \nBalanced performance for \nsearch, insert, and delete \noperations \nMemory Usage \nRequires more memory for \ninternal nodes \nRequires less memory as \nkeys and values are stored \nin the same node \n \nImplementation of B+ Tree \nIn order, to implement dynamic multilevel indexing, B-tree and B+ tree are \ngenerally employed. The drawback of the B-tree used for indexing, however, is \n \n \nthat it stores the data pointer (a pointer to the disk file block containing the key \nvalue), corresponding to a particular key value, along with that key value in the \nnode of a B-tree. This technique greatly reduces the number of entries that can \nbe packed into a node of a B-tree, thereby contributing to the increase in the \nnumber of levels in the B-tree, hence increasing the search time of a record. B+ \ntree eliminates the above drawback by storing data pointers only at the leaf \nnodes of the tree. Thus, the structure of the leaf nodes of a B+ tree is quite \ndifferent from the structure of the internal nodes of the B tree. It may be noted \nhere that, since data pointers are present only at the leaf nodes, the leaf nodes \nmust necessarily store all the key values along with their corresponding data \npointers to the disk file block, to access them.  \nMoreover, the leaf nodes are linked to providing ordered access to the records. \nThe leaf nodes, therefore, form the first level of the index, with the internal \nnodes forming the other levels of a multilevel index. Some of the key values of \nthe leaf nodes also appear in the internal nodes, to simply act as a medium to \ncontrol the searching of a record. \n \nStructure of B+ Tree \nB+ Trees contain two types of nodes: \n\uf0b7 Internal Nodes: Internal Nodes are the nodes that are present in at \nleast n/2 record pointers, but not in the root node, \n\uf0b7 Leaf Nodes: Leaf Nodes are the nodes that have n pointers. \n \nAdvantages of B+Trees \n\uf0b7 B+ tree with \u2018l\u2019 levels can store more entries in its internal nodes \ncompared to a B-tree having the same \u2018l\u2019 levels. This accentuates the \nsignificant improvement made to the search time for any given key. \nHaving lesser levels and the presence of Pnext pointers imply that the \nB+ trees is very quick and efficient in accessing records from disks.  \n\uf0b7 Data stored in a B+ tree can be accessed both sequentially and directly. \n\uf0b7 It takes an equal number of disk accesses to fetch records. \n\uf0b7 B+trees have redundant search keys, and storing search keys \nrepeatedly is not possible.  \n \nDisadvantages of B+ Trees \n\uf0b7 The major drawback of B-tree is the difficulty of traversing the keys \nsequentially. The B+ tree retains the rapid random access property of \nthe B-tree while also allowing rapid sequential access. \nApplication of B+ Trees \n\uf0b7 Multilevel Indexing \n\uf0b7 Faster operations on the tree (insertion, deletion, search) \n\uf0b7 Database indexing \n \n \n \n \nInsertion in B+ Tree \n During insertion following properties of B+ Tree must be followed:  \n\uf0b7 Each node except root can have a maximum of M children and at \nleast ceil(M/2) children. \n\uf0b7 Each node can contain a maximum of M \u2013 1 keys and a minimum \nof ceil(M/2) \u2013 1 keys. \n\uf0b7 The root has at least two children and atleast one search key. \n\uf0b7 While insertion overflow of the node occurs when it contains more \nthan M \u2013 1 search key values. \nHere M is the order of B+ tree. \nSteps for insertion in B+ Tree \nStep 1. Every element is inserted into a leaf node. So, go to the appropriate \nleaf node. \nStep 2. Insert the key into the leaf node in increasing order only if there is \nno overflow. If there is an overflow go ahead with the following steps \nmentioned below to deal with overflow while maintaining the B+ Tree \nproperties. \nProperties for insertion B+ Tree \nCase 1: Overflow in leaf node  \n\uf0b7 Split the leaf node into two nodes. \n\uf0b7 First node contains ceil((m-1)/2) values. \n\uf0b7 Second node contains the remaining values. \n\uf0b7 Copy the smallest search key value from second node to the parent \nnode.(Right biased) \nBelow is the illustration of inserting 8 into B+ Tree of order of 5:  \n                               Figure 43: Insertion in B+ Tree \nCase 2: Overflow in non-leaf node  \n\uf0b7 Split the non-leaf node into two nodes. \n\uf0b7 First node contains ceil(m/2)-1 values. \n\uf0b7 Move the smallest among remaining to the parent. \n\uf0b7 Second node contains the remaining keys. \n \n \nBelow is the illustration of inserting 15 into B+ Tree of order of 5:  \n                          Figure 44: Insertion in B+ Tree \n \nDifference between B Tree and B+ Tree \nBasis \nof \nComparison \nB tree \nB+ tree \nPointers \nAll internal and leaf nodes have data \npointers \nOnly leaf nodes have data \npointers \nSearch \nSince all keys are not available at leaf, \nsearch often takes more time. \nAll keys are at leaf nodes, \nhence search is faster and \nmore accurate. \nRedundant \nKeys \nNo duplicate of keys is maintained in \nthe tree. \nDuplicate \nof \nkeys \nare \nmaintained and all nodes are \npresent at the leaf. \nInsertion \nInsertion takes more time and it is not \npredictable sometimes. \nInsertion is easier and the \nresults are always the same. \nDeletion \nDeletion of the internal node is very \ncomplex, and the tree has to undergo a \nlot of transformations. \nDeletion of any node is easy \nbecause all nodes are found \nat leaf. \nLeaf Nodes \nLeaf nodes are not stored as structural \nlinked list. \nLeaf nodes are stored as \nstructural linked list. \nAccess \nSequential access to nodes is not \npossible \nSequential access is possible \njust like linked list \nHeight \nFor a particular number nodes height \nis larger \nHeight is lesser than B tree \nfor the same number of \nnodes \nApplication \nB-Trees used in Databases, Search \nengines \nB+ Trees used in Multilevel \nIndexing, Database indexing \nNumber \nof \nNodes \nNumber of nodes at any intermediary \nlevel \u2018l\u2019 is 2l. \nEach intermediary node can \nhave n/2 to n children."
    },
    {
      "document": "DBMS_Full_Notes.pdf",
      "importance_rank": 5,
      "refined_text": "fi\n2.\nIts a Pattern 3 in LEC-19 (Database Scaling Pattern). (Command Query Responsibility Segregation) \n3.\nThe true or latest data is kept in the Master DB thus write operations are directed there. Reading ops are done only from slaves. This architecture \nserves the purpose of safeguarding site reliability, availability, reduce latency etc . If a site receives a lot of traf\ufb01c and the only available \ndatabase is one master, it will be overloaded with reading and writing requests. Making the entire system slow for everyone on the site. \n4.\nDB replication will take care of distributing data from Master machine to Slaves machines. This can be synchronous or asynchronous depending \nupon the system\u2019s need.\nCodeHelp"
    },
    {
      "document": "DBMS_Full_Notes.pdf",
      "importance_rank": 6,
      "refined_text": "2.\nForeign Key\n1.\nFK refers to PK of other table.\n2.\nEach relation can having any number of FK.\n3.\nCREATE TABLE ORDER (\nid INT PRIMARY KEY,\ndelivery_date DATE,\norder_placed_date DATE,\ncust_id INT,\nFOREIGN KEY (cust_id) REFERENCES customer(id)\n);\n3.\nUNIQUE\n1.\nUnique, can be null, table can have multiple unique attributes.\n2.\nCREATE TABLE customer (\n\u2026\nemail VARCHAR(1024) UNIQUE,\n\u2026\n);\n4.\nCHECK\n1.\nCREATE TABLE customer (\n\u2026\nCONSTRAINT age_check CHECK (age > 12),\n\u2026\n);\n2.\n\u201cage_check\u201d, can also avoid this, MySQL generates name of constraint automatically.\nCodeHelp\n5.\nDEFAULT\n1.\nSet default value of the column.\n2.\nCREATE TABLE account (\u2028\n\u2026\u2028\nsaving-rate DOUBLE NOT NULL DEFAULT 4.25,\u2028\n\u2026\u2028\n);\n6.\nAn attribute can be PK and FK both in a table.\n7.\nALTER OPERATIONS\n1.\nChanges schema\n2.\nADD\n1.\nAdd new column.\n2.\nALTER TABLE table_name ADD new_col_name datatype ADD new_col_name_2 datatype;\n3.\ne.g., ALTER TABLE customer ADD age INT NOT NULL;\n3.\nMODIFY\n1.\nChange datatype of an attribute.\n2.\nALTER TABLE table-name MODIFY col-name col-datatype;\n3.\nE.g., VARCHAR TO CHAR\u2028\nALTER TABLE customer MODIFY name CHAR(1024);\n4.\nCHANGE COLUMN\n1.\nRename column name.\n2.\nALTER TABLE table-name CHANGE COLUMN old-col-name new-col-name new-col-datatype;\n3.\ne.g., ALTER TABLE customer CHANGE COLUMN name customer-name VARCHAR(1024);\n5.\nDROP COLUMN\n1.\nDrop a column completely.\n2.\nALTER TABLE table-name DROP COLUMN col-name;\n3.\ne.g., ALTER TABLE customer DROP COLUMN middle-name;\n6.\nRENAME\n1.\nRename table name itself.\n2.\nALTER TABLE table-name RENAME TO new-table-name;\n3.\ne.g., ALTER TABLE customer RENAME TO customer-details;\nDATA MANIPULATION LANGUAGE (DML)\n1.\nINSERT\n1.\nINSERT INTO table-name(col1, col2, col3) VALUES (v1, v2, v3), (val1, val2, val3);\n2.\nUPDATE\n1.\nUPDATE table-name SET col1 = 1, col2 = \u2018abc\u2019 WHERE id = 1;\n2.\nUpdate multiple rows e.g.,\n1.\nUPDATE student SET standard = standard + 1;\n3.\nON UPDATE CASCADE\n1.\nCan be added to the table while creating constraints. Suppose there is a situation where we have two tables\nsuch that primary key of one table is the foreign key for another table. if we update the primary key of the first\ntable then using the ON UPDATE CASCADE foreign key of the second table automatically get updated.\n3.\nDELETE\n1.\nDELETE FROM table-name WHERE id = 1;\n2.\nDELETE FROM table-name; //all rows will be deleted.\n3.\nDELETE CASCADE - (to overcome DELETE constraint of Referential constraints)\n1.\nWhat would happen to child entry if parent table\u2019s entry is deleted?\n2.\nCREATE TABLE ORDER (\u2028\norder_id int PRIMARY KEY,\u2028\ndelivery_date DATE,\u2028\ncust_id INT,\u2028\nCodeHelp\nFOREIGN KEY(cust_id) REFERENCES customer(id) ON DELETE CASCADE\u2028\n);\n3.\nON DELETE NULL - (can FK have null values?)\n1.\nCREATE TABLE ORDER (\u2028\norder_id int PRIMARY KEY,\u2028\ndelivery_date DATE,\u2028\ncust_id INT,\u2028\nFOREIGN KEY(cust_id) REFERENCES customer(id) ON DELETE SET NULL\u2028\n);\n4.\nREPLACE\n1.\nPrimarily used for already present tuple in a table.\n2.\nAs UPDATE, using REPLACE with the help of WHERE clause in PK, then that row will be replaced.\n3.\nAs INSERT, if there is no duplicate data new tuple will be inserted.\n4.\nREPLACE INTO student (id, class) VALUES(4, 3);\n5.\nREPLACE INTO table SET col1 = val1, col2 = val2;\nJOINING TABLES\n1.\nAll RDBMS are relational in nature, we refer to other tables to get meaningful outcomes.\n2.\nFK are used to do reference to other table.\n3.\nINNER JOIN\n1.\nReturns a resultant table that has matching values from both the tables or all the tables.\n2.\nSELECT column-list FROM table1 INNER JOIN table2 ON condition1\u2028\nINNER JOIN table3 ON condition2\u2028\n\u2026;\n3.\nAlias in MySQL (AS)\n1.\nAliases in MySQL is used to give a temporary name to a table or a column in a table for the purpose of\na particular query. It works as a nickname for expressing the tables or column names. It makes the query short\nand neat.\n2.\nSELECT col_name AS alias_name FROM table_name;\n3.\nSELECT col_name1, col_name2,... FROM table_name AS alias_name;\n4.\nOUTER JOIN\n1.\nLEFT JOIN\n1.\nThis returns a resulting table that all the data from left table and the matched data from the right table.\n2.\nSELECT columns FROM table LEFT JOIN table2 ON Join_Condition;\n2.\nRIGHT JOIN\n1.\nThis returns a resulting table that all the data from right table and the matched data from the left table.\n2.\nSELECT columns FROM table RIGHT JOIN table2 ON join_cond;\n3.\nFULL JOIN\n1.\nThis returns a resulting table that contains all data when there is a match on left or right table data.\n2.\nEmulated in MySQL using LEFT and RIGHT JOIN.\n3.\nLEFT JOIN UNION RIGHT JOIN.\n4.\nSELECT columns FROM table1 as t1 LEFT JOIN table2 as t2 ON t1.id = t2.id\u2028\nUNION\u2028\nSELECT columns FROM table1 as t1 RIGHT JOIN table2 as t2 ON t1.id = t2.id;\n5.\nUNION ALL, can also be used this will duplicate values as well while UNION gives unique values.\n5.\nCROSS JOIN\n1.\nThis returns all the cartesian products of the data present in both tables. Hence, all possible variations\nare reflected in the output.\n2.\nUsed rarely in practical purpose.\n3.\nTable-1 has 10 rows and table-2 has 5, then resultant would have 50 rows.\n4.\nSELECT column-lists FROM table1 CROSS JOIN table2;\n6.\nSELF JOIN\nCodeHelp\n1.\nIt is used to get the output from a particular table when the same table is joined to itself.\n2.\nUsed very less.\n3.\nEmulated using INNER JOIN.\n4.\nSELECT columns FROM table as t1 INNER JOIN table as t2 ON t1.id = t2.id;\n7.\nJoin without using join keywords.\n1.\nSELECT * FROM table1,  table2 WHERE condition;\n2.\ne.g., SELECT artist_name, album_name, year_recordedFROM artist, albumWHERE artist.id = album.artist_id;\nSET OPERATIONS\n1.\nUsed to combine multiple select statements.\n2.\nAlways gives distinct rows.\n3.\nUNION\n1.\nCombines two or more SELECT statements.\n2.\nSELECT * FROM table1\u2028\nUNION\u2028\nSELECT * FROM table2;\n3.\nNumber of column, order of column must be same for table1 and table2.\n4.\nINTERSECT\n1.\nReturns common values of the tables.\n2.\nEmulated.\n3.\nSELECT DISTINCT column-list FROM table-1 INNER JOIN table-2 USING(join_cond);\n4.\nSELECT DISTINCT * FROM table1 INNER JOIN table2 ON USING(id);\n5.\nMINUS\n1.\nThis operator returns the distinct row from the first table that does not occur in the second table.\n2.\nEmulated.\n3.\nSELECT column_list FROM table1 LEFT JOIN table2 ON condition WHERE table2.column_name IS NULL;\n4.\ne.g., SELECT id FROM table-1 LEFT JOIN table-2 USING(id) WHERE table-2.id IS NULL;"
    },
    {
      "document": "OS_Full_Notes.pdf",
      "importance_rank": 7,
      "refined_text": "UNIT 1 \nAN INTRODUCTION TO OPERATING SYSTEMS \nApplication software performs specific task for the user. \nSystem software operates and controls the computer system and provides a platform to run \napplication software. \nAn operating system is a piece of software that manages all the resources of a computer \nsystem, both hardware and software, and provides an environment in which the user can \nexecute his/her programs in a convenient and efficient manner by hiding underlying \ncomplexity of the hardware and acting as a resource manager. \nWhy OS? \n1.\nWhat if there is no OS?\na.\nBulky and complex app. (Hardware interaction code must be in app\u2019s\ncode base)\nb.\nResource exploitation by 1 App.\nc.\nNo memory protection.\n2.\nWhat is an OS made up of?\na.\nCollection of system software.\nAn operating system function - \n- \nAccess to the computer hardware. \n-\ninterface between the user and the computer hardware\n-\nResource management (Aka, Arbitration) (memory, device, file, security, process etc)\n-\nHides the underlying complexity of the hardware. (Aka, Abstraction)\n-\nfacilitates execution of application programs by providing isolation and protection.\nUser \nApplication programs \nOperating system \nComputer hardware \nThe operating system provides the means for proper use of the resources in the operation of \nthe computer system. \nOS goals \u2013 \n\u2022\nMaximum CPU utilization\n\u2022\nLess process starvation\n\u2022\nHigher priority job execution\nTypes of operating systems \u2013 \n[MS DOS, 1981] \n[ATLAS, Manchester Univ., late 1950s \u2013 early 1960s] \n-\nMultiprogramming operating system \n[THE, Dijkstra, early 1960s]\n-\nSingle process operating system\n-\nBatch-processing operating system\n-\nMultitasking operating system\n-\nMulti-processing operating system\n-\nDistributed system\n-\nReal time OS\n[CTSS, MIT, early 1960s] \n[Windows NT] \n[LOCUS] \n[ATCS] \nLEC-2: Types of OS\nCodeHelp\nSingle process OS, only 1 process executes at a time from the ready queue. [Oldest] \nBatch-processing OS,  \n1.\nFirstly, user prepares his job using punch cards.\n2.\nThen, he submits the job to the computer operator.\n3.\nOperator collects the jobs from different users and sort the jobs into batches with\nsimilar needs.\n4.\nThen, operator submits the batches to the processor one by one.\n5.\nAll the jobs of one batch are executed together.\n-\nPriorities cannot be set, if a job comes with some higher priority.\n-\nMay lead to starvation. (A batch may take more time to complete)\n-\nCPU may become idle in case of I/O operations.\nMultiprogramming increases CPU utilization by keeping multiple jobs (code and data) \nin the memory so that the CPU always has one to execute in case some job gets busy with \nI/O. \n-\nSingle CPU\n-\nContext switching for processes.\n-\nSwitch happens when current process goes to wait state.\n-\nCPU idle time reduced.\nMultitasking is a logical extension of \nmultiprogramming. \n-\nSingle CPU\n-\nAble to run more than one task\nsimultaneously.\n-\nContext switching and time sharing used.\n-\nIncreases responsiveness.\n-\nCPU idle time is further reduced.\nMulti-processing OS, more than 1 CPU in a single computer. \n-\nIncreases reliability, 1 CPU fails, other\ncan work\n-\nBetter throughput.\n-\nLesser process starvation, (if 1 CPU is\nworking on some process, other can be\nexecuted on other CPU.\nCodeHelp\nDistributed OS, \nRTOS \n-\nOS manages many bunches of resources, \n>=1 CPUs, >=1 memory, >=1 GPUs, etc\n-\nLoosely connected autonomous, \ninterconnected computer nodes.\n-\ncollection of independent, networked, \ncommunicating, and physically separate \ncomputational nodes.\n-\nReal time error free, computations \nwithin tight-time boundaries.\n-\nAir Traffic control system, ROBOTS etc.\nCodeHelp\nLEC-3: Multi-Tasking vs Multi-Threading \nProgram: A Program is an executable file which contains a certain set of instructions written \nto complete the specific job or operation on your computer. \n\u2022\nIt\u2019s a compiled code. Ready to be executed.\n\u2022\nStored in Disk\nProcess: Program under execution. Resides in Computer\u2019s primary memory (RAM). \nThread:  \n\u2022\nSingle sequence stream within a process.\n\u2022\nAn independent path of execution in a process.\n\u2022\nLight-weight process.\n\u2022\nUsed to achieve parallelism by dividing a process\u2019s tasks which are independent path\nof execution.\n\u2022\nE.g., Multiple tabs in a browser, text editor (When you are typing in an editor, spell-\nchecking, formatting of text and saving the text are done concurrently by multiple\nthreads.)\nMulti-Tasking \nMulti-Threading \nThe execution of more than one task \nsimultaneously is called as multitasking. \nA process is divided into several different \nsub-tasks called as threads, which has its \nown path of execution. This concept is \ncalled as multithreading.  \nConcept of more than 1 processes being \ncontext switched. \nConcept of more than 1 thread. Threads are \ncontext switched. \nNo. of CPU 1. \nNo. of CPU >= 1. (Better to have more than \n1) \nIsolation and memory protection exists. \nOS must allocate separate memory and \nresources to each program that CPU is \nexecuting. \nNo isolation and memory protection, \nresources are shared among threads of that \nprocess.  \nOS allocates memory to a process; multiple \nthreads of that process share the same \nmemory and resources allocated to the \nprocess. \nThread Scheduling: \nThreads are scheduled for execution based on their priority. Even though threads are \nexecuting within the runtime, all threads are assigned processor time slices by the operating \nsystem. \nDifference between Thread Context Switching and Process Context Switching: \nThread Context switching \nProcess context switching \nOS saves current state of thread & switches \nto another thread of same process. \nOS saves current state of process & \nswitches to another process by restoring its \nstate. \nCodeHelp\nDoesn\u2019t includes switching of memory \naddress space.  \n(But Program counter, registers & stack are \nincluded.) \nIncludes switching of memory address \nspace. \nFast switching. \nSlow switching. \nCPU\u2019s cache state is preserved. \nCPU\u2019s cache state is flushed. \nCodeHelp\nLEC-4: Components of OS\n1.\nKernel: A kernel is that part of the operating system which interacts directly with \nthe hardware and performs the most crucial tasks.\na.\nHeart of OS/Core component\nb.\nVery first part of OS to load on start-up.\n2.\nUser space: Where application software runs, apps don\u2019t have privileged access to the \nunderlying hardware. It interacts with kernel.\na.\nGUI\nb.\nCLI\nA shell, also known as a command interpreter, is that part of the operating system that receives \ncommands from the users and gets them executed. \nFunctions of Kernel: \n1.\nProcess management:\na.\nScheduling processes and threads on the CPUs.\nb.\nCreating & deleting both user and system process.\nc.\nSuspending and resuming processes\nd.\nProviding mechanisms for process synchronization or process \ncommunication.\n2.\nMemory management:\na.\nAllocating and deallocating memory space as per need.\nb.\nKeeping track of which part of memory are currently being used and by \nwhich process.\n3.\nFile management:\na.\nCreating and deleting files.\nb.\nCreating and deleting directories to organize files.\nc.\nMapping files into secondary storage.\nd.\nBackup support onto a stable storage media.\n4.\nI/O management: to manage and control I/O operations and I/O devices\na.\nBuffering (data copy between two devices), caching and spooling.\ni.\nSpooling\n1.\nWithin differing speed two jobs.\n2.\nEg. Print spooling and mail spooling.\nii.\nBuffering\n1.\nWithin one job.\n2.\nEg. Youtube video buffering\niii.\nCaching\n1.\nMemory caching, Web caching etc.\na.\nAll functions are in kernel itself.\nb.\nBulky in size.\nc.\nMemory required to run is high.\nd.\nLess reliable, one module crashes -> whole kernel is down.\ne.\nHigh performance as communication is fast. (Less user mode, kernel\nmode overheads)\nf.\nEg. Linux, Unix, MS-DOS.\nTypes of Kernels: \n1.\nMonolithic kernel\nCodeHelp\n2.\nMicro Kernel\na.\nOnly major functions are in kernel.\ni.\nMemory mgmt.\nii.\nProcess mgmt.\nb.\nFile mgmt. and IO mgmt. are in User-space.\nc.\nsmaller in size.\nd.\nMore Reliable\ne.\nMore stable\nf.\nPerformance is slow.\ng.\nOverhead switching b/w user mode and kernel mode.\nh.\nEg. L4 Linux, Symbian OS, MINIX etc.\n3. Hybrid Kernel:\na.\nAdvantages of both worlds. (File mgmt. in User space and rest in Kernel \nspace. )\nb.\nCombined approach.\nc.\nSpeed and design of mono.\nd.\nModularity and stability of micro.\ne.\nEg. MacOS, Windows NT/7/10\nf.\nIPC also happens but lesser overheads\n4. Nano/Exo kernels\u2026\nQ. How will communication happen between user mode and kernel mode?\nAns. Inter process communication (IPC).\n1.\nTwo processes executing independently, having independent memory space (Memory \nprotection), But some may need to communicate to work.\n2.\nDone by shared memory and message passing.\nCodeHelp\n \nLEC-5: System Calls \nHow do apps interact with Kernel? -> using system calls. \nEg. Mkdir laks \n- \nMkdir indirectly calls kernel and asked the file mgmt. module to create a new \ndirectory. \n- \nMkdir is just a wrapper of actual system calls. \n- \nMkdir interacts with kernel using system calls. \n \nEg. Creating a process. \n- \nUser executes a process. (User space) \n- \nGets system call. (US) \n- \nExec system call to create a process. (KS) \n- \nReturn to US. \n \nTransitions from US to KS done by software interrupts. \n \nSystem calls are implemented in C. \n \nA system call is a mechanism using which a user program can request a service from the kernel for \nwhich it does not have the permission to perform. \nUser programs typically do not have permission to perform operations like accessing I/O devices and \ncommunicating other programs. \n \nSystem Calls are the only way through which a process can go into kernel mode from user mode. \n \n \nTypes of System Calls: \n1) Process Control \na. end, abort \nb. load, execute \nc. create process, terminate process \nd. get process attributes, set process attributes \ne. wait for time \nf. wait event, signal event \ng. allocate and free memory \n \n2) File Management \na. create file, delete file \nb. open, close \nc. read, write, reposition \nd. get file attributes, set file attributes \n \n3) Device Management \na. request device, release device \nb. read, write, reposition \nc. get device attributes, set device attributes \nd. logically attach or detach devices \n \n4) Information maintenance \na. get time or date, set time or date \nb. get system data, set system data \nc. get process, file, or device attributes \nd. set process, file, or device attributes \n \n5) Communication Management \na. create, delete communication connection \nb. send, receive messages \nc. transfer status information \nd. attach or detach remote devices \n \nExamples of Windows & Unix System calls: \nCategory \nWindows \nUnix \n \n \n \nProcess Control \nCreateProcess() \nExitProcess() \nWaitForSingleObject() \nfork() \nexit() \nwait() \n \n \n \nFile Management \nCreateFile() \nReadFile() \nWriteFile() \nCloseHandle() \nSetFileSecurity() \nInitlializeSecurityDescriptor() \nSetSecurityDescriptorGroup() \nopen () \nread () \nwrite () \nclose () \nchmod() \numask( \nchown() \n \n \n \nDevice Management \nSetConsoleMode() \nReadConsole() \nWriteConsole() \nioctI() \nread() \nwrite() \n \n \n \nInformation Management \nGetCurrentProcessID() \nSetTimer() \nSleep() \ngetpid () \nalarm () \nsleep () \n \n \n \nCommunication \nCreatePipe() \nCreateFileMapping() \nMapViewOfFile() \npipe () \nshmget () \nmmap() \ni.\nPC On\nii.\nCPU initializes itself and looks for a firmware program (BIOS) stored in \nBIOS Chip (Basic input-output system chip is a ROM chip found on \nmother board that allows to access & setup computer system at most \nbasic level.)\n1.\nIn modern PCs, CPU loads UEFI (Unified extensible firmware \ninterface)\niii.\nCPU runs the BIOS which tests and initializes system hardware. Bios \nloads configuration settings. If something is not appropriate (like missing \nRAM) error is thrown and boot process is stopped.\nThis is called POST (Power on self-test) process.\n(UEFI can do a lot more than just initialize hardware; it\u2019s really a tiny \noperating system. For example, Intel CPUs have the Intel Management \nEngine. This provides a variety of features, including powering Intel\u2019s \nActive Management Technology, which allows for remote management \nof business PCs.)\niv.\nBIOS will handoff responsibility for booting your PC to your OS\u2019s \nbootloader.\n1.\nBIOS looked at the MBR (master boot record), a special boot\nsector at the beginning of a disk. The MBR contains code that\nloads the rest of the operating system, known as a \u201cbootloader.\u201d\nThe BIOS executes the bootloader, which takes it from there and\nbegins booting the actual operating system\u2014Windows or Linux,\nfor example.\nIn other words,\nthe BIOS or UEFI examines a storage device on your system to\nlook for a small program, either in the MBR or on an EFI system\npartition, and runs it.\nv. The bootloader is a small program that has the large task of booting the\nrest of the operating system (Boots Kernel then, User Space). Windows \nuses a bootloader named Windows Boot Manager (Bootmgr.exe), most \nLinux systems use GRUB, and Macs use something called boot.efi\nLEC-6: What happens when you turn on your computer?\nCodeHelp\nLec-7: 32-Bit vs 64-Bit OS \n1.\nA 32-bit OS has 32-bit registers, and it can access 2^32 unique memory addresses. i.e., 4GB of\nphysical memory.\n2.\nA 64-bit OS has 64-bit registers, and it can access 2^64 unique memory addresses. i.e.,\n17,179,869,184 GB of physical memory.\n3.\n32-bit CPU architecture can process 32 bits of data & information.\n4.\n64-bit CPU architecture can process 64 bits of data & information.\n5.\nAdvantages of 64-bit over the 32-bit operating system:\na.\nAddressable Memory: 32-bit CPU -> 2^32 memory addresses, 64-bit CPU -> 2^64\nmemory addresses.\nb.\nResource usage: Installing more RAM on a system with a 32-bit OS doesn't impact\nperformance. However, upgrade that system with excess RAM to the 64-bit version of\nWindows, and you'll notice a difference.\nc.\nPerformance: All calculations take place in the registers. When you\u2019re performing math in\nyour code, operands are loaded from memory into registers. So, having larger registers\nallow you to perform larger calculations at the same time.\n32-bit processor can execute 4 bytes of data in 1 instruction cycle while 64-bit means that\nprocessor can execute 8 bytes of data in 1 instruction cycle.\n(In 1 sec, there could be thousands to billons of instruction cycles depending upon a\nprocessor design)\nd.\nCompatibility: 64-bit CPU can run both 32-bit and 64-bit OS. While 32-bit CPU can only\nrun 32-bit OS.\ne.\nBetter Graphics performance: 8-bytes graphics calculations make graphics-intensive apps\nrun faster.\nCodeHelp\nLec-8: Storage Devices Basics \nWhat are the different memory present in the computer system? \n1.\nRegister: Smallest unit of storage. It is a part of CPU itself.\nA register may hold an instruction, a storage address, or any data (such as bit sequence or individual\ncharacters).\nRegisters are a type of computer memory used to quickly accept, store, and transfer data and\ninstructions that are being used immediately by the CPU.\n2.\nCache: Additional memory system that temporarily stores frequently used instructions and data for\nquicker processing by the CPU.\n3.\nMain Memory: RAM.\n4.\nSecondary Memory: Storage media, on which computer can store data & programs.\nComparison \n1.\nCost:\na.\nPrimary storages are costly.\nb.\nRegisters are most expensive due to expensive semiconductors & labour.\nc.\nSecondary storages are cheaper than primary.\n2.\nAccess Speed:\na.\nPrimary has higher access speed than secondary memory.\nb.\nRegisters has highest access speed, then comes cache, then main memory.\n3.\nStorage size:\na.\nSecondary has more space.\n4.\nVolatility:\na.\nPrimary memory is volatile.\nb.\nSecondary is non-volatile.\nCodeHelp\nLec-9: Introduction to Process \n1.\nWhat is a program? Compiled code, that is ready to execute.\n2.\nWhat is a process? Program under execution.\n3.\nHow OS creates a process? Converting program into a process.\nSTEPS:\na.\nLoad the program & static data into memory.\nb.\nAllocate runtime stack.\nc.\nHeap memory allocation.\nd.\nIO tasks.\ne.\nOS handoffs control to main ().\n4.\nArchitecture of process:\n5.\nAttributes of process:\na.\nFeature that allows identifying a process uniquely.\nb.\nProcess table\ni. All processes are being tracked by OS using a table like data structure.\nii. Each entry in that table is process control block (PCB).\nc.\nPCB: Stores info/attributes of a process.\ni. Data structure used for each process, that stores information of a process such as\nprocess id, program counter, process state, priority etc.\n6.\nPCB structure:\nRegisters in the PCB, it is a data structure. When a processes is running and it's time slice expires, the \ncurrent value of process specific registers would be stored in the PCB and the process would be swapped \nout. When the process is scheduled to be run, the register values is read from the PCB and written to the \nCPU registers. This is the main purpose of the registers in the PCB. \nCodeHelp\nLec-10: Process States | Process Queues \n1.\nProcess States: As process executes, it changes state. Each process may be in one of the following\nstates.\na.\nNew: OS is about to pick the program & convert it into process. OR the process is being\ncreated.\nb.\nRun: Instructions are being executed; CPU is allocated.\nc.\nWaiting: Waiting for IO.\nd.\nReady: The process is in memory, waiting to be assigned to a processor.\ne.\nTerminated: The process has finished execution. PCB entry removed from process table.\n2.\nProcess Queues:\na.\nJob Queue:\ni. Processes in new state.\nii. Present in secondary memory.\niii. Job Schedular (Long term schedular (LTS)) picks process from the pool and\nloads them into memory for execution.\nb.\nReady Queue:\ni. Processes in Ready state.\nii. Present in main memory.\niii. CPU Schedular (Short-term schedular) picks process from ready queue and\ndispatch it to CPU.\nc.\nWaiting Queue:\ni. Processes in Wait state.\n3.\nDegree of multi-programming: The number of processes in the memory.\na.\nLTS controls degree of multi-programming.\n4.\nDispatcher: The module of OS that gives control of CPU to a process selected by STS.\nCodeHelp\nLEC-11: Swapping | Context-Switching | Orphan process | Zombie process \n1.\nSwapping\na.\nTime-sharing system may have medium term schedular (MTS).\nb.\nRemove processes from memory to reduce degree of multi-programming.\nc.\nThese removed processes can be reintroduced into memory, and its execution can be continued\nwhere it left off. This is called Swapping.\nd.\nSwap-out and swap-in is done by MTS.\ne.\nSwapping is necessary to improve process mix or because a change in memory requirements has\novercommitted available memory, requiring memory to be freed up.\nf.\nSwapping is a mechanism in which a process can be swapped temporarily out of main memory (or\nmove) to secondary storage (disk) and make that memory available to other processes. At some\nlater time, the system swaps back the process from the secondary storage to main memory.\n2.\nContext-Switching\na.\nSwitching the CPU to another process requires performing a state save of the current process and a\nstate restore of a different process.\nb.\nWhen this occurs, the kernel saves the context of the old process in its PCB and loads the saved\ncontext of the new process scheduled to run.\nc.\nIt is pure overhead, because the system does no useful work while switching.\nd.\nSpeed varies from machine to machine, depending on the memory speed, the number of registers\nthat must be copied etc.\n3.\nOrphan process\na.\nThe process whose parent process has been terminated and it is still running.\nb.\nOrphan processes are adopted by init process.\nc.\nInit is the first process of OS.\n4.\nZombie process / Defunct process\na.\nA zombie process is a process whose execution is completed but it still has an entry in the process\ntable.\nb.\nZombie processes usually occur for child processes, as the parent process still needs to read its\nchild\u2019s exit status. Once this is done using the wait system call, the zombie process is eliminated\nfrom the process table. This is known as reaping the zombie process.\nc.\nIt is because parent process may call wait () on child process for a longer time duration and child\nprocess got terminated much earlier.\nd.\nAs entry in the process table can only be removed, after the parent process reads the exit status of\nchild process. Hence, the child process remains a zombie till it is removed from the process table. \nCodeHelp\nLEC-12: Intro to Process Scheduling | FCFS | Convoy Effect \n1.\nProcess Scheduling\na.\nBasis of Multi-programming OS.\nb.\nBy switching the CPU among processes, the OS can make the computer more productive.\nc.\nMany processes are kept in memory at a time, when a process must wait or time quantum expires,\nthe OS takes the CPU away from that process & gives the CPU to another process & this pattern\ncontinues.\n2.\nCPU Scheduler\na.\nWhenever the CPU become ideal, OS must select one process from the ready queue to be executed.\nb.\nDone by STS.\n3.\nNon-Preemptive scheduling\na.\nOnce CPU has been allocated to a process, the process keeps the CPU until it releases CPU either by\nterminating or by switching to wait-state.\nb.\nStarvation, as a process with long burst time may starve less burst time process.\nc.\nLow CPU utilization.\n4.\nPreemptive scheduling\na.\nCPU is taken away from a process after time quantum expires along with terminating or switching\nto wait-state.\nb.\nLess Starvation\nc.\nHigh CPU utilization.\n5.\nGoals of CPU scheduling\na.\nMaximum CPU utilization\nb.\nMinimum Turnaround time (TAT).\nc.\nMin. Wait-time\nd.\nMin. response time.\ne.\nMax. throughput of system.\n6.\nThroughput: No. of processes completed per unit time.\n7.\nArrival time (AT): Time when process is arrived at the ready queue.\n8.\nBurst time (BT): The time required by the process for its execution.\n9.\nTurnaround time (TAT): Time taken from first time process enters ready state till it terminates. (CT - AT)\n10. Wait time (WT): Time process spends waiting for CPU. (WT = TAT \u2013 BT)\n11.\nResponse time: Time duration between process getting into ready queue and process getting CPU for the \nfirst time.\n12. Completion Time (CT): Time taken till process gets terminated.\n13. FCFS (First come-first serve):\na.\nWhichever process comes first in the ready queue will be given CPU first.\nb.\nIn this, if one process has longer BT. It will have major effect on average WT of diff processes, called \nConvoy effect.\nc.\nConvoy Effect is a situation where many processes, who need to use a resource for a short time, are \nblocked by one process holding that resource for a long time.\ni.\nThis cause poor resource management.\nCodeHelp\nLEC-13: CPU Scheduling | SJF | Priority | RR \n1.\nShortest Job First (SJF) [Non-preemptive]\na.\nProcess with least BT will be dispatched to CPU first.\nb.\nMust do estimation for BT for each process in ready queue beforehand, Correct estimation of BT is\nan impossible task (ideally.)\nc.\nRun lowest time process for all time then, choose job having lowest BT at that instance.\nd.\nThis will suffer from convoy effect as if the very first process which came is Ready state is having a\nlarge BT.\ne.\nProcess starvation might happen.\nf.\nCriteria for SJF algos, AT + BT.\n2.\nSJF [Preemptive]\na.\nLess starvation.\nb.\nNo convoy effect.\nc.\nGives average WT less for a given set of processes as scheduling short job before a long one\ndecreases the WT of short job more than it increases the WT of the long process.\n3.\nPriority Scheduling [Non-preemptive]\na.\nPriority is assigned to a process when it is created.\nb.\nSJF is a special case of general priority scheduling with priority inversely proportional to BT.\n4.\nPriority Scheduling [Preemptive]\na.\nCurrent RUN state job will be preempted if next job has higher priority.\nb.\nMay cause indefinite waiting (Starvation) for lower priority jobs. (Possibility is they won\u2019t get\nexecuted ever). (True for both preemptive and non-preemptive version)\ni.\nSolution: Ageing is the solution.\nii.\nGradually increase priority of process that wait so long. E.g., increase priority by 1 every 15\nminutes.\n5.\nRound robin scheduling (RR)\na.\nMost popular\nb.\nLike FCFS but preemptive.\nc.\nDesigned for time sharing systems.\nd.\nCriteria: AT + time quantum (TQ), Doesn\u2019t depend on BT.\ne.\nNo process is going to wait forever, hence very low starvation. [No convoy effect]\nf.\nEasy to implement.\ng.\nIf TQ is small, more will be the context switch (more overhead).\nCodeHelp\nLEC-14: MLQ | MLFQ \n1.\nMulti-level queue scheduling (MLQ)\na.\nReady queue is divided into multiple queues depending upon priority.\nb.\nA process is permanently assigned to one of the queues (inflexible) based on some property of\nprocess, memory, size, process priority or process type.\nc.\nEach queue has its own scheduling algorithm. E.g., SP -> RR, IP -> RR & BP -> FCFS.\nd.\nSystem process: Created by OS (Highest priority)\nInteractive process (Foreground process): Needs user input (I/O).\nBatch process (Background process): Runs silently, no user input required.\ne.\nScheduling among different sub-queues is implemented as fixed priority preemptive\nscheduling. E.g., foreground queue has absolute priority over background queue.\nf.\nIf an interactive process comes & batch process is currently executing. Then, batch process will\nbe preempted.\ng.\nProblem: Only after completion of all the processes from the top-level ready queue, the\nfurther level ready queues will be scheduled.\nThis came starvation for lower priority process.\nh.\nConvoy effect is present.\n2.\nMulti-level feedback queue scheduling (MLFQ)\na.\nMultiple sub-queues are present.\nb.\nAllows the process to move between queues. The idea is to separate processes according to\nthe characteristics of their BT. If a process uses too much CPU time, it will be moved to lower\npriority queue. This scheme leaves I/O bound and interactive processes in the higher-priority\nqueue.\nIn addition, a process that waits too much in a lower-priority queue may be moved to a higher \npriority queue. This form of ageing prevents starvation. \nc.\nLess starvation then MLQ.\nd.\nIt is flexible.\ne.\nCan be configured to match a specific system design requirement.\nSample MLFQ design:\nCodeHelp\n3.\nComparison:\nFCFS \nSJF \nPSJF \nPriority \nP-\nPriority \nRR \nMLQ \nMLFQ \nDesign \nSimple \nComplex \nComplex \nComplex \nComplex \nSimple \nComplex \nComplex \nPreemption No \nNo \nYes \nNo \nYes \nYes \nYes \nYes \nConvoy \neffect \nYes \nYes \nNo \nYes \nYes \nNo \nYes \nYes \nOverhead \nNo \nNo \nYes \nNo \nYes \nYes \nYes \nYes \nCodeHelp\n\u00b5\n\u2467\nLEC-15: Introduction to Concurrency \n1.\nConcurrency is the execution of the multiple instruction sequences at the same time. It happens in\nthe operating system when there are several process threads running in parallel.\n2.\nThread:\n\u2022\nSingle sequence stream within a process.\n\u2022\nAn independent path of execution in a process.\n\u2022\nLight-weight process.\n\u2022\nUsed to achieve parallelism by dividing a process\u2019s tasks which are independent path of\nexecution.\n\u2022\nE.g., Multiple tabs in a browser, text editor (When you are typing in an editor, spell\nchecking, formatting of text and saving the text are done concurrently by multiple threads.)\n3.\nThread Scheduling: Threads are scheduled for execution based on their priority. Even though\nthreads are executing within the runtime, all threads are assigned processor time slices by the\noperating system.\n4.\nThreads context switching\n\u2022\nOS saves current state of thread & switches to another thread of same process.\n\u2022\nDoesn\u2019t includes switching of memory address space. (But Program counter, registers &\nstack are included.)\n\u2022\nFast switching as compared to process switching\n\u2022\nCPU\u2019s cache state is preserved.\n5.\nHow each thread get access to the CPU?\n\u2022\nEach thread has its own program counter.\n\u2022\nDepending upon the thread scheduling algorithm, OS schedule these threads.\n\u2022\nOS will fetch instructions corresponding to PC of that thread and execute instruction.\n6.\nI/O or TQ, based context switching is done here as well\n\u2022\nWe have TCB (Thread control block) like PCB for state storage management while\nperforming context switching.\n7.\nWill single CPU system would gain by multi-threading technique?\n\u2022\nNever.\n\u2022\nAs two threads have to context switch for that single CPU.\n\u2022\nThis won\u2019t give any gain.\n8.\nBenefits of Multi-threading.\n\u2022\nResponsiveness\n\u2022\nResource sharing: Efficient resource sharing.\n\u2022\nEconomy: It is more economical to create and context switch threads.\n1.\nAlso, allocating memory and resources for process creation is costly, so better to\ndivide tasks into threads of same process.\n\u2022\nThreads allow utilization of multiprocessor architectures to a greater scale and efficiency.\nCodeHelp\nLEC-16: Critical Section Problem and How to address it \n1. Process synchronization techniques play a key role in maintaining\nthe consistency of shared data\n2. Critical Section (C.S)\na. The critical section refers to the segment of code where processes/threads\naccess shared resources, such as common variables and files, and perform\nwrite operations on them. Since processes/threads execute concurrently, any\nprocess can be interrupted mid-execution.\n3. Major Thread scheduling issue\na. Race Condition\ni. A race condition occurs when two or more threads can access shared\ndata and they try to change it at the same time. Because the thread\nscheduling algorithm can swap between threads at any time, you\ndon't know the order in which the threads will attempt to access the\nshared data. Therefore, the result of the change in data is dependent\non the thread scheduling algorithm, i.e., both threads are \"racing\" to\naccess/change the data.\n4. Solution to Race Condition\na. Atomic operations: Make Critical code section an atomic operation, i.e.,\nExecuted in one CPU cycle.\nb. Mutual Exclusion using locks.\nc.\nSemaphores\n5. Can we use a simple flag variable to solve the problem of race condition?\na. No.\n6. Peterson\u2019s solution can be used to avoid race condition but holds good for only 2 \nprocess/threads.\n7. Mutex/Locks\na. Locks can be used to implement mutual exclusion and avoid race condition \nby allowing only one thread/process to access critical section.\nb. Disadvantages:\ni. Contention: one thread has acquired the lock, other threads will be \nbusy waiting, what if thread that had acquired the lock dies, then all \nother threads will be in infinite waiting.\nii. Deadlocks\niii. Debugging\niv. Starvation of high priority threads.\nCodeHelp\nLEC-17: Conditional Variable and Semaphores for Threads synchronization \n1. Conditional variable\na. The condition variable is a synchronization primitive that lets the thread wait\nuntil a certain condition occurs.\nb. Works with a lock\nc.\nThread can enter a wait state only when it has acquired a lock. When a\nthread enters the wait state, it will release the lock and wait until another\nthread notifies that the event has occurred. Once the waiting thread enters\nthe running state, it again acquires the lock immediately and starts executing.\nd. Why to use conditional variable?\ni. To avoid busy waiting.\ne. Contention is not here.\n2. Semaphores\na. Synchronization method.\nb. An integer that is equal to number of resources\nc.\nMultiple threads can go and execute C.S concurrently.\nd. Allows multiple program threads to access the finite instance of resources\nwhereas mutex allows multiple threads to access a single shared resource\none at a time.\ne. Binary semaphore: value can be 0 or 1.\ni. Aka, mutex locks\nf.\nCounting semaphore\ni. Can range over an unrestricted domain.\nii. Can be used to control access to a given resource consisting of a finite\nnumber of instances.\ng. To overcome the need for busy waiting, we can modify the definition of\nthe wait () and signal () semaphore operations. When a process executes the\nwait () operation and finds that the semaphore value is not positive, it must\nwait. However, rather than engaging in busy waiting, the process car block\nitself. The block- operation places a process into a waiting queue associated\nwith the semaphore, and the state of the process is switched to the Waiting\nstate. Then control is transferred to the CPU scheduler, which selects another\nprocess to execute.\nh. A process that is blocked, waiting on a semaphore S, should be restarted\nwhen some other process executes a signal () operation. The process is\nrestarted by a wakeup () operation, which changes the process from the\nwaiting state to the ready state. The process is then placed in the ready\nqueue.\nCodeHelp\nLec-20: The Dining Philosophers problem \n1.\nWe have 5 philosophers.\n2.\nThey spend their life just being in two states:\na.\nThinking\nb.\nEating\n3.\nThey sit on a circular table surrounded by 5 chairs (1 each), in the center of table is a bowl of \nnoodles, and the table is laid with 5 single forks.\n4.\nThinking state: When a ph. Thinks, he doesn\u2019t interact with others.\n5.\nEating state: When a ph. Gets hungry, he tries to pick up the 2 forks adjacent to him (Left and \nRight). He can pick one fork at a time.\n6.\nOne can\u2019t pick up a fork if it is already taken.\n7.\nWhen ph. Has both forks at the same time, he eats without releasing forks.\n8.\nSolution can be given using semaphores.\na.\nEach fork is a binary semaphore.\nb.\nA ph. Calls wait() operation to acquire a fork.\nc.\nRelease fork by calling signal().\nd.\nSemaphore fork[5]{1};\n9.\nAlthough the semaphore solution makes sure that no two neighbors are eating simultaneously \nbut it could still create Deadlock.\n10. Suppose that all 5 ph. Become hungry at the same time and each picks up their left fork, then \nAll fork semaphores would be 0.\n11. When each ph. Tries to grab his right fork, he will be waiting for ever (Deadlock)\n12. We must use some methods to avoid Deadlock and make the solution work\na.\nAllow at most 4 ph. To be sitting simultaneously.\nb.\nAllow a ph. To pick up his fork only if both forks are available and to do this, he must \npick them up in a critical section (atomically).\nCodeHelp\nc. Odd-even rule.\nan odd ph. Picks up first his left fork and then his right fork, whereas an even ph. Picks\nup his right fork then his left fork.\n13. Hence, only semaphores are not enough to solve this problem.\nWe must add some enhancement rules to make deadlock free solution.\nCodeHelp\nLEC-21: Deadlock Part-1 \n1.\nIn Multi-programming environment, we have several processes competing for finite number of\nresources\n2.\nProcess requests a resource (R), if R is not available (taken by other process), process enters in a\nwaiting state. Sometimes that waiting process is never able to change its state because the resource,\nit has requested is busy (forever), called DEADLOCK (DL)\n3.\nTwo or more processes are waiting on some resource\u2019s availability, which will never be available as\nit is also busy with some other process. The Processes are said to be in Deadlock.\n4.\nDL is a bug present in the process/thread synchronization method.\n5.\nIn DL, processes never finish executing, and the system resources are tied up, preventing other jobs\nfrom starting.\n6.\nExample of resources: Memory space, CPU cycles, files, locks, sockets, IO devices etc.\n7.\nSingle resource can have multiple instances of that. E.g., CPU is a resource, and a system can have 2\nCPUs.\n8.\nHow a process/thread utilize a resource?\na.\nRequest: Request the R, if R is free Lock it, else wait till it is available.\nb.\nUse\nc.\nRelease: Release resource instance and make it available for other processes\n9.\nDeadlock Necessary Condition: 4 Condition should hold simultaneously.\na.\nMutual Exclusion\ni. Only 1 process at a time can use the resource, if another process requests that \nresource, the requesting process must wait until the resource has been released.\nb.\nHold & Wait\ni. A process must be holding at least one resource & waiting to acquire additional \nresources that are currently being held by other processes.\nc.\nNo-preemption\ni. Resource must be voluntarily released by the process after completion of \nexecution. (No resource preemption)\nd.\nCircular wait\ni. A set {P0, P1, \u2026 ,Pn} of waiting processes must exist such that P0 is waiting for a \nresource held by P1, P1 is waiting for a resource held by P2, and so on.\n10.\nMethods for handling Deadlocks:\na.\nUse a protocol to prevent or avoid deadlocks, ensuring that the system will never enter a \ndeadlocked state.\nb.\nAllow the system to enter a deadlocked state, detect it, and recover.\nCodeHelp\nc.\nIgnore the problem altogether and pretend that deadlocks never occur in system. (Ostrich\nalgorithm) aka, Deadlock ignorance.\n11. To ensure that deadlocks never occur, the system can use either a deadlock prevention or\ndeadlock avoidance scheme.\n12. Deadlock Prevention: by ensuring at least one of the necessary conditions cannot hold.\na.\nMutual exclusion\ni. Use locks (mutual exclusion) only for non-sharable resource.\nii. Sharable resources like Read-Only files can be accessed by multiple\nprocesses/threads.\niii. However, we can\u2019t prevent DLs by denying the mutual-exclusion condition,\nbecause some resources are intrinsically non-sharable.\nb.\nHold & Wait\ni. To ensure H&W condition never occurs in the system, we must guarantee that,\nwhenever a process requests a resource, it doesn\u2019t hold any other resource.\nii. Protocol (A) can be, each process has to request and be allocated all its resources\nbefore its execution.\niii. Protocol (B) can be, allow a process to request resources only when it has none. It\ncan request any additional resources after it must have released all the resources\nthat it is currently allocated.\nc.\nNo preemption\ni. If a process is holding some resources and request another resource that cannot\nbe immediately allocated to it, then all the resources the process is currently\nholding are preempted. The process will restart only when it can regain its old\nresources, as well as the new one that it is requesting. (Live Lock may occur).\nii. If a process requests some resources, we first check whether they are available. If\nyes, we allocate them. If not, we check whether they are allocated to some other\nprocess that is waiting for additional resources. If so, preempt the desired resource\nfrom waiting process and allocate them to the requesting process.\nd.\nCircular wait\ni. To ensure that this condition never holds is to impose a proper ordering of\nresource allocation.\nii. P1 and P2 both require R1 and R1, locking on these resources should be like, both\ntry to lock R1 then R2. By this way which ever process first locks R1 will get R2.\nCodeHelp\nLEC-22: Deadlock Part-2 \n1.\nDeadlock Avoidance: Idea is, the kernel be given in advance info concerning which resources will\nuse in its lifetime.\nBy this, system can decide for each request whether the process should wait.\nTo decide whether the current request can be satisfied or delayed, the system must consider the\nresources currently available, resources currently allocated to each process in the system and the\nfuture requests and releases of each process.\na.\nSchedule process and its resources allocation in such a way that the DL never occur.\nb.\nSafe state: A state is safe if the system can allocate resources to each process (up to its\nmax.) in some order and still avoid DL.\nA system is in safe state only if there exists a safe sequence.\nc.\nIn an Unsafe state, the operating system cannot prevent processes from requesting\nresources in such a way that any deadlock occurs. It is not necessary that all unsafe states\nare deadlocks; an unsafe state may lead to a deadlock.\nd.\nThe main key of the deadlock avoidance method is whenever the request is made for\nresources then the request must only be approved only in the case if the resulting state is a\nsafe state.\ne.\nIn a case, if the system is unable to fulfill the request of all processes, then the state of the\nsystem is called unsafe.\nf.\nScheduling algorithm using which DL can be avoided by finding safe state. (Banker\nAlgorithm)\n2.\nBanker Algorithm\na.\nWhen a process requests a set of resources, the system must determine whether allocating\nthese resources will leave the system in a safe state. If yes, then the resources may be\nallocated to the process. If not, then the process must wait till other processes release\nenough resources.\n3.\nDeadlock Detection: Systems haven\u2019t implemented deadlock-prevention or a deadlock avoidance\ntechnique, then they may employ DL detection then, recovery technique.\na.\nSingle Instance of Each resource type (wait-for graph method)\ni. A deadlock exists in the system if and only if there is a cycle in the wait-for graph.\nIn order to detect the deadlock, the system needs to maintain the wait-for graph\nand periodically system invokes an algorithm that searches for the cycle in the\nwait-for graph.\nb.\nMultiple instances for each resource type\ni. Banker Algorithm\n4.\nRecovery from Deadlock\na.\nProcess termination\ni. Abort all DL processes\nii. Abort one process at a time until DL cycle is eliminated.\nb.\nResource preemption\ni. To eliminate DL, we successively preempt some resources from processes and\ngive these resources to other processes until DL cycle is broken.\nCodeHelp\nLEC-24: Memory Management Techniques | Contiguous Memory Allocation \n1.\nIn Multi-programming environment, we have multiple processes in the main memory (Ready Queue) to\nkeep the CPU utilization high and to make computer responsive to the users.\n2.\nTo realize this increase in performance, however, we must keep several processes in the memory; that is, we\nmust share the main memory. As a result, we must manage main memory for all the different processes.\n3.\nLogical versus Physical Address Space\na.\nLogical Address\ni.\nAn address generated by the CPU.\nii.\nThe logical address is basically the address of an instruction or data used by a process.\niii.\nUser can access logical address of the process.\niv.\nUser has indirect access to the physical address through logical address.\nv.\nLogical address does not exist physically. Hence, aka, Virtual address.\nvi.\nThe set of all logical addresses that are generated by any program is referred to as Logical\nAddress Space.\nvii.\nRange: 0 to max.\nb.\nPhysical Address\ni.\nAn address loaded into the memory-address register of the physical memory.\nii.\nUser can never access the physical address of the Program.\niii.\nThe physical address is in the memory unit. It\u2019s a location in the main memory physically.\niv.\nA physical address can be accessed by a user indirectly but not directly.\nv.\nThe set of all physical addresses corresponding to the Logical addresses is commonly\nknown as Physical Address Space.\nvi.\nIt is computed by the Memory Management Unit (MMU).\nvii.\nRange: (R + 0) to (R + max), for a base value R.\nc.\nThe runtime mapping from virtual to physical address is done by a hardware device called the\nmemory-management unit (MMU).\nd.\nThe user's program mainly generates the logical address, and the user thinks that the program is\nrunning in this logical address, but the program mainly needs physical memory in order to\ncomplete its execution.\ne. \n4.\nHow OS manages the isolation and protect? (Memory Mapping and Protection)\na.\nOS provides this Virtual Address Space (VAS) concept.\nb.\nTo separate memory space, we need the ability to determine the range of legal addresses that the\nprocess may access and to ensure that the process can access only these legal addresses.\nc.\nThe relocation register contains value of smallest physical address (Base address [R]); the limit\nregister contains the range of logical addresses (e.g., relocation = 100040 & limit = 74600).\nd.\nEach logical address must be less than the limit register.\nCodeHelp\ne. MMU maps the logical address dynamically by adding the value in the relocation register. \nf. \nWhen CPU scheduler selects a process for execution, the dispatcher loads the relocation and limit \nregisters with the correct values as part of the context switch. Since every address generated by the  \nCPU (Logical address) is checked against these registers, we can protect both OS and other users\u2019 \nprograms and data from being modified by running process. \ng. Any attempt by a program executing in user mode to access the OS memory or other uses\u2019 \nmemory results in a trap in the OS, which treat the attempt as a fatal error. \nh. Address Translation \n \n \n5. Allocation Method on Physical Memory \na. Contiguous Allocation \nb. Non-contiguous Allocation \n6. Contiguous Memory Allocation \na. In this scheme, each process is contained in a single contiguous block of memory. \nb. Fixed Partitioning \ni. The main memory is divided into partitions of equal or different sizes. \nii. \n \niii. Limitations: \n1. \nInternal Fragmentation: if the size of the process is lesser then the total size of \nthe partition then some size of the partition gets wasted and remain unused. \nThis is wastage of the memory and called internal fragmentation. \n2. External Fragmentation: The total unused space of various partitions cannot be \nused to load the processes even though there is space available but not in the \ncontiguous form. \n3. Limitation on process size: If the process size is larger than the size of maximum \nsized partition then that process cannot be loaded into the memory. Therefore, a \nlimitation can be imposed on the process size that is it cannot be larger than the \nsize of the largest partition. \nCodeHelp\n4. Low degree of multi-programming: In fixed partitioning, the degree of \nmultiprogramming is fixed and very less because the size of the partition cannot \nbe varied according to the size of processes. \nc. Dynamic Partitioning \ni. In this technique, the partition size is not declared initially. It is declared at the time of \nprocess loading. \nii. \n \niii. Advantages over fixed partitioning \n1. \nNo internal fragmentation \n2. No limit on size of process \n3. Better degree of multi-programming \niv. Limitation \n1. \nExternal fragmentation \n \nCodeHelp\nLEC-25: Free Space Management \n1.\nDefragmentation/Compaction\na.\nDynamic partitioning suffers from external fragmentation.\nb.\nCompaction to minimize the probability of external fragmentation.\nc.\nAll the free partitions are made contiguous, and all the loaded partitions are brought together.\nd.\nBy applying this technique, we can store the bigger processes in the memory. The free partitions\nare merged which can now be allocated according to the needs of new processes. This technique is\nalso called defragmentation.\ne.\nThe efficiency of the system is decreased in the case of compaction since all the free spaces will be\ntransferred from several places to a single place.\n2.\nHow free space is stored/represented in OS?\na.\nFree holes in the memory are represented by a free list (Linked-List data structure).\n3.\nHow to satisfy a request of a of n size from a list of free holes?\na.\nVarious algorithms which are implemented by the Operating System in order to find out the holes\nin the linked list and allocate them to the processes.\nb.\nFirst Fit\ni.\nAllocate the first hole that is big enough.\nii.\nSimple and easy to implement.\niii.\nFast/Less time complexity\nc.\nNext Fit\ni.\nEnhancement on First fit but starts search always from last allocated hole.\nii.\nSame advantages of First Fit.\nd.\nBest Fit\ni.\nAllocate smallest hole that is big enough.\nii.\nLesser internal fragmentation.\niii.\nMay create many small holes and cause major external fragmentation.\niv.\nSlow, as required to iterate whole free holes list.\ne.\nWorst Fit\ni.\nAllocate the largest hole that is big enough.\nii.\nSlow, as required to iterate whole free holes list.\niii.\nLeaves larger holes that may accommodate other processes.\nCodeHelp\nLEC-26: Paging | Non-Contiguous Memory Allocation \n1.\nThe main disadvantage of Dynamic partitioning is External Fragmentation.\na.\nCan be removed by Compaction, but with overhead.\nb.\nWe need more dynamic/flexible/optimal mechanism, to load processes in the partitions.\n2.\nIdea behind Paging\na.\nIf we have only two small non-contiguous free holes in the memory, say 1KB each.\nb.\nIf OS wants to allocate RAM to a process of 2KB, in contiguous allocation, it is not possible, as we\nmust have contiguous memory space available of 2KB. (External Fragmentation)\nc.\nWhat if we divide the process into 1KB-1KB blocks?\n3.\nPaging\na.\nPaging is a memory-management scheme that permits the physical address space of a\nprocess to be non-contiguous.\nb.\nIt avoids external fragmentation and the need of compaction.\nc.\nIdea is to divide the physical memory into fixed-sized blocks called Frames, along with divide\nlogical memory into blocks of same size called Pages. (# Page size = Frame size)\nd.\nPage size is usually determined by the processor architecture. Traditionally, pages in a system had\nuniform size, such as 4,096 bytes. However, processor designs often allow two or more, sometimes\nsimultaneous, page sizes due to its benefits.\ne.\nPage Table\ni.\nA Data structure stores which page is mapped to which frame.\nii.\nThe page table contains the base address of each page in the physical memory.\nf.\nEvery address generated by CPU (logical address) is divided into two parts: a page number (p) and\na page offset (d). The p is used as an index into a page table to get base address the corresponding\nframe in physical memory.\ng.\nPage table is stored in main memory at the time of process creation and its base address is stored\nin process control block (PCB).\nh.\nA page table base register (PTBR) is present in the system that points to the current page table.\nChanging page tables requires only this one register, at the time of context-switching.\n4.\nHow Paging avoids external fragmentation?\na.\nNon-contiguous allocation of the pages of the process is allowed in the random free frames of the\nphysical memory.\n5.\nWhy paging is slow and how do we make it fast?\na.\nThere are too many memory references to access the desired location in physical memory.\n6.\nTranslation Look-aside buffer (TLB)\na.\nA Hardware support to speed-up paging process.\nb.\nIt\u2019s a hardware cache, high speed memory.\nc.\nTBL has key and value.\nCodeHelp\nd.\nPage table is stores in main memory & because of this when the memory references is made the\ntranslation is slow.\ne.\nWhen we are retrieving physical address using page table, after getting frame address\ncorresponding to the page number, we put an entry of the into the TLB. So that next time, we can\nget the values from TLB directly without referencing actual page table. Hence, make paging process\nfaster.\nf.\nTLB hit, TLB contains the mapping for the requested logical address.\ng.\nAddress space identifier (ASIDs) is stored in each entry of TLB. ASID uniquely identifies each\nprocess and is used to provide address space protection and allow to TLB to contain entries for\nseveral different processes. When TLB attempts to resolve virtual page numbers, it ensures that\nthe ASID for the currently executing process matches the ASID associated with virtual page. If it\ndoesn\u2019t match, the attempt is treated as TLB miss.\nCodeHelp\nLEC-27: Segmentation | Non-Contiguous Memory Allocation \n1.\nAn important aspect of memory management that become unavoidable with paging is separation of user\u2019s\nview of memory from the actual physical memory.\n2.\nSegmentation is memory management technique that supports the user view of memory.\n3.\nA logical address space is a collection of segments, these segments are based on user view of logical\nmemory.\n4.\nEach segment has segment number and offset, defining a segment.\n<segment-number, offset> {s,d}\n5.\nProcess is divided into variable segments based on user view.\n6.\nPaging is closer to the Operating system rather than the User. It divides all the processes into the form of\npages although a process can have some relative parts of functions which need to be loaded in the same\npage.\n7.\nOperating system doesn't care about the User's view of the process. It may divide the same function into\ndifferent pages and those pages may or may not be loaded at the same time into the memory. It\ndecreases the efficiency of the system.\n8.\nIt is better to have segmentation which divides the process into the segments. Each segment contains the\nsame type of functions such as the main function can be included in one segment and the library functions\ncan be included in the other segment.\n9. \n10. Advantages:\na.\nNo internal fragmentation.\nb.\nOne segment has a contiguous allocation, hence efficient working within segment.\nc.\nThe size of segment table is generally less than the size of page table.\nd.\nIt results in a more efficient system because the compiler keeps the same type of functions in one\nsegment.\n11.\nDisadvantages:\na.\nExternal fragmentation.\nb.\nThe different size of segment is not good that the time of swapping.\n12. Modern System architecture provides both segmentation and paging implemented in some hybrid\napproach.\nCodeHelp\nLEC-28: What is Virtual Memory? || Demand Paging || Page Faults \n1.\nVirtual memory is a technique that allows the execution of processes that are not completely in the\nmemory. It provides user an illusion of having a very big main memory. This is done by treating a part of\nsecondary memory as the main memory. (Swap-space)\n2.\nAdvantage of this is, programs can be larger than physical memory.\n3.\nIt is required that instructions must be in physical memory to be executed. But it limits the size of a\nprogram to the size of physical memory. In fact, in many cases, the entire program is not needed at the\nsame time. So, we want an ability to execute a program that is only partially in memory would give\nmany benefits:\na.\nA program would no longer be constrained by the amount of physical memory that is\navailable.\nb.\nBecause each user program could take less physical memory, more programs could be run at\nthe same time, with a corresponding increase in CPU utilization and throughput.\nc.\nRunning a program that is not entirely in memory would benefit both the system and the\nuser.\n4.\nProgrammer is provided very large virtual memory when only a smaller physical memory is available.\n5.\nDemand Paging is a popular method of virtual memory management.\n6.\nIn demand paging, the pages of a process which are least used, get stored in the secondary memory.\n7.\nA page is copied to the main memory when its demand is made, or page fault occurs. There are various\npage replacement algorithms which are used to determine the pages which will be replaced.\n8.\nRather than swapping the entire process into memory, we use Lazy Swapper.  A lazy swapper never\nswaps a page into memory unless that page will be needed.\n9.\nWe are viewing a process as a sequence of pages, rather than one large contiguous address space, using\nthe term Swapper is technically incorrect. A swapper manipulates entire processes, whereas a Pager is\nconcerned with individual pages of a process.\n10. How Demand Paging works?\na.\nWhen a process is to be swapped-in, the pager guesses which pages will be used.\nb.\nInstead of swapping in a whole process, the pager brings only those pages into memory. This,\nit avoids reading into memory pages that will not be used anyway.\nc.\nAbove way, OS decreases the swap time and the amount of physical memory needed.\nd.\nThe valid-invalid bit scheme in the page table is used to distinguish between pages that are\nin memory and that are on the disk.\ni.\nValid-invalid bit 1 means, the associated page is both legal and in memory.\nii.\nValid-invalid bit 0 means, the page either is not valid (not in the LAS of the process)\nor is valid but is currently on the disk.\nCodeHelp\ne. \n \nf. \nIf a process never attempts to access some invalid bit page, the process will be executed \nsuccessfully without even the need pages present in the swap space. \ng. What happens if the process tries to access a page that was not brought into memory, access \nto a page marked invalid causes page fault. Paging hardware noticing invalid bit for a \ndemanded page will cause a trap to the OS. \nh. The procedure to handle the page fault: \ni. Check an internal table (in PCB of the process) to determine whether the reference \nwas valid or an invalid memory access. \nii. If ref. was invalid process throws exception. \nIf ref. is valid, pager will swap-in the page. \niii. We find a free frame (from free-frame list) \niv. Schedule a disk operation to read the desired page into the newly allocated frame. \nv. When disk read is complete, we modify the page table that, the page is now in \nmemory. \nvi. Restart the instruction that was interrupted by the trap. The process can now access \nthe page as through it had always been in memory. \nCodeHelp\ni. \n \nj. \nPure Demand Paging \ni. In extreme case, we can start executing a process with no pages in memory. When \nOS sets the instruction pointer to the first instruction of the process, which is not in \nthe memory. The process immediately faults for the page and page is brought in the \nmemory. \nii. Never bring a page into memory until it is required. \nk. We use locality of reference to bring out reasonable performance from demand paging. \n11. Advantages of Virtual memory \na. The degree of multi-programming will be increased. \nb. User can run large apps with less real physical memory. \n12. Disadvantages of Virtual Memory \na. The system can become slower as swapping takes time. \nb. Thrashing may occur. \nCodeHelp\nLEC-29: Page Replacement Algorithms \n1.\nWhenever Page Fault occurs, that is, a process tries to access a page which is not currently present in a\nframe and OS must bring the page from swap-space to a frame.\n2.\nOS must do page replacement to accommodate new page into a free frame, but there might be a possibility\nthe system is working in high utilization and all the frames are busy, in that case OS must replace one of the\npages allocated into some frame with the new page.\n3.\nThe page replacement algorithm decides which memory page is to be replaced. Some allocated page is\nswapped out from the frame and new page is swapped into the freed frame.\n4.\nTypes of Page Replacement Algorithm: (AIM is to have minimum page faults)\na.\nFIFO\ni.\nAllocate frame to the page as it comes into the memory by replacing the oldest page.\nii.\nEasy to implement.\niii.\nPerformance is not always good\n1.\nThe page replaced may be an initialization module that was used long time ago\n(Good replacement candidate)\n2.\nThe page may contain a heavily used variable that was initialized early and is in\ncontent use. (Will again cause page fault)\niv.\nBelady\u2019s anomaly is present.\n1.\nIn the case of LRU and optimal page replacement algorithms, it is seen that\nthe number of page faults will be reduced if we increase the number of\nframes. However, Balady found that, In FIFO page replacement algorithm, the\nnumber of page faults will get increased with the increment in number of\nframes.\n2.\nThis is the strange behavior shown by FIFO algorithm in some of the cases.\nb.\nOptimal page replacement\ni.\nFind if a page that is never referenced in future. If such a page exists, replace this page\nwith new page.\nIf no such page exists, find a page that is referenced farthest in future. Replace this page\nwith new page.\nii.\nLowest page fault rate among any algorithm.\niii.\nDifficult to implement as OS requires future knowledge of reference string which is\nkind of impossible. (Similar to SJF scheduling)\nc.\nLeast-recently used (LRU)\ni.\nWe can use recent past as an approximation of the near future then we can replace the \npage that has not been used for the longest period.\nii.\nCan be implemented by two ways\n1.\nCounters\na.\nAssociate time field with each page table entry.\nb.\nReplace the page with smallest time value.\n2.\nStack\na.\nKeep a stack of page number.\nb.\nWhenever page is referenced, it is removed from the stack & put on \nthe top.\nc.\nBy this, most recently used is always on the top, & least recently used \nis always on the bottom.\nd.\nAs entries might be removed from the middle of the stack, so Doubly \nlinked list can be used.\nd.\nCounting-based page replacement \u2013 Keep a counter of the number of references that have been \nmade to each page. (Reference counting)\nCodeHelp\ni.\nLeast frequently used (LFU)\n1.\nActively used pages should have a large reference count.\n2.\nReplace page with the smallest count.\nii.\nMost frequently used (MFU)\n1.\nBased on the argument that the page with the smallest count was probably just\nbrought in and has yet to be used.\niii.\nNeither MFU nor LFU replacement is common.\nCodeHelp\nLEC-30: Thrashing \n1.\nThrashing\na.\nIf the process doesn\u2019t have the number of frames it needs to support pages in active use, it will\nquickly page-fault. At this point, it must replace some page. However, since all its pages are in active\nuse, it must replace a page that will be needed again right away. Consequently, it quickly faults\nagain, and again, and again, replacing pages that it must bring back in immediately.\nb.\nThis high paging activity is called Thrashing.\nc.\nA system is Thrashing when it spends more time servicing the page faults than executing\nprocesses.\nd.\nTechnique to Handle Thrashing\ni.\nWorking set model\n1.\nThis model is based on the concept of the Locality Model.\n2.\nThe basic principle states that if we allocate enough frames to a process to\naccommodate its current locality, it will only fault whenever it moves to some\nnew locality. But if the allocated frames are lesser than the size of the current\nlocality, the process is bound to thrash.\nii.\nPage Fault frequency\n1.\nThrashing has a high page-fault rate.\n2.\nWe want to control the page-fault rate.\n3.\nWhen it is too high, the process needs more frames. Conversely, if the page-fault\nrate is too low, then the process may have too many frames.\n4.\nWe establish upper and lower bounds on the desired page fault rate.\n5.\nIf pf-rate exceeds the upper limit, allocate the process another frame, if pf-rate\nfails falls below the lower limit, remove a frame from the process.\n6.\nBy controlling pf-rate, thrashing can be prevented.\nCodeHelp"
    }
  ]
}